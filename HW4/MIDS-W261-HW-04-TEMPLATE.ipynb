{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#MIDS---w261-Machine-Learning-At-Scale\" data-toc-modified-id=\"MIDS---w261-Machine-Learning-At-Scale-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MIDS - w261 Machine Learning At Scale</a></div><div class=\"lev2 toc-item\"><a href=\"#Assignment---HW4\" data-toc-modified-id=\"Assignment---HW4-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment - HW4</a></div><div class=\"lev3 toc-item\"><a href=\"#INSTRUCTIONS-for-SUBMISSION\" data-toc-modified-id=\"INSTRUCTIONS-for-SUBMISSION-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>INSTRUCTIONS for SUBMISSION</a></div><div class=\"lev3 toc-item\"><a href=\"#CONFIGURATION\" data-toc-modified-id=\"CONFIGURATION-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>CONFIGURATION</a></div><div class=\"lev3 toc-item\"><a href=\"#DATASETS\" data-toc-modified-id=\"DATASETS-113\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>DATASETS</a></div><div class=\"lev1 toc-item\"><a href=\"#HW-Problems\" data-toc-modified-id=\"HW-Problems-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>HW Problems</a></div><div class=\"lev2 toc-item\"><a href=\"#HW4.0\" data-toc-modified-id=\"HW4.0-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>HW4.0</a></div><div class=\"lev2 toc-item\"><a href=\"#HW4.1\" data-toc-modified-id=\"HW4.1-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>HW4.1</a></div><div class=\"lev2 toc-item\"><a href=\"#HW4.2----Preprocess-log-file-data\" data-toc-modified-id=\"HW4.2----Preprocess-log-file-data-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>HW4.2  - Preprocess log file data</a></div><div class=\"lev2 toc-item\"><a href=\"#HW4.3---Find-the-most-frequent-pages\" data-toc-modified-id=\"HW4.3---Find-the-most-frequent-pages-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>HW4.3 - Find the most frequent pages</a></div><div class=\"lev2 toc-item\"><a href=\"#HW4.4----Find-the-most-frequent-visitor\" data-toc-modified-id=\"HW4.4----Find-the-most-frequent-visitor-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>HW4.4  - Find the most frequent visitor</a></div><div class=\"lev3 toc-item\"><a href=\"#Run-a-test-on-some-data-that-has-multiple-visits\" data-toc-modified-id=\"Run-a-test-on-some-data-that-has-multiple-visits-251\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Run a test on some data that has multiple visits</a></div><div class=\"lev2 toc-item\"><a href=\"#HW4.5---Clustering-Tweet-Dataset\" data-toc-modified-id=\"HW4.5---Clustering-Tweet-Dataset-26\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>HW4.5 - Clustering Tweet Dataset</a></div><div class=\"lev2 toc-item\"><a href=\"#K-Means\" data-toc-modified-id=\"K-Means-27\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>K-Means</a></div><div class=\"lev2 toc-item\"><a href=\"#K-means-algorithm\" data-toc-modified-id=\"K-means-algorithm-28\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>K-means algorithm</a></div><div class=\"lev2 toc-item\"><a href=\"#Calculating-purity\" data-toc-modified-id=\"Calculating-purity-29\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>Calculating purity</a></div><div class=\"lev2 toc-item\"><a href=\"#HW4.6----(OPTIONAL)-Scaleable-K-MEANS++\" data-toc-modified-id=\"HW4.6----(OPTIONAL)-Scaleable-K-MEANS++-210\"><span class=\"toc-item-num\">2.10&nbsp;&nbsp;</span>HW4.6  - (OPTIONAL) Scaleable K-MEANS++</a></div><div class=\"lev3 toc-item\"><a href=\"#4.6.1-(OPTIONAL)-Apply-K-MEANS||\" data-toc-modified-id=\"4.6.1-(OPTIONAL)-Apply-K-MEANS||-2101\"><span class=\"toc-item-num\">2.10.1&nbsp;&nbsp;</span>4.6.1 (OPTIONAL) Apply K-MEANS||</a></div><div class=\"lev2 toc-item\"><a href=\"#HW4.7---(OPTIONAL)-Canopy-Clustering\" data-toc-modified-id=\"HW4.7---(OPTIONAL)-Canopy-Clustering-211\"><span class=\"toc-item-num\">2.11&nbsp;&nbsp;</span>HW4.7 - (OPTIONAL) Canopy Clustering</a></div><div class=\"lev3 toc-item\"><a href=\"#4.7.1-(OPTIONAL)-Apply-Canopy-Clustering-based-K-MEANS\" data-toc-modified-id=\"4.7.1-(OPTIONAL)-Apply-Canopy-Clustering-based-K-MEANS-2111\"><span class=\"toc-item-num\">2.11.1&nbsp;&nbsp;</span>4.7.1 (OPTIONAL) Apply Canopy Clustering based K-MEANS</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale \n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW4\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  *Your Name Goes Here*   \n",
    "__Class:__ MIDS w261 (Section *Your Section Goes Here*, e.g., Fall 2016 Group 1)     \n",
    "__Email:__  *Your UC Berkeley Email Goes Here*@iSchool.Berkeley.edu     \n",
    "__StudentId__  123457    __End of StudentId__     \n",
    "\n",
    "__NOTE:__ please replace `1234567` with your student id above      \n",
    "\n",
    "### INSTRUCTIONS for SUBMISSION\n",
    "\n",
    "This homework can be completed locally on your computer. __Please submit your notebook to your classroom github repository 24 hours prior to the next live session.__ \n",
    "\n",
    "### CONFIGURATION\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 2.7.14 \n",
      "HDFS filesystem running at: \n",
      "\t hdfs://quickstart.cloudera:8020\n"
     ]
    }
   ],
   "source": [
    "# print some configuration details for future replicability.\n",
    "print 'Python Version: %s' % (sys.version.split('|')[0])\n",
    "hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER\n",
    "#hdfs_conf = !hdfs getconf -confKey fs.default.name ### UNCOMMENT ON ALTISCALE\n",
    "print 'HDFS filesystem running at: \\n\\t %s' % (hdfs_conf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an HDFS directory for this assignment\n",
    "!hdfs dfs -mkdir hw4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[OPTIONAL]:__ Save yourself some typing by defining global variables for paths we'll reuse frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HDFS_DIR = \"/user/root/hw4\" # eg. /user/root/hw3 \n",
    "HOME_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[OPTIONAL]:__ Fix chrome formatting. _The cell below implements a quick hack based on [this stackoverflow thread](http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line) to fix [this known issue](https://github.com/mathjax/MathJax/issues/1300) with Mathjax formatting in Chrome (a rounding issue adds a border to the right of mathjax markup)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASETS\n",
    "For this homework we be using two different datasets:\n",
    "* __Microsoft log files data__ (available from the [UC Irvine KDD Archive](https://kdd.ics.uci.edu/databases/msweb/msweb.html) at [this url](http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/) - _This data, referenced in the asynch lectures, captures which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998. We'll perform preprocessing on the data in HW 4.2 and find the most frequently visited pages and visitors in HW 4.3 and 4.4_\n",
    "* __Tweets data__ (from [this publication](https://arxiv.org/abs/1505.04342) in the Journal of Computation Science and available at [this url] - _This data, comes from a corpus of Tweets that were hand coded to reflect whether the tweet was written by a human,cyborg,robot or spammer. The data are in two files: the primary [topUsers file](https://www.dropbox.com/s/6129k2urvbvobkr/topUsers_Apr-Jul_2014_1000-words.txt?dl=0), and an auxilary [word summaries](https://www.dropbox.com/s/w4oklbsoqefou3b/topUsers_Apr-Jul_2014_1000-words_summaries.txt?dl=0) file (more details below). We'll perform a KMeans clustering analysis on this data in HW 4.5._\n",
    "\n",
    "Follow the directions below to load each of these datasets. You may want to familiarize yourself with their contents before proceeding to the homework questions.\n",
    "\n",
    "__`anonymous-msweb.data`__  \n",
    "Notes on the data format:\n",
    "> The data is in an ASCII-based sparse-data format called \"DST\". Each line of the data file starts with a letter which tells the line's type. The three line types of interest are:\n",
    "\n",
    ">__Attribute lines__:\n",
    "e.g. `A,1277,1,\"NetShow for PowerPoint\",\"/stream\"`\n",
    "Where:\n",
    "  'A' marks this as an attribute line, \n",
    "  '1277' is the attribute ID number for an area of the website (called a Vroot),\n",
    "  '1' may be ignored, \n",
    "  '\"NetShow for PowerPoint\"' is the title of the Vroot, \n",
    "  '\"/stream\"' is the URL relative to \"http://www.microsoft.com\"\n",
    "\n",
    ">__Case and Vote Lines__:\n",
    "For each user, there is a case line followed by zero or more vote lines.\n",
    "For example:\n",
    "  C,\"10164\",10164\n",
    "  V,1123,1\n",
    "  V,1009,1\n",
    "  V,1052,1\n",
    "Where:\n",
    "  'C' marks this as a case line, \n",
    "  '10164' is the case ID number of a user, \n",
    "  'V' marks the vote lines for this case, \n",
    "  '1123', 1009', 1052' are the attributes ID's of Vroots that a user visited. \n",
    "  '1' may be ignored.\n",
    "  \n",
    "Run the cells below to download and examine the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1389k  100 1389k    0     0   347k      0  0:00:04  0:00:04 --:--:--  303k\n"
     ]
    }
   ],
   "source": [
    "# download the data\n",
    "!curl -L http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/anonymous-msweb.data -o anonymous-msweb.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I,4,\"www.microsoft.com\",\"created by getlog.pl\"\n",
      "T,1,\"VRoot\",0,0,\"VRoot\"\n",
      "N,0,\"0\"\n",
      "N,1,\"1\"\n",
      "T,2,\"Hide1\",0,0,\"Hide\"\n"
     ]
    }
   ],
   "source": [
    "# take a look\n",
    "!head -n 5 anonymous-msweb.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`topUsers_Apr-Jul_2014_1000-words.txt`__  \n",
    "Notes about the data coding:  \n",
    "> This file consists of word frequency distributions for 1,000 twitter users. These Twitter users use language in very different ways,and were classified by hand according to the criteria:  \n",
    "\n",
    ">__0__: Human, _where only basic human-human communication is observed._  \n",
    "\n",
    ">__1__: Cyborg, _where language is primarily borrowed from other sources. (e.g., jobs listings, classifieds postings, advertisements, etc...)._  \n",
    "\n",
    "> __2__: Robot, _where language is formulaically derived from unrelated sources(e.g., weather/seismology, police/fire event logs, etc...)._  \n",
    "\n",
    ">__3__: Spammer, _where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc...)_\n",
    "\n",
    "Data format:\n",
    "> `USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...`  \n",
    "where   \n",
    "USERID = unique user identifier  \n",
    "CODE = 0/1/2/3 class code  \n",
    "TOTAL = sum of the word counts  \n",
    "\n",
    "Run the cells below to load and preview the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 2493k  100 2493k    0     0   623k      0  0:00:04  0:00:04 --:--:--  691k\n"
     ]
    }
   ],
   "source": [
    "# download the main Twitter data\n",
    "!curl -L -O https://www.dropbox.com/s/6129k2urvbvobkr/topUsers_Apr-Jul_2014_1000-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180025371,2,1724608,75552,827,57603,7128,4282,45674,66811,27632,0,8,23783,2,42853,0,62335,22349,21428,19801,4125,0,0,2,1585,21118,1,1,1,16079,19676,1587,0,19695,0,0,0,0,0,0,2,20216,60,4278,0,16,46,788,2,0,0,3,0,3,0,0,111122,0,12,0,0,0,2,739,0,176,0,0,0,38,626,0,0,0,6,1584,0,19672,510,0,0,0,12,0,1675,0,0,0,0,5,2,0,0,1,9,0,0,31,0,0,2,0,0,0,0,4,64,476,0,1,0,617,0,0,15672,70315,70317,0,2997,0,0,0,665,0,0,12,0,0,0,3135,1,2,39,0,0,0,0,23,0,1,0,179,667,0,0,32,0,0,224,5,0,0,66,0,3,450,96,0,0,0,0,8,15,15,0,115,0,0,19672,0,46,15,0,0,2,0,51,0,0,0,298,0,0,5,2,165,3,0,0,46497,0,19675,0,4,0,42036,0,0,40035,84,0,103,0,2,12,1924,7,0,0,0,0,3,0,42629,197,15490,0,0,45,0,0,0,0,0,0,301,0,0,0,0,134,3300,0,422,386,0,19826,2,0,0,46,9,354,175,71,165,20338,0,109,0,1,44376,0,1370,0,0,0,0,0,0,0,0,0,0,2,0,0,4462,0,0,5,0,202,436,408,0,61,0,0,39888,74,0,19672,0,0,0,0,0,19672,19672,2,2,349,0,13,0,30,0,0,8,0,40,0,23,0,12,337,0,12,19952,26,0,0,15489,0,0,0,0,39,0,0,26,0,0,19,144,161,0,0,0,5558,0,23,1561,52,0,0,0,9,0,0,35319,0,0,68,0,0,0,0,0,0,0,8,0,0,222,463,60,0,77,0,20219,0,1,4581,0,0,0,297,0,0,0,0,0,68,0,17942,0,38,226,0,0,0,0,9,19,0,0,0,0,0,217,12,261,0,25052,263,0,0,0,1,0,59,27,14,133,76,234,24966,0,0,1,2,11,44,3,0,0,43,0,3,3,2,0,0,0,2,0,0,7,47,0,0,0,0,2,0,0,1,0,0,0,0,175,0,0,5,1,0,0,0,7,0,0,104,53,0,0,16,13,26,0,11,0,6,77,0,17,0,360,0,0,1693,0,147,0,10,0,0,0,61,1,0,0,113,0,0,1,0,305,14,23,0,42,0,0,0,1,14,2391,6,0,21,0,0,911,3,0,0,0,0,0,0,0,0,39,0,25088,0,0,0,34,134,59,35,45,1584,40,2,8,0,249,0,72,0,0,0,0,0,2,0,0,0,0,0,25,0,13,0,0,4223,30,0,0,0,0,0,0,184,1,5558,0,0,24,0,61,7,38,0,309,0,25,0,0,0,0,0,0,0,0,0,3,258,0,0,0,0,0,710,0,62,0,21,0,0,0,0,820,0,0,19672,0,1,0,0,8,74,0,0,0,87,15390,12,20216,1,0,2,10810,11,0,0,47,0,0,797,95,19826,143,0,2,772,0,0,0,117,90,0,56,1,0,0,86,382,807,0,0,0,77,0,97,0,169,282,0,0,0,0,0,0,14,1,0,0,0,0,202,825,1,10,0,803,98,0,167,113,0,263,18506,0,18521,0,243,4,88,0,46,0,0,0,0,15,0,718,1,0,0,0,0,261,0,0,0,454,3028,0,0,0,48,0,0,0,48,0,112,0,0,140,0,1,0,0,0,0,0,2701,0,0,13,0,0,178,741,0,0,1,0,0,0,0,0,352,5,0,0,42,0,0,94,0,0,0,9,0,0,0,0,2,15489,0,11,225,0,5,0,811,0,5,0,43,0,16,277,26,0,11,0,0,0,0,0,0,0,0,0,0,0,0,15489,0,0,0,0,0,0,0,0,0,0,0,95,61,9,0,70,0,28,0,0,135,0,0,0,31,12,0,0,0,0,1,0,0,1,202,0,0,0,39,0,0,125,0,0,48,0,0,24,0,13,272,48,22,0,74,0,0,0,23,0,5,0,2,21,0,0,0,69,12512,0,0,38,0,0,16,0,0,0,0,0,7,2,1,0,17,1,0,0,0,0,18,14,0,0,0,90,0,0,0,128,45,0,1,0,0,0,2,8,0,30,11,15,28,0,0,0,0,0,0,0,0,0,113,0,0,0,50,0,0,0,5,0,118,0,6,85,56,15,12,0,0,0,176,10,57,12,289,0,27,0,0,0,0,0,23,89,0,221,0,16,0,0,0,0,0,14,33,126,11,32,1,137,13,0,0,0,0,0,0,0,0,0,1,0,98,0,0,0,45,0,215,0,0,0,0,71,76,0,0,0,15,108,0,0,176,0,0,0,0,121,0,0,0,0\n"
     ]
    }
   ],
   "source": [
    "# take a look at the first row\n",
    "!head -n 1 topUsers_Apr-Jul_2014_1000-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`topUsers_Apr-Jul_2014_1000-words_summaries.txt`__\n",
    "This file contains 5 special word-frequency distributions.\n",
    "Notes about the format of this auxillary information:  \n",
    "> Row 1: Words  \n",
    "Row 2: 1000-user-wide aggregated distribution across all classes  \n",
    "Row 3-6 class-aggregated distributions for clases 0-3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 31952  100 31952    0     0  31952      0  0:00:01  0:00:01 --:--:-- 46173\n"
     ]
    }
   ],
   "source": [
    "# download auxillary file\n",
    "!curl -L -O https://www.dropbox.com/s/w4oklbsoqefou3b/topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ID\",\"CODE\",\"TOTAL_WORDS\",\"http\",\"I\",\"the\",\"to\",\"you\"\n",
      "ALL_CODES,NA,61819567,2488393,1989622,1329663,1259298,1181631\n",
      "CODE,0,35130977,449927,1668694,914155,957278,916553\n",
      "CODE,1,11423284,1239122,28497,117272,104367,10209\n",
      "CODE,2,9373246,613561,42672,191091,60120,31309\n",
      "CODE,3,5892060,185783,249759,107145,137533,223560\n"
     ]
    }
   ],
   "source": [
    "# take a look at the first few columns\n",
    "!cut -d ',' -f 1-8 topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.0 \n",
    "\n",
    "What is MrJob? How is it different to Hadoop MapReduce? \n",
    "What are the mapper_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRJob is a framework for running MR tasks in python, which allows us to specify our MR steps in a Python class. mapper_init is the method used to initiate the mapper, which we can use to set up any variables to be uesd through the mapper, mapper_final() is the method called at the end of the mapper run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.1\n",
    "- What is serialization in the context of MrJob or Hadoop? \n",
    "\n",
    "This is the process of taking structured object data and turning it into a byte stream for transmission over a network or for writing to persistent storage. The text processing in MRJob is slow because it doesn't support automatic serialization, meaning that we need to perform our own encoding to binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When it used in these frameworks? \n",
    "\n",
    "When communicating between nodes and when writing to persistent storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the default serialization mode for input and outputs for MrJob? \n",
    "\n",
    "Raw text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.2  - Preprocess log file data\n",
    "\n",
    "For this homework question we'll work with the Microsoft log files we downloaded to `anonymous-msweb.data`. Your job is to  transform/preprocess the data on a single node (i.e., not on a cluster of nodes) from the following format:\n",
    "\n",
    ">C,\"10001\",10001   #Visitor id 10001  \n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000  \n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001  \n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002  \n",
    "C,\"10002\",10002   #Visitor id 10001  \n",
    "V  \n",
    "Note: #denotes comments  \n",
    "\n",
    "\n",
    "to the following format (V, PageID, 1, C, Visitor):\n",
    "\n",
    ">V,1000,1,C, 10001  \n",
    "V,1001,1,C, 10001  \n",
    "V,1002,1,C, 10001  \n",
    "\n",
    "Write the python code to accomplish this transformation. Save your output to a file called `anonymous-msweb-preprocessed.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess-msweb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess-msweb.py\n",
    "#START STUDENT CODE42\n",
    "\n",
    "import sys\n",
    "curr_visitor = None\n",
    "file_name = sys.argv[1]\n",
    "with open(file_name) as file:\n",
    "    for line in file.readlines():\n",
    "        fields = line.split(',')\n",
    "        if fields[0] ==\"C\":\n",
    "            curr_visitor = fields[1].replace(\"\\\"\",\"\")\n",
    "        elif fields[0] == \"V\":\n",
    "            print \"V,%s,1,C,%s\" % (fields[1],curr_visitor)\n",
    "#END STUDENT CODE42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess-msweb.py anonymous-msweb.test > anonymous-msweb-preprocessed.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess-msweb.py anonymous-msweb.data > anonymous-msweb-preprocessed.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,1000,1,C,10001\n",
      "V,1001,1,C,10001\n",
      "V,1002,1,C,10001\n",
      "V,1001,1,C,10002\n",
      "V,1003,1,C,10002\n",
      "V,1001,1,C,10003\n",
      "V,1003,1,C,10003\n",
      "V,1004,1,C,10003\n",
      "V,1005,1,C,10004\n",
      "V,1006,1,C,10005\n",
      "98654 anonymous-msweb-preprocessed.data\n"
     ]
    }
   ],
   "source": [
    "# Take a look at your results\n",
    "!head -10 anonymous-msweb-preprocessed.data\n",
    "!wc -l anonymous-msweb-preprocessed.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess-att-msweb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess-att-msweb.py\n",
    "#START STUDENT CODE42\n",
    "\n",
    "import sys\n",
    "curr_visitor = None\n",
    "file_name = sys.argv[1]\n",
    "with open(file_name) as file:\n",
    "    for line in file.readlines():\n",
    "        fields = line.split(',')\n",
    "        if fields[0] ==\"A\":\n",
    "            print \"%s,%s\" % (fields[1],fields[4].replace(\"\\\"\",\"\").replace(\"\\n\",\"\"))\n",
    "#END STUDENT CODE42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287,/autoroute\n",
      "1288,/library\n",
      "1289,/masterchef\n",
      "1297,/centroam\n",
      "1215,/developer\n"
     ]
    }
   ],
   "source": [
    "!python preprocess-att-msweb.py anonymous-msweb.data > anonymous-attr-preprocessed.data\n",
    "!head -n 5 anonymous-attr-preprocessed.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test43.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile test43.data\n",
    "V,1000,1,C,10001\n",
    "V,1001,1,C,10001\n",
    "V,1002,1,C,10001\n",
    "V,1001,1,C,10002\n",
    "V,1003,1,C,10002\n",
    "V,1001,1,C,10003\n",
    "V,1003,1,C,10003\n",
    "V,1004,1,C,10003\n",
    "V,1005,1,C,10004\n",
    "V,1006,1,C,10005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting firstmr.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile firstmr.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        print \"reducer init\"\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/firstmr.root.20180203.185622.715808\n",
      "Running step 1 of 1...\n",
      "reducer init\n",
      "reducer init\n",
      "Streaming final output from /tmp/firstmr.root.20180203.185622.715808/output...\n",
      "\"chars\"\t80\n",
      "\"lines\"\t12\n",
      "\"words\"\t46\n",
      "Removing temp directory /tmp/firstmr.root.20180203.185622.715808...\n"
     ]
    }
   ],
   "source": [
    "!python firstmr.py sometext.txt sometext.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting secondmr.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile secondmr.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import logging\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "\n",
    "    JOBCONF = {\"mapreduce.job.reduces\": \"1\"}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRWordFreqCount, self).__init__(*args,**kwargs)\n",
    "        \n",
    "    def init_get_words(self):\n",
    "        self.words = {}\n",
    "\n",
    "    def get_words(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            word = word.lower()\n",
    "            self.words.setdefault(word, 0)\n",
    "            self.words[word] = self.words[word] + 1\n",
    "\n",
    "    def final_get_words(self):\n",
    "        for word, val in self.words.iteritems():\n",
    "            yield word, val\n",
    "    \n",
    "    def sum_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "    \n",
    "    def map_max_words(self, word, count):\n",
    "        logging.warning(word)\n",
    "        logging.warning(count)\n",
    "        yield word, int(count)\n",
    "        \n",
    "    def reduce_max_words(self, word, count):\n",
    "        yield word, sum(count)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.init_get_words,\n",
    "                       mapper=self.get_words,\n",
    "                       mapper_final=self.final_get_words,\n",
    "                       reducer=self.sum_words),\n",
    "               MRStep(mapper = self.map_max_words,\n",
    "                      reducer = self.reduce_max_words,\n",
    "                        jobconf={\n",
    "                        \"mapreduce.job.reduces\": \"1\",\n",
    "                        \"stream.num.map.output.key.fields\": 2,\n",
    "                        \"mapreduce.job.output.key.comparator.class\" : \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                        \"mapreduce.partition.keycomparator.options\":\"-k2,2nr\",\n",
    "                        \"mapred.num.key.comparator.options\":\"-k2,2nr\",\n",
    "                        \"mapred.text.key.comparator.options\": \"-k2,2nr\",\n",
    "                        \"SORT_VALUES\":True\n",
    "                   })\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting thirdmr.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile thirdmr.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "\n",
    "    JOBCONF = {\"mapreduce.job.reduces\": \"1\"}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRWordFreqCount, self).__init__(*args,**kwargs)\n",
    "    \n",
    "    def map_max_words(self, _, line):\n",
    "        word, count = line.split()\n",
    "        yield word, int(count)\n",
    "        \n",
    "    def reduce_max_words(self, word, count):\n",
    "        yield word, count.next()\n",
    "        \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper = self.map_max_words,\n",
    "                      reducer = self.reduce_max_words,\n",
    "                        jobconf={\n",
    "                        \"mapreduce.job.reduces\": \"1\",\n",
    "                        \"stream.num.map.output.key.fields\": 2,\n",
    "                         \"mapreduce.job.output.key.comparator.class\" : \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                        \"mapreduce.partition.keycomparator.options\":\"-k2,2nr\",\n",
    "                        \"mapred.num.key.comparator.options\":\"-k2,2nr\",\n",
    "#                       \"mapred.text.key.comparator.options\": \"-k2,2nr\",\n",
    "                        \"SORT_VALUES\":True\n",
    "                   })\n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sometext.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sometext.txt\n",
    "z z z\n",
    "a a b a\n",
    "b c c d\n",
    "z c a z\n",
    "z z z z\n",
    "c c z z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting res.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile res.txt\n",
    "a 4\n",
    "b 5\n",
    "c 1\n",
    "z 11\n",
    "d 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/secondmr.root.20180203.172738.102438\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/secondmr.root.20180203.172738.102438/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2197694743868221798.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1517652631830_0017\n",
      "  Submitted application application_1517652631830_0017\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1517652631830_0017/\n",
      "  Running job: job_1517652631830_0017\n",
      "  Job job_1517652631830_0017 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1517652631830_0017 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/secondmr.root.20180203.172738.102438/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=68\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=31\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=69\n",
      "\t\tFILE: Number of bytes written=355148\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=386\n",
      "\t\tHDFS: Number of bytes written=31\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6735872\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2841600\n",
      "\t\tTotal time spent by all map tasks (ms)=6578\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6578\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2775\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2775\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6578\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2775\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1400\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=104\n",
      "\t\tInput split bytes=318\n",
      "\t\tMap input records=6\n",
      "\t\tMap output bytes=49\n",
      "\t\tMap output materialized bytes=75\n",
      "\t\tMap output records=7\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=743841792\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=5\n",
      "\t\tReduce shuffle bytes=75\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=14\n",
      "\t\tTotal committed heap usage (bytes)=552599552\n",
      "\t\tVirtual memory (bytes) snapshot=4084174848\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8591660935218899365.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1517652631830_0018\n",
      "  Submitted application application_1517652631830_0018\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1517652631830_0018/\n",
      "  Running job: job_1517652631830_0018\n",
      "  Job job_1517652631830_0018 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1517652631830_0018 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/secondmr.root.20180203.172738.102438/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=47\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=31\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=52\n",
      "\t\tFILE: Number of bytes written=356974\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=383\n",
      "\t\tHDFS: Number of bytes written=31\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7870464\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2886656\n",
      "\t\tTotal time spent by all map tasks (ms)=7686\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7686\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2819\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2819\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7686\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2819\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1380\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=74\n",
      "\t\tInput split bytes=336\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=36\n",
      "\t\tMap output materialized bytes=58\n",
      "\t\tMap output records=5\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=760791040\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=5\n",
      "\t\tReduce shuffle bytes=58\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=10\n",
      "\t\tTotal committed heap usage (bytes)=619184128\n",
      "\t\tVirtual memory (bytes) snapshot=4088250368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/secondmr.root.20180203.172738.102438/output...\n",
      "\"z\"\t11\n",
      "\"c\"\t5\n",
      "\"a\"\t4\n",
      "\"b\"\t2\n",
      "\"d\"\t1\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/secondmr.root.20180203.172738.102438...\n",
      "Removing temp directory /tmp/secondmr.root.20180203.172738.102438...\n"
     ]
    }
   ],
   "source": [
    "!python secondmr.py sometext.txt -r hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/thirdmr.root.20180203.171336.602585\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/thirdmr.root.20180203.171336.602585/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7849667324169741335.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1517652631830_0008\n",
      "  Submitted application application_1517652631830_0008\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1517652631830_0008/\n",
      "  Running job: job_1517652631830_0008\n",
      "  Job job_1517652631830_0008 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1517652631830_0008 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/thirdmr.root.20180203.171336.602585/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=12\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=72\n",
      "\t\tFILE: Number of bytes written=356960\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=336\n",
      "\t\tHDFS: Number of bytes written=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7597056\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3845120\n",
      "\t\tTotal time spent by all map tasks (ms)=7419\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7419\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3755\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3755\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7419\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3755\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1620\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=117\n",
      "\t\tInput split bytes=306\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=56\n",
      "\t\tMap output materialized bytes=78\n",
      "\t\tMap output records=5\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=760156160\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=78\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=10\n",
      "\t\tTotal committed heap usage (bytes)=552075264\n",
      "\t\tVirtual memory (bytes) snapshot=4100775936\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/thirdmr.root.20180203.171336.602585/output...\n",
      "null\t\"z 11\"\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/thirdmr.root.20180203.171336.602585...\n",
      "Removing temp directory /tmp/thirdmr.root.20180203.171336.602585...\n"
     ]
    }
   ],
   "source": [
    "!python thirdmr.py res.txt -r hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.3 - Find the most frequent pages\n",
    "\n",
    "Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostFrequentVisits.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MostFrequentVisits.py\n",
    "#!/usr/bin/env python\n",
    "#START STUDENT CODE43\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRPageCount(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    JOBCONF = {\"mapreduce.job.reduces\": \"1\"}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRPageCount, self).__init__(*args,**kwargs)\n",
    "        \n",
    "    def mapper_pages(self, _, line):\n",
    "        fields = line.split(\",\")\n",
    "        yield fields[1], int(fields[2])\n",
    "\n",
    "    def reducer_pages(self, page, visits):\n",
    "#         yield None, (sum(visits), page)\n",
    "        yield page, sum(visits)\n",
    "    \n",
    "#     def reducer_find_most_freq_pages(self, _,page_freq_pairs):\n",
    "#         for pair in sorted(page_freq_pairs, reverse=True)[:5]:\n",
    "#             yield pair\n",
    "    \n",
    "    def mapper_most(self, page, count):\n",
    "        yield page, int(count)\n",
    "        \n",
    "    def reducer_most(self, page, count):\n",
    "        yield page, sum(count)\n",
    "            \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper = self.mapper_pages,\n",
    "                  reducer = self.reducer_pages),\n",
    "            MRStep(\n",
    "                mapper = self.mapper_most,\n",
    "                reducer = self.reducer_most, \n",
    "                jobconf={\n",
    "                        \"mapreduce.job.reduces\": \"1\",\n",
    "                        \"stream.num.map.output.key.fields\": 2,\n",
    "                        \"mapreduce.job.output.key.comparator.class\" : \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                        #\"mapreduce.partition.keycomparator.options\":\"-k2,2nr\",\n",
    "                        #\"mapred.num.key.comparator.options\":\"-k2,2nr\",\n",
    "                        \"mapred.text.key.comparator.options\": \"-k2,2nr\",\n",
    "                        \"SORT_VALUES\":True\n",
    "                   }\n",
    "            )\n",
    "        ]\n",
    "if __name__ == \"__main__\":\n",
    "    MRPageCount.run()\n",
    "#END STUDENT CODE43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x MostFrequentVisits.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'1008', 10836)\n",
      "(u'1034', 9383)\n",
      "(u'1004', 8463)\n",
      "(u'1018', 5330)\n",
      "(u'1017', 5108)\n"
     ]
    }
   ],
   "source": [
    "from MostFrequentVisits import MRPageCount\n",
    "mr_job = MRPageCount(args=[\"anonymous-msweb-preprocessed.data\",\"-r\", \"hadoop\"])\n",
    "count = 0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)\n",
    "        count  +=1\n",
    "        if count >=5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/MostFrequentVisits.root.20180203.194413.518476\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/MostFrequentVisits.root.20180203.194413.518476/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7877103873928319542.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1517652631830_0031\n",
      "  Submitted application application_1517652631830_0031\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1517652631830_0031/\n",
      "  Running job: job_1517652631830_0031\n",
      "  Job job_1517652631830_0031 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1517652631830_0031 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/MostFrequentVisits.root.20180203.194413.518476/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1681214\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2903\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1183854\n",
      "\t\tFILE: Number of bytes written=2723231\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1681596\n",
      "\t\tHDFS: Number of bytes written=2903\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9966592\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4592640\n",
      "\t\tTotal time spent by all map tasks (ms)=9733\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9733\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4485\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4485\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9733\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4485\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3410\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=90\n",
      "\t\tInput split bytes=382\n",
      "\t\tMap input records=98654\n",
      "\t\tMap output bytes=986540\n",
      "\t\tMap output materialized bytes=1183860\n",
      "\t\tMap output records=98654\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=771309568\n",
      "\t\tReduce input groups=285\n",
      "\t\tReduce input records=98654\n",
      "\t\tReduce output records=285\n",
      "\t\tReduce shuffle bytes=1183860\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=197308\n",
      "\t\tTotal committed heap usage (bytes)=685244416\n",
      "\t\tVirtual memory (bytes) snapshot=4105240576\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "Running step 2 of 2...\n",
      "  mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2101642672992434153.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1517652631830_0032\n",
      "  Submitted application application_1517652631830_0032\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1517652631830_0032/\n",
      "  Running job: job_1517652631830_0032\n",
      "  Job job_1517652631830_0032 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1517652631830_0032 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/MostFrequentVisits.root.20180203.194413.518476/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4355\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2903\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3764\n",
      "\t\tFILE: Number of bytes written=364404\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4711\n",
      "\t\tHDFS: Number of bytes written=2903\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8003584\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4013056\n",
      "\t\tTotal time spent by all map tasks (ms)=7816\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7816\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3919\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3919\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7816\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3919\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1570\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=46\n",
      "\t\tInput split bytes=356\n",
      "\t\tMap input records=285\n",
      "\t\tMap output bytes=3188\n",
      "\t\tMap output materialized bytes=3770\n",
      "\t\tMap output records=285\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=735031296\n",
      "\t\tReduce input groups=285\n",
      "\t\tReduce input records=285\n",
      "\t\tReduce output records=285\n",
      "\t\tReduce shuffle bytes=3770\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=570\n",
      "\t\tTotal committed heap usage (bytes)=548405248\n",
      "\t\tVirtual memory (bytes) snapshot=4110348288\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/MostFrequentVisits.root.20180203.194413.518476/output...\n",
      "\"1008\"\t10836\n",
      "\"1034\"\t9383\n",
      "\"1004\"\t8463\n",
      "\"1018\"\t5330\n",
      "\"1017\"\t5108\n",
      "\"1009\"\t4628\n",
      "\"1001\"\t4451\n",
      "\"1026\"\t3220\n",
      "\"1003\"\t2968\n",
      "\"1025\"\t2123\n",
      "\"1035\"\t1791\n",
      "\"1040\"\t1506\n",
      "\"1041\"\t1500\n",
      "\"1032\"\t1446\n",
      "\"1037\"\t1160\n",
      "\"1030\"\t1115\n",
      "\"1038\"\t1110\n",
      "\"1020\"\t1087\n",
      "\"1000\"\t912\n",
      "\"1007\"\t865\n",
      "\"1052\"\t842\n",
      "\"1036\"\t759\n",
      "\"1002\"\t749\n",
      "\"1014\"\t728\n",
      "\"1295\"\t716\n",
      "\"1010\"\t698\n",
      "\"1058\"\t672\n",
      "\"1053\"\t670\n",
      "\"1046\"\t636\n",
      "\"1070\"\t602\n",
      "\"1074\"\t584\n",
      "\"1031\"\t574\n",
      "\"1067\"\t548\n",
      "\"1024\"\t521\n",
      "\"1027\"\t507\n",
      "\"1045\"\t474\n",
      "\"1078\"\t462\n",
      "\"1076\"\t444\n",
      "\"1075\"\t396\n",
      "\"1130\"\t395\n",
      "\"1060\"\t391\n",
      "\"1021\"\t380\n",
      "\"1123\"\t372\n",
      "\"1119\"\t365\n",
      "\"1039\"\t345\n",
      "\"1049\"\t343\n",
      "\"1054\"\t338\n",
      "\"1022\"\t325\n",
      "\"1064\"\t324\n",
      "\"1065\"\t323\n",
      "\"1100\"\t291\n",
      "\"1016\"\t287\n",
      "\"1042\"\t281\n",
      "\"1056\"\t276\n",
      "\"1061\"\t269\n",
      "\"1055\"\t264\n",
      "\"1059\"\t258\n",
      "\"1082\"\t241\n",
      "\"1088\"\t237\n",
      "\"1069\"\t227\n",
      "\"1043\"\t224\n",
      "\"1124\"\t222\n",
      "\"1081\"\t215\n",
      "\"1096\"\t214\n",
      "\"1048\"\t210\n",
      "\"1073\"\t204\n",
      "\"1125\"\t199\n",
      "\"1068\"\t198\n",
      "\"1057\"\t195\n",
      "\"1023\"\t191\n",
      "\"1087\"\t189\n",
      "\"1071\"\t187\n",
      "\"1084\"\t186\n",
      "\"1105\"\t183\n",
      "\"1113\"\t181\n",
      "\"1136\"\t181\n",
      "\"1011\"\t179\n",
      "\"1118\"\t172\n",
      "\"1044\"\t168\n",
      "\"1183\"\t167\n",
      "\"1134\"\t162\n",
      "\"1089\"\t157\n",
      "\"1077\"\t155\n",
      "\"1131\"\t148\n",
      "\"1062\"\t141\n",
      "\"1079\"\t136\n",
      "\"1006\"\t135\n",
      "\"1127\"\t132\n",
      "\"1029\"\t132\n",
      "\"1112\"\t128\n",
      "\"1072\"\t128\n",
      "\"1157\"\t124\n",
      "\"1137\"\t123\n",
      "\"1080\"\t121\n",
      "\"1099\"\t120\n",
      "\"1102\"\t118\n",
      "\"1140\"\t118\n",
      "\"1135\"\t115\n",
      "\"1063\"\t113\n",
      "\"1019\"\t111\n",
      "\"1090\"\t107\n",
      "\"1050\"\t106\n",
      "\"1083\"\t105\n",
      "\"1095\"\t102\n",
      "\"1098\"\t98\n",
      "\"1092\"\t97\n",
      "\"1148\"\t96\n",
      "\"1188\"\t94\n",
      "\"1150\"\t93\n",
      "\"1168\"\t93\n",
      "\"1028\"\t93\n",
      "\"1158\"\t90\n",
      "\"1147\"\t86\n",
      "\"1051\"\t86\n",
      "\"1085\"\t86\n",
      "\"1066\"\t82\n",
      "\"1015\"\t79\n",
      "\"1146\"\t79\n",
      "\"1156\"\t75\n",
      "\"1167\"\t72\n",
      "\"1091\"\t69\n",
      "\"1133\"\t69\n",
      "\"1154\"\t67\n",
      "\"1093\"\t65\n",
      "\"1121\"\t63\n",
      "\"1176\"\t63\n",
      "\"1013\"\t61\n",
      "\"1143\"\t60\n",
      "\"1109\"\t59\n",
      "\"1184\"\t57\n",
      "\"1097\"\t56\n",
      "\"1203\"\t55\n",
      "\"1155\"\t52\n",
      "\"1152\"\t52\n",
      "\"1189\"\t51\n",
      "\"1164\"\t49\n",
      "\"1190\"\t48\n",
      "\"1162\"\t48\n",
      "\"1114\"\t48\n",
      "\"1108\"\t47\n",
      "\"1171\"\t47\n",
      "\"1186\"\t46\n",
      "\"1110\"\t46\n",
      "\"1172\"\t45\n",
      "\"1215\"\t45\n",
      "\"1169\"\t44\n",
      "\"1012\"\t44\n",
      "\"1177\"\t43\n",
      "\"1005\"\t42\n",
      "\"1159\"\t41\n",
      "\"1165\"\t38\n",
      "\"1187\"\t38\n",
      "\"1201\"\t38\n",
      "\"1141\"\t36\n",
      "\"1160\"\t36\n",
      "\"1144\"\t36\n",
      "\"1111\"\t36\n",
      "\"1103\"\t36\n",
      "\"1104\"\t35\n",
      "\"1208\"\t34\n",
      "\"1166\"\t33\n",
      "\"1138\"\t33\n",
      "\"1197\"\t32\n",
      "\"1227\"\t32\n",
      "\"1116\"\t31\n",
      "\"1204\"\t30\n",
      "\"1216\"\t30\n",
      "\"1223\"\t29\n",
      "\"1206\"\t29\n",
      "\"1126\"\t27\n",
      "\"1033\"\t26\n",
      "\"1194\"\t26\n",
      "\"1163\"\t25\n",
      "\"1212\"\t25\n",
      "\"1193\"\t25\n",
      "\"1185\"\t24\n",
      "\"1205\"\t24\n",
      "\"1220\"\t23\n",
      "\"1132\"\t23\n",
      "\"1086\"\t22\n",
      "\"1151\"\t21\n",
      "\"1230\"\t21\n",
      "\"1145\"\t20\n",
      "\"1231\"\t19\n",
      "\"1142\"\t19\n",
      "\"1218\"\t18\n",
      "\"1198\"\t18\n",
      "\"1200\"\t18\n",
      "\"1139\"\t18\n",
      "\"1211\"\t16\n",
      "\"1170\"\t16\n",
      "\"1224\"\t16\n",
      "\"1161\"\t16\n",
      "\"1106\"\t16\n",
      "\"1115\"\t15\n",
      "\"1195\"\t15\n",
      "\"1226\"\t14\n",
      "\"1101\"\t14\n",
      "\"1094\"\t14\n",
      "\"1122\"\t13\n",
      "\"1250\"\t13\n",
      "\"1228\"\t13\n",
      "\"1217\"\t13\n",
      "\"1181\"\t12\n",
      "\"1207\"\t12\n",
      "\"1149\"\t12\n",
      "\"1179\"\t11\n",
      "\"1251\"\t11\n",
      "\"1222\"\t11\n",
      "\"1240\"\t11\n",
      "\"1107\"\t11\n",
      "\"1236\"\t10\n",
      "\"1174\"\t10\n",
      "\"1221\"\t10\n",
      "\"1246\"\t10\n",
      "\"1209\"\t9\n",
      "\"1241\"\t9\n",
      "\"1235\"\t9\n",
      "\"1180\"\t9\n",
      "\"1117\"\t9\n",
      "\"1234\"\t8\n",
      "\"1153\"\t8\n",
      "\"1182\"\t7\n",
      "\"1225\"\t7\n",
      "\"1192\"\t7\n",
      "\"1129\"\t7\n",
      "\"1175\"\t6\n",
      "\"1210\"\t5\n",
      "\"1267\"\t5\n",
      "\"1257\"\t5\n",
      "\"1253\"\t5\n",
      "\"1232\"\t4\n",
      "\"1242\"\t4\n",
      "\"1191\"\t4\n",
      "\"1202\"\t4\n",
      "\"1238\"\t4\n",
      "\"1244\"\t4\n",
      "\"1264\"\t4\n",
      "\"1262\"\t4\n",
      "\"1243\"\t4\n",
      "\"1219\"\t4\n",
      "\"1229\"\t4\n",
      "\"1255\"\t3\n",
      "\"1213\"\t3\n",
      "\"1214\"\t3\n",
      "\"1173\"\t3\n",
      "\"1237\"\t3\n",
      "\"1239\"\t3\n",
      "\"1247\"\t3\n",
      "\"1252\"\t3\n",
      "\"1256\"\t3\n",
      "\"1258\"\t3\n",
      "\"1265\"\t3\n",
      "\"1276\"\t3\n",
      "\"1249\"\t2\n",
      "\"1278\"\t2\n",
      "\"1280\"\t2\n",
      "\"1261\"\t2\n",
      "\"1263\"\t2\n",
      "\"1266\"\t2\n",
      "\"1269\"\t2\n",
      "\"1178\"\t2\n",
      "\"1245\"\t2\n",
      "\"1282\"\t2\n",
      "\"1199\"\t1\n",
      "\"1233\"\t1\n",
      "\"1196\"\t1\n",
      "\"1284\"\t1\n",
      "\"1248\"\t1\n",
      "\"1268\"\t1\n",
      "\"1279\"\t1\n",
      "\"1270\"\t1\n",
      "\"1271\"\t1\n",
      "\"1272\"\t1\n",
      "\"1273\"\t1\n",
      "\"1254\"\t1\n",
      "\"1274\"\t1\n",
      "\"1277\"\t1\n",
      "\"1283\"\t1\n",
      "\"1281\"\t1\n",
      "\"1259\"\t1\n",
      "\"1260\"\t1\n",
      "\"1275\"\t1\n",
      "\"1128\"\t1\n",
      "\"1120\"\t1\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/MostFrequentVisits.root.20180203.194413.518476...\n",
      "Removing temp directory /tmp/MostFrequentVisits.root.20180203.194413.518476...\n"
     ]
    }
   ],
   "source": [
    "!python MostFrequentVisits.py anonymous-msweb-preprocessed.data -r hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.4  - Find the most frequent visitor\n",
    "\n",
    "Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.  You may get a weird result.  HINT: The maximum visits by any visitor to any given webpage is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAADQCAYAAAFdDMvvAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5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\nIAiCOCX/PwD2h+6Lr7Jp0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('ms-data.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mostFrequentVisitors.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mostFrequentVisitors.py\n",
    "#!/usr/bin/env python\n",
    "#START STUDENT CODE44\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import logging\n",
    "class MRVisitorCount(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    JOBCONF = {\"mapreduce.job.reduces\": \"1\"}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRVisitorCount, self).__init__(*args,**kwargs)\n",
    "        \n",
    "    def mapper_visits(self, _, line):\n",
    "        fields = line.split(\",\")\n",
    "#         if len(fields) == 5:\n",
    "#             yield (fields[1],\"C\",fields[4]),1\n",
    "#         elif len(fields) == 2:\n",
    "#             yield (fields[0],\"A\",fields[1]),1\n",
    "        if len(fields) == 5:\n",
    "            yield int(fields[1]), (\"C\",fields[4], 1)\n",
    "        elif len(fields) == 2:\n",
    "            yield int(fields[0]), (\"A\",fields[1], 1)\n",
    "\n",
    "    def reducer_visits(self, page_id, visits):\n",
    "        visits = list(visits)\n",
    "        logging.warning(page_id)\n",
    "        logging.warning(visits)\n",
    "        page_path = None\n",
    "        curr_cust = None\n",
    "        curr_count = 0\n",
    "        max_cust = None\n",
    "        max_count = 0\n",
    "        for val in visits:\n",
    "            if val[0] == \"A\":\n",
    "                page_path = val[1]\n",
    "            elif curr_cust == val[1]:\n",
    "                curr_count +=val[2]\n",
    "            else:\n",
    "                if curr_cust:\n",
    "                    if curr_count > max_count:\n",
    "                        max_cust = curr_cust\n",
    "                        max_count = curr_count\n",
    "                curr_cust = val[1]\n",
    "                curr_count = val[2]\n",
    "        if curr_count > max_count:\n",
    "            max_cust = curr_cust\n",
    "            max_count = curr_count\n",
    "        logging.warning( \"end for \"+str(page_id))\n",
    "        yield page_path, (page_id, max_cust, max_count)\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper = self.mapper_visits,\n",
    "                   reducer = self.reducer_visits,\n",
    "                   jobconf={\n",
    "                       \"mapreduce.job.reduces\": \"1\",\n",
    "                       \"stream.num.map.output.key.fields\": 3,\n",
    "                       \"mapreduce.job.output.key.comparator.class\" : \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                       \"mapreduce.partition.keycomparator.options\":\"-k1,1 -k2,2 -k3,3n\",\n",
    "                       \"SORT_VALUES\":True\n",
    "                   }\n",
    "                  )\n",
    "        ]\n",
    "if __name__ == \"__main__\":\n",
    "    MRVisitorCount.run()\n",
    "#END STUDENT CODE44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mostFrequentVisitors.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a test on some data that has multiple visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.data\n",
    "V,1000,1,C,10001\n",
    "V,1001,1,C,10001\n",
    "V,1001,1,C,10001\n",
    "V,1001,1,C,10001\n",
    "V,1001,1,C,10002\n",
    "V,1002,1,C,10001\n",
    "V,1001,1,C,10002\n",
    "V,1003,1,C,10002\n",
    "V,1001,1,C,10003\n",
    "V,1003,1,C,10003\n",
    "V,1004,1,C,10003\n",
    "V,1005,1,C,10004\n",
    "V,1006,1,C,10005\n",
    "V,1006,1,C,10005\n",
    "V,1006,1,C,10005\n",
    "V,1006,1,C,10005\n",
    "V,1006,1,C,10003\n",
    "V,1006,1,C,10003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test-att.data\n"
     ]
    }
   ],
   "source": [
    "%%writefile test-att.data\n",
    "1000,/regwiz\n",
    "1001,/support\n",
    "1002,/athome\n",
    "1003,/kb\n",
    "1004,/search\n",
    "1005,/norge\n",
    "1006,/misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287,/autoroute\n",
      "1288,/library\n",
      "1289,/masterchef\n",
      "1297,/centroam\n",
      "1215,/developer\n"
     ]
    }
   ],
   "source": [
    "!cat anonymous-attr-preprocessed.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output on test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "            URL |    PageID |    CustID | Num Visits\n",
    "----------------------------------------------------------------------\n",
    "      \"/regwiz\" |      1000 |     10001 |         1\n",
    "     \"/support\" |      1001 |     10001 |         3\n",
    "      \"/athome\" |      1002 |     10001 |         1\n",
    "          \"/kb\" |      1003 |     10002 |         1\n",
    "      \"/search\" |      1004 |     10003 |         1\n",
    "       \"/norge\" |      1005 |     10004 |         1\n",
    "        \"/misc\" |      1006 |     10005 |         4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/mostFrequentVisitors.root.20180203.194825.277359\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/mostFrequentVisitors.root.20180203.194825.277359/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8950375242425686798.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1517652631830_0033\n",
      "  Submitted application application_1517652631830_0033\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1517652631830_0033/\n",
      "  Running job: job_1517652631830_0033\n",
      "  Job job_1517652631830_0033 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 33% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1517652631830_0033 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mostFrequentVisitors.root.20180203.194825.277359/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=500\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=197\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=664\n",
      "\t\tFILE: Number of bytes written=477725\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1008\n",
      "\t\tHDFS: Number of bytes written=197\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=3\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=21380096\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3509248\n",
      "\t\tTotal time spent by all map tasks (ms)=20879\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=20879\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3427\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3427\n",
      "\t\tTotal vcore-seconds taken by all map tasks=20879\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3427\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1840\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=108\n",
      "\t\tInput split bytes=508\n",
      "\t\tMap input records=25\n",
      "\t\tMap output bytes=608\n",
      "\t\tMap output materialized bytes=676\n",
      "\t\tMap output records=25\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPhysical memory (bytes) snapshot=974467072\n",
      "\t\tReduce input groups=18\n",
      "\t\tReduce input records=25\n",
      "\t\tReduce output records=7\n",
      "\t\tReduce shuffle bytes=676\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=50\n",
      "\t\tTotal committed heap usage (bytes)=743440384\n",
      "\t\tVirtual memory (bytes) snapshot=5444448256\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/mostFrequentVisitors.root.20180203.194825.277359/output...\n",
      "\"/regwiz\"\t[1000, \"10001\", 1]\n",
      "\"/support\"\t[1001, \"10001\", 3]\n",
      "\"/athome\"\t[1002, \"10001\", 1]\n",
      "\"/kb\"\t[1003, \"10002\", 1]\n",
      "\"/search\"\t[1004, \"10003\", 1]\n",
      "\"/norge\"\t[1005, \"10004\", 1]\n",
      "\"/misc\"\t[1006, \"10005\", 4]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mostFrequentVisitors.root.20180203.194825.277359...\n",
      "Removing temp directory /tmp/mostFrequentVisitors.root.20180203.194825.277359...\n"
     ]
    }
   ],
   "source": [
    "!python mostFrequentVisitors.py test.data test-att.data -r hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'/regwiz', [1000, u'10001', 1])\n",
      "(u'/support', [1001, u'10001', 3])\n",
      "(u'/athome', [1002, u'10001', 1])\n",
      "(u'/kb', [1003, u'10002', 1])\n",
      "(u'/search', [1004, u'10003', 1])\n",
      "(u'/norge', [1005, u'10004', 1])\n",
      "(u'/misc', [1006, u'10005', 4])\n"
     ]
    }
   ],
   "source": [
    "from mostFrequentVisitors import MRVisitorCount\n",
    "mr_job = MRVisitorCount(args=[\"test.data\",\"test-att.data\",\"-r\", \"hadoop\"])\n",
    "count = 0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mostFrequentVisitors import MRVisitorCount\n",
    "mr_job = MRVisitorCount(args=[\"anonymous-msweb-preprocessed.data\",\"anonymous-attr-preprocessed.data\",\"-r\", \"hadoop\"])\n",
    "count = 0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample Output:\n",
    "(u'/regwiz', [1000, u'10001', 1])\n",
    "(u'/support', [1001, u'10001', 1])\n",
    "(u'/athome', [1002, u'10001', 1])\n",
    "(u'/kb', [1003, u'10002', 1])\n",
    "(u'/search', [1004, u'10003', 1])\n",
    "(u'/norge', [1005, u'10004', 1])\n",
    "(u'/misc', [1006, u'10005', 1])\n",
    "(u'/ie_intl', [1007, u'10007', 1])\n",
    "(u'/msdownload', [1008, u'10009', 1])\n",
    "(u'/windows', [1009, u'10009', 1])\n",
    "(u'/vbasic', [1010, u'10010', 1])\n",
    "(u'/officedev', [1011, u'10010', 1])\n",
    "(u'/outlookdev', [1012, u'10010', 1])\n",
    "(u'/vbasicsupport', [1013, u'10010', 1])\n",
    "(u'/officefreestuff', [1014, u'10010', 1])\n",
    "(u'/msexcel', [1015, u'10011', 1])\n",
    "(u'/excel', [1016, u'10011', 1])\n",
    "(u'/products', [1017, u'10011', 1])\n",
    "(u'/isapi', [1018, u'10011', 1])\n",
    "(u'/mspowerpoint', [1019, u'10011', 1])\n",
    "(u'/msdn', [1020, u'10012', 1])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.5 - Clustering Tweet Dataset\n",
    "\n",
    "For this question use the Tweet data in `topUsers_Apr-Jul_2014_1000-words.txt`, you will implement a 1000-dimensional K-means algorithm in MrJob on the users by their 1000-dimensional word stripes/vectors using several centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that word-frequency distributions are generally heavy-tailed power-laws(often called Zipf distributions), and are very rare in the larger class of discrete, random distributions. For each user you will have to normalize by its \"TOTAL\" column. __Try several parameterizations and initializations__ :\n",
    "\n",
    "* (A) K=4 uniform random centroid-distributions over the 1000 words (generate 1000 random numbers and normalize the vectors)\n",
    "* (B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "* (C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "* (D) K=4 \"trained\" centroids, determined by the sums across the classes. Use use the \n",
    "(row-normalized) class-level aggregates as 'trained' starting centroids (i.e., the training is already done for you!).\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the class-aggregated distributions, which are rows in the auxiliary file `topUsers_Apr-Jul_2014_1000-words_summaries.txt`. \n",
    "\n",
    "For (A),  we select 4 users randomly from a uniform distribution [1,...,1,000]. For (B), (C), and (D)  you will have to use data from the auxiliary file. In parts (B) and (C), you will have to perturb the 1000-user aggregate (after initially normalizing by its sum, which is also provided). So if in (B) you want to create 2 perturbations of the aggregate, startwith (1), normalize, and generate 1000 random numbers uniformly from the unit interval (0,1) twice (for two centroids), using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "numbers = random.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take these 1000 numbers and add them (component-wise) to the 1000-user aggregate,\n",
    "and then renormalize to obtain one of your aggregate-perturbed initial centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0025991419968166979, 0.00095450305532186557, 0.0018325150675689932, 0.0021479808130179944, 0.0014880791052683789, 0.00085726016935641874, 0.00096945462232145025, 0.00043376266809876286, 0.00074180035979351956, 0.0018990374762636482, 0.0019574914497546732, 0.0017484478420545669, 0.00051446909478588805, 0.0014449007356082582, 0.00041543981365194392, 0.00027035097011570618, 0.0012984492855712853, 0.0019012199475215428, 0.0014989756260885959, 0.00081164487741945083, 0.00197773003623201, 0.0019404095657609129, 0.00018041191151372442, 0.0019644670898236556, 0.00051862443555590024, 0.00091358774923010655, 0.00039166718323870503, 0.00047898504984666496, 0.0010992483725340515, 0.00032279078090707332, 0.00084461825423882628, 0.0017906081607582619, 0.00085115026823042676, 0.00034789122537766555, 0.0004519231576385138, 0.001787950889670099, 0.0018153900512168258, 0.00085312360706076809, 0.00084644458947111039, 7.4992618090290717e-05, 0.0018110688568119663, 0.00015096727951375031, 0.00019887554948895696, 0.0016509204069764863, 0.0012098830977143756, 0.0015168546565521657, 0.0015168808374095885, 0.0011347975293940901, 0.0016735914540416427, 0.0013160669615846115, 0.0013946413378477793, 0.0016823711655151792, 0.00063661894726129245, 0.0011399269451558979, 0.0018646384958815334, 0.00094287248029682519, 0.00078820141082910325, 0.0012527641402621352, 0.001134384926484933, 0.0015380958938099024, 0.00092707784408907002, 0.0013203876752422719, 0.0018400797039107534, 0.00063604567345650629, 0.0015985736667527472, 0.0017230083003555753, 0.0018382687060967771, 0.0018427670820168791, 0.0018831330575837535, 0.0005195815898081977, 0.0011467943405703909, 0.0016585270213296227, 0.0011113773958690682, 0.00043781435837040325, 0.001877319974782365, 0.0015097707158505602, 0.0017621466287260185, 0.00023176297911584463, 0.00038632930017449082, 0.0013952682585647411, 0.00047633774462000809, 0.001332312488909535, 0.0018442027700626298, 0.00097519577052914037, 0.00067104674672974478, 0.000892273252779432, 0.0014864762028047229, 0.0016593042599909783, 0.00027015290236811557, 8.2197750159649784e-05, 0.001798169855116391, 0.00092715810754680987, 0.0016516274908794045, 0.0012147750557217442, 5.4673227664998686e-05, 0.00024483899559847236, 0.0016502663809134376, 0.0017731483291796631, 0.00046118053673352964, 0.0013515317889675794, 0.00084109970725567665, 0.0012878608706538726, 0.0012467692136985834, 0.00020137304543865297, 0.00063381931705206373, 0.0017476559824562163, 0.001912031604286196, 0.0017626118206703362, 0.00043678582163030229, 4.4844291349062476e-05, 0.00126751052651282, 0.0012790974381657646, 0.0013647680043093516, 0.0018587659173065728, 0.0011354541102605298, 0.00062863782348539612, 0.00092564728368813216, 0.00085789584370155904, 0.0015929526107122279, 0.0010619504685330263, 0.0008756466103124682, 0.00012300147455576306, 0.0011670024745829299, 0.0011095782639600395, 0.0006789663142594309, 0.001765482320956688, 0.0013964605099517325, 0.00033730591439722784, 0.00099355718172114654, 0.0013746334531984771, 0.0018815223985346981, 0.0012939639925227137, 0.0004560411988071446, 0.001301522159630677, 0.00027420664696429554, 0.00026584065589256253, 0.00055026144133293821, 0.00059171479122261165, 0.001028615082560183, 5.3643111851681239e-05, 0.001654756665379361, 0.00096071171633716488, 0.0013526548302126014, 0.00035430139537653426, 0.0013698746608760051, 0.0014972824121724079, 0.0016321383552054614, 0.0010863485798539012, 0.0017849061880802787, 0.0013210069325078663, 0.00032749215821806205, 0.00030096861019908514, 0.00087253513512643892, 0.0012697911543512184, 0.00077561008857599846, 0.00035363274905770344, 0.0016046605657023772, 0.0016050268965069178, 0.0015035126617238082, 0.00031867265031851495, 0.00095259457717128859, 0.0014821865821128526, 0.00097692263372468197, 0.0014068753897632769, 0.00074618156741341709, 0.0016792379774734436, 0.0011479519526957065, 0.00052921451827408431, 0.0018012278636582572, 0.00072702691591782799, 0.00051050935059592547, 0.00026224838253003827, 0.0018193442870363251, 0.0011759819629921532, 0.00089829175440595379, 0.0015175760574268609, 0.00026261853482093585, 0.0019000017538505526, 0.00054549148482593444, 0.00032824954411008166, 0.00057186037918634833, 4.2590438773408602e-05, 0.0005031431281684297, 0.0019119927910992117, 0.00027324146859342358, 0.0017501937295905776, 0.00081570993168161951, 0.0012241141127542028, 0.00047868936283375682, 0.001157860117402045, 0.00072752049552925061, 0.0016572229840853159, 0.0002066580286365513, 0.0017427679836754329, 0.0010509540881860689, 0.0015119482659793539, 0.0011760451431785319, 0.0016964944746884886, 0.00067559938425934389, 0.00036863818141164733, 0.0001305884293681712, 0.0012617490495534676, 0.0014333700668844726, 0.0002625222599990002, 0.0003077982634196243, 0.00085460613750150773, 0.00091590885567446087, 2.91635453807348e-05, 0.00017716454634341617, 0.00089485683808342533, 0.00035644360597791711, 0.00080221956971024259, 0.00084565226092655967, 0.0007489631420086859, 0.00054298065295730431, 0.0013390831038875618, 0.00077439848446904077, 0.001858770566531241, 0.00088848322992927898, 0.0011973980575686071, 0.0011731355400431216, 0.00077565075993498854, 0.00085632666344556186, 0.0010196353633638042, 0.00047051923381497956, 0.00015841146205489276, 0.001730011432569868, 0.00043251501545084673, 0.001262543332307029, 0.00060764224146492524, 0.00081111675217984637, 0.00081193915119760415, 0.00074141231142845735, 0.0010580127148574177, 0.0010976142109166709, 0.0008738995985830524, 0.0017536680007069227, 0.0014149988329962074, 0.00030124689189768035, 0.00081593869678819419, 0.0018972674737530476, 0.0003552961261215641, 0.0017237302130256637, 0.0017964596712403572, 0.00175546157223949, 0.00098245172038679217, 0.00026401832507909188, 0.0013556030102267781, 0.00097191134140553524, 0.0010580547378232158, 0.0012397057603580966, 0.001460376059828812, 0.0012781215847756675, 0.0017908579183687188, 0.00090960888390450168, 0.00014986083013387907, 0.0002367331012136011, 0.0013611039155919161, 0.0013875033077739413, 0.00090595510245002401, 0.0010306401217291834, 0.00055459818613733342, 0.00062042190878768499, 0.001852520287209711, 0.0018839566572415689, 0.0010521735532121985, 0.0011293592613060006, 0.0012985319394327455, 0.0016541050714961082, 0.0018404809604296208, 0.0002390327205003271, 0.0010023031525248393, 0.0018110830064082897, 0.0013468870142261046, 0.0016848584474729485, 0.00028105024378135803, 0.00022645310994063916, 0.0013388694521473412, 0.0017885469530276679, 0.0013014615111326918, 0.0016533020156980821, 0.00018227906045684632, 0.00088413483525927284, 0.00072623057279924924, 0.0017672527512837485, 0.00035886393927578222, 0.00031311671635063425, 0.0011939007485535988, 0.00064741979201654901, 0.00037779598047373264, 0.00048050352924812142, 0.0014388283984517179, 0.0014750162176387907, 0.0011700574565180783, 0.0014769841423440725, 0.0011831480941838287, 0.00038509405864011658, 0.0013808201913884258, 0.00053314546596256048, 0.0015775717808359207, 0.00075125731274756056, 0.00098195606917455256, 0.0014280655294205244, 0.0015900821741694925, 0.00018861867471118763, 0.0016815195657616348, 0.00046721910342880583, 0.0006190043691407215, 0.0002597870472412376, 0.0010408056034093512, 0.00090674888813771601, 0.00092202654034324134, 0.00094878202683890275, 0.0013583921858343342, 0.0016219625853249322, 0.00054831881984945688, 0.0014059073069675062, 0.00064658602884771934, 0.00041043523090997821, 0.00024199748513309955, 0.00087495151127916886, 0.0010501519722625201, 0.00073533843317399156, 0.0018344641841159248, 0.00041862524903473044, 0.0018954226643716362, 0.0010288305711702555, 0.0013060443659436351, 3.7658070355408657e-05, 0.00074615229338871006, 0.00024045618716007894, 0.0013409716443634624, 0.0012751120507991734, 0.0017686366516732025, 0.0012125632362818993, 0.001456589664066737, 0.0015696501823167835, 0.00020324460457865487, 0.00042864255725879555, 0.001716971641172565, 0.0015869163153257558, 0.0010134232023091961, 0.0017715921322860716, 0.00056627135234333693, 0.0012673636817728976, 0.00087022698657453082, 0.0015206299187828806, 0.0004407481434939907, 8.6144143030647134e-05, 0.00076991019439242225, 0.0018660988624160987, 0.0013018834572944651, 0.0018460857769240046, 0.0002027743400108467, 0.0011775953909003573, 0.0012125609209759165, 0.00028821302789869941, 0.0009510580091506573, 0.00012681052245495697, 0.0017260334655005833, 0.0013692185576827547, 0.00070738940066730847, 0.00017696309553926238, 0.0013539976385590772, 0.00072419823731953838, 0.00085088994984247558, 0.00049111727338484765, 0.0015902181923235531, 0.00090017751804482519, 0.0013781058144807705, 0.0014291335507600296, 0.00027101097063591894, 0.0010076138142464447, 0.0018399898506955684, 0.0016559402378003025, 0.0010898772967505061, 0.00016099370544109468, 0.0010039938614934498, 0.00050807510873986236, 0.00170474418366204, 0.0011833212250642855, 0.001040078889649781, 0.0001041162786267387, 0.00086669773815254271, 0.0017133678253965043, 0.00072956501268128685, 0.00066816151006770434, 9.4425140635880184e-05, 9.4668570089161881e-05, 0.00084357127802819995, 0.0016040031216707868, 0.0002983788071255454, 0.00054367191695424563, 0.0010371370716028064, 6.8688858348881491e-05, 0.0010176427655554353, 0.00079611676200904466, 0.00089587205932459583, 0.00095895652006710392, 0.00066814035219825993, 0.0016070718798036844, 0.00090711567106382832, 0.00054938717246314189, 0.00083579529077880464, 0.001540937955699832, 0.0009027855444208757, 0.0017264334619641927, 7.9482445334834827e-05, 0.0014149675364938193, 0.00065306087290895585, 0.00079770026426782402, 0.00068283681400558086, 0.0016454572960700276, 0.0016062719060562169, 0.00096151321042331527, 0.0015792488560986868, 0.0001489290285152669, 0.00087080475705616112, 9.6475875008561498e-05, 0.00096059343990934239, 0.001342552208506392, 0.0011986186633958214, 0.00016000534062748538, 0.00075017922572853928, 0.00045032603280552633, 0.0014329004280716877, 0.00029192822511538711, 4.6777993150178704e-05, 0.00010048428664205793, 0.001554805036502267, 0.00093964911457912546, 0.00079036465421980354, 0.0011980296628792495, 0.0012749939047819769, 0.00035717620134260942, 0.0014677395908325483, 0.00094620554207013594, 0.00087314579942327712, 0.00019236357814436127, 0.00053736754389282853, 0.00010070859345300778, 7.787442355084306e-05, 0.0013622049820326272, 0.0012642758100889764, 0.00077044190914713178, 4.2781773020914928e-05, 0.0018772294346049547, 0.0017985749972832005, 0.00057037872078828872, 0.0012503197968935155, 0.00068501705624583699, 0.00055886322656296071, 0.0011910841933996054, 0.0018250894641126413, 0.0014259122420714919, 0.00066213637593587338, 0.00072435023200707644, 0.0014116175665489631, 0.0010079215227102609, 0.0018176312938116303, 0.00098317957802791615, 0.0014387696993641158, 0.0013191109622781244, 0.00017061826021477005, 0.00018696758291583611, 0.00050898649971606227, 0.0017648829397638151, 0.00085733920177726106, 0.0011598298242291717, 0.00039426461618278201, 0.0019068709498413967, 0.0011983944367755432, 0.00058685461730450508, 0.0017267246430678917, 0.0012795620316517766, 0.0015628400580452368, 0.00032082198356430557, 0.00072541399802365022, 0.00084484217584856601, 0.0017888010622199566, 0.00044429696940195064, 0.001293363885187928, 0.00040551759372981937, 0.00087942958735717303, 0.0002911744441597398, 0.0016388621411385759, 2.9914937173785667e-05, 9.8880483486061031e-05, 0.00016388859096949179, 0.00074437704616684233, 0.0017387819526289889, 0.00086960804929884627, 0.00017782152669102219, 0.0014962674545689371, 0.0013030095679164976, 0.0010015637934033747, 0.0009524012841815127, 0.00096227990390243903, 0.0014724935778060362, 0.0013706467027429067, 8.0503179693416083e-05, 0.00057738838888104994, 0.001368867477264902, 0.00029089998707905164, 0.0014667323783790489, 0.00048346672045705946, 0.00036467995635892551, 0.0011989790705687372, 0.00039771594961155246, 0.00060068711145390777, 0.0015293680660587057, 0.00070884718959199735, 0.0011762259147894531, 0.0009381459884405843, 0.00082680412138778155, 0.0003651677583059339, 0.0005916974677914272, 0.0016776840980353563, 0.0018559510492821743, 0.001168210124730981, 0.00012860611455710105, 0.00021373713665477176, 0.001276102295366016, 0.00088590799134134431, 0.00018976405287565493, 0.0014695393399476607, 0.0018692660554983065, 0.0013238852425687844, 0.0003277541816320972, 0.0017696297092770167, 0.0013078781763926055, 0.00013198459994668061, 1.5231431474671893e-05, 0.00092008193715002208, 0.0015558384374851873, 0.0016673102520832585, 0.0011103404349397524, 0.00036781150660460376, 0.0017934779852471397, 0.00027843250515682469, 0.0006832466312812922, 0.0011291619880171304, 0.0012209463790229908, 0.0016964490918798384, 0.0011979280147222059, 0.0010682401989202857, 0.0017790585355915562, 0.001549739482905351, 0.00078173942520673955, 1.0201124575622432e-05, 0.00082282148575803075, 2.8887621781444416e-05, 0.00040970666039334333, 3.1166447988888356e-05, 0.0015510173046605295, 0.0015698796838923162, 0.00055031397292881832, 0.001811337676055064, 0.00057345510563363926, 7.1058523432751142e-06, 0.0010322819729138681, 0.00082997032312889503, 0.0015151440765988223, 0.00035680368408994516, 0.00097478088442447828, 0.0016298061969751542, 0.00027210356205872779, 0.00029129589571720549, 0.00097795980670532562, 0.0014759979794103076, 0.0009285132175234752, 0.00058990100268491922, 0.00077450637360341416, 0.00030063329168630723, 0.00038460735982032162, 0.00099614749325541911, 0.0011815040393605431, 0.00056866132136160273, 0.0015048034917038813, 0.0011264153454188131, 0.0002187645044170343, 0.00090100822110898856, 0.0010497779213105361, 0.00074839790745799605, 0.00071799958222966388, 0.001108272235750134, 0.00099662149979001997, 0.00029031575323423646, 0.00053267726268123479, 0.0014112554745333169, 0.00043165024637097316, 0.00099229009612924958, 0.0014223182473336712, 0.0014169029014825139, 0.00090643828985064714, 0.0010690360615457657, 0.00058077291329101759, 0.0011191156448220297, 0.0012942169051975, 0.00029544136988766772, 0.00029518687399647845, 0.0017242331294014305, 0.0013469721427263001, 0.00073244623383769713, 0.00019581765004535618, 0.0012420552838831897, 0.0018748132181630431, 0.0016638699746146025, 0.00026873171559973786, 0.0016094540878764073, 6.378674684387227e-05, 0.00041924111908417828, 5.279684519502428e-05, 0.00088695825305938473, 9.5904294396247393e-05, 0.0010946154429233811, 0.00044377618235397876, 0.001547323839184525, 3.8436696121894627e-05, 0.00057657099596086087, 0.00094167606583809942, 2.0357848267221471e-05, 0.00088013828610005883, 0.0014192839197187429, 0.00060705088315386713, 0.00047384827469984519, 0.00028995088489806235, 0.001867582106493909, 0.00072387720997668442, 0.00063284383064673901, 0.0017799514653338943, 0.0001281099893344464, 0.00095221191598898346, 0.0016339856437881848, 0.00063566363227185831, 0.0012670467643319035, 0.0016841012401767948, 0.0016045417233923635, 0.001582018784573646, 0.0017021439245442902, 0.0018513300379926455, 0.0013967176041684902, 0.00069887040880860696, 0.0013403692463339033, 0.00019630511475000789, 0.00029274733293403808, 0.00019439862399444688, 0.00098802439665068766, 0.00046446711717415383, 0.00025021752441509089, 0.00029247650964148419, 0.0009918773376470162, 0.0012370776547879867, 0.0013201223278780732, 0.0012761307700536872, 0.00019232935097948179, 0.0016305032306836197, 0.0012164918947205347, 0.0016328332486993434, 0.0014695574035639553, 0.00084484925758459802, 0.0013477667360916375, 0.0014560778103973402, 0.00011986733887233819, 0.00061588275240127008, 0.00093637392349707948, 0.00091219325232500936, 0.0016710338937942532, 0.001341113447650164, 0.00040443569284087611, 0.001829228275557188, 0.0015510905422429866, 0.0013166381090802676, 0.00022134104223839541, 0.0010108073231238203, 0.00010394462962456327, 0.0010107661313831864, 0.0013085548378486973, 0.00036680745618147793, 0.0016740527634201841, 0.0015725184381492876, 0.0015160058036016396, 0.0013786140583243597, 0.0016831846449578824, 0.0011209440270123286, 0.0016356730032793775, 0.0018350845136188428, 0.00014737780150878959, 0.0007213415996984669, 0.0012033319611567238, 0.0010024672246524149, 0.00032666096187403804, 0.0014461625988387763, 0.00060603937588652043, 0.00035081480924086824, 0.000538788408988736, 0.00028873269226357701, 0.001711989269908886, 0.001050941764689013, 0.0014365055446715431, 0.00082242029244626081, 0.00089114390242899422, 0.0010391153075172395, 0.00073290194727595342, 0.0017000300364599533, 0.0017190509762476118, 0.0014635433390332519, 0.0010081119545091175, 0.0010804747787710896, 0.00071686893539497031, 0.00054679824304305045, 0.0010681211543761709, 2.8082135655368649e-05, 0.0013668718383462494, 0.0013725250435142724, 0.0010101357098774707, 0.0012262617554411109, 0.00093594932675269342, 0.00059513258015873405, 0.0017552378481306172, 0.0018127933563924946, 0.0014793723600485809, 0.0017387997224533091, 0.00014733736455097658, 5.6886087749433828e-05, 0.0016155590289677968, 0.00170137627929516, 0.0014252368839817642, 0.00029709191906451035, 0.0018974498846610096, 0.0013195898206049622, 0.0015758300219207899, 0.0017135596752183978, 0.0015696922303090838, 0.00024715459829016619, 0.00068688235351272342, 0.00011180451461075087, 0.0016906020521887189, 0.0013989141118835454, 0.0012416158822169969, 0.00088721467493695029, 0.0013907741994731338, 0.0018278881597504533, 0.00022094164345057824, 0.00017085580432623621, 0.00022926004433914825, 0.0010479628140925516, 0.00048173845114749003, 0.0018184435217814385, 0.0013083920022815537, 0.0017499983129862361, 0.0017700260726567043, 0.0013535319127628647, 0.00085650680803721737, 0.0010875151947621123, 0.0016323663470368388, 0.0016150236370058701, 0.00028106367404416604, 2.1434853252381512e-05, 0.0010495488111086225, 0.0016046243248919083, 0.0014211845730947789, 0.0017181828510558223, 7.8798228553384836e-05, 0.00071183286924119066, 0.0013465905302032035, 0.0013915406607487536, 0.0017828570310628343, 0.00030865459327730896, 0.0010739446117225444, 0.0011494563002642172, 0.00073543488478698682, 0.00039528367551379769, 0.0015003684808127568, 7.312053811344259e-05, 9.6368709077443107e-05, 0.0014347223275406778, 0.00051121119638803368, 0.001486962181866978, 0.0015746604473589351, 0.00016814156304596143, 0.00082127945792916763, 0.00089704320316948691, 0.00073345549139727567, 0.0017229070983236774, 0.0010972538013255386, 0.00084024230942310015, 0.00012016428764843126, 0.0015130564594341947, 0.00059076601500817998, 0.0011776731949294391, 0.00041802762942748947, 0.0014745173787907855, 0.0011295187650201927, 0.0018434180528383328, 0.0017429621175586111, 0.00099189842962146488, 0.00015372935280649549, 0.00143907085488604, 0.0011572836047190928, 0.0013305252624816284, 0.00051081270674652895, 0.00081574986245908315, 0.0001179196529205793, 0.00071764316386919438, 0.00083806788893959485, 0.001581735399137056, 0.0018858490309292444, 0.00026798418984490458, 0.0014676919737152629, 0.0017109602603291236, 0.0016824664543684802, 0.00018348173260791879, 0.0017499377116641692, 0.0018730421517861311, 0.00014304662811441806, 0.0015320915868383331, 0.0010463038724876862, 0.00037427619956798637, 0.00031673079276240746, 0.0011283518906567841, 0.00015442511009987889, 6.1951537985947922e-05, 0.0018872165013392394, 0.0017755322148614759, 0.0015397299649812278, 0.00034003051674123434, 0.0001609115503719771, 0.00048007667623441608, 0.00066701928881040813, 0.00015309647288632675, 0.0014287837861056865, 0.00037293540686455118, 0.0013545622439197615, 0.00084756822543220929, 0.0010170232434319692, 0.00032992036288526859, 0.00095290907346831048, 0.0017078094939404875, 0.00062623800926684021, 0.0016696448884895006, 0.00058947094540551107, 0.0015260286658264706, 0.00094567023885190117, 0.00048948746699954758, 0.0016153645116579805, 0.00037648302098334385, 0.0007532592429317145, 0.00012366635441063992, 0.00092729422233377384, 0.00011357007908535494, 0.0016227804125713127, 0.0015993769974106183, 0.00038724749510892403, 0.0018118737834106273, 0.00025435856972984402, 0.0014229033008338863, 0.0017316509722241056, 0.00077926666943816671, 0.001134667854307503, 0.00074146538599009141, 0.00013527092428759652, 0.0003473551087843294, 0.00047590520802661616, 0.0012723281008932821, 0.0010171561445366656, 0.0015530787227631953, 0.0015415629175891015, 0.00034073370556955424, 0.00136812403438336, 0.00055490402087988349, 0.0001100964923194889, 0.00026228664039663072, 0.0016380663446698279, 0.0010764285726273233, 0.0015006088653460108, 0.0011960139098517858, 0.00066407689825858708, 0.0018831673007817214, 0.00055886045617216807, 0.001283221668982196, 0.0014437309086699985, 0.001125890476868546, 0.0011599953815139296, 0.00066140893317410607, 0.0017998133738916318, 0.00084295643267297553, 0.0017154317214597031, 0.0018832411834201637, 0.00066474065817444033, 0.0017689284359218922, 0.00050917488479675262, 0.0011670611586764219, 0.0014936786132530009, 0.0014773761804816731, 0.0005519191403972651, 0.00086190529585043675, 0.0018619549108848592, 0.00034150147546839357, 0.0013547498932793157, 7.0697752276126962e-05, 0.0014920801743495752, 0.001639674181167723, 0.0001474551021726521, 0.00018081862238816918, 0.00039437363267990667, 0.00044485017314158744, 0.0011081548965255152, 0.0016245718383858297, 0.0015485185284256039, 0.00034730148240779161, 0.00098352853516591138, 0.0010789158477204535, 0.001151583171346605, 0.0016190569352450487, 0.00070201310790651936, 4.6499058850668062e-05, 3.602041349689935e-05, 0.0014614951215987965, 0.0016483418026089932, 0.00067634768701711658, 0.00045893332681720022, 0.0015608870100202988, 0.000787862483358079, 0.00065272880087317278, 0.00075129759046175398, 0.001378902078729743, 0.0015277771676375724, 0.00099880402209717571, 0.00086564065875438718, 0.0011268739012235919, 0.00061367868442138837, 0.0017481270731388905, 0.00067572834447334287, 0.0011794269198787488, 0.0017656172133414399, 0.0013640050562701903, 0.0004502177956680565, 0.00093752571365096865, 0.0016799808874701741, 0.00046316218147580013, 0.0013929928704775084, 0.001085758107002268, 0.0014427998211718398, 9.0668166410036474e-05, 5.6974328463915145e-05, 0.0018786414311949909, 4.6583525674110094e-05, 0.0018435652200047318, 0.00012376050518974103, 0.0012372690432055535, 0.00075943274100290703, 0.0015723331666398811, 0.00037093130449615167, 0.0012558197360073187, 0.0018687681182181233, 0.0013283849858547979, 0.0015657494032076489, 0.00040896140445536525, 0.0018041470959421157, 0.00016890138275767541, 0.0018488711651716999, 0.0016754580020036359, 0.00082628596379636136, 0.0014009903579620196, 0.0018069474865982354, 0.0016013561775591965, 0.00077918960776980216, 0.00081373095136096411, 0.0017334619937186932, 0.001892371046481415, 0.00041824025387532409, 0.0010916976783368611, 0.0015031099318450853, 0.0013896549124798696, 0.00049918865926359057, 0.00046556536338119946, 0.0016876182893519674, 9.7382668895644071e-05, 0.001624955760537661, 0.00069431647318168085, 0.00024981248275932966, 0.0010090523408114054, 0.000990798232190025, 0.00015920340656315466, 0.00015035374185566413, 3.4945005343652189e-05, 0.0017720849857732752, 0.00080251002371444424, 0.00071953431972164648, 0.00047126122151706207, 0.00099750142675303854, 0.0014245326514595448, 0.00092152252387805609, 0.0010941823781834177, 0.00085157718037383269, 0.0014904535144155599, 0.00076014986791349802, 0.0011650249896638962, 0.0016592492733323433]\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "##Geneate random initial centroids around the global aggregate\n",
    "##Part (B) and (C) of this question\n",
    "###################################################################################\n",
    "def startCentroidsBC(k):\n",
    "    counter = 0\n",
    "    for line in open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        if counter == 1:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "        counter += 1\n",
    "    #perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(1000)\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    return centroids\n",
    "\n",
    "print startCentroidsBC(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startCentroidsA(k):\n",
    "    counter = 0\n",
    "    centroid_rows = np.random.randint(0,999, size=k)\n",
    "    centroids = []\n",
    "    for line in open(\"topUsers_Apr-Jul_2014_1000-words.txt\").readlines():\n",
    "        if counter in centroid_rows:\n",
    "            data = re.split(\",\",line)\n",
    "            centroids.append([float(d) for d in data[3:]])\n",
    "        if counter >max(centroid_rows):\n",
    "            break\n",
    "        counter +=1\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0249540805817\n"
     ]
    }
   ],
   "source": [
    "# centroid_rows = np.random.randint(0,999, size=4)\n",
    "# print max(centroid_rows)\n",
    "# print centroid_rows[3:]\n",
    "\n",
    "c=startCentroidsA(4)\n",
    "c=np.array(c)\n",
    "import time\n",
    "start = time.time()\n",
    "for i in xrange(1,4000):\n",
    "    np.linalg.norm(c[0]-c[1]) # euclidean distance, don't distribute it\n",
    "print time.time() - start\n",
    "# ar = [-1,2,3]\n",
    "# br = [4,0,-3]\n",
    "# abr = [ar,br]\n",
    "# a= np.array(ar)\n",
    "# b= np.array(br)\n",
    "# ab = np.array(abr)\n",
    "# print np.linalg.norm(ab[0]-ab[1])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: distribute closest cluster calculation (step 2 below) using MR so that each point A is sent as:\n",
    "(A,C1)\n",
    "(A,C2)\n",
    "(A,C3)\n",
    "(A,C4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experiments A, B, C and D and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>K-Means</h2>\n",
    "K-means is a clustering method that aims to find the positions μi,i=1...k of the clusters that minimize the distance from the data points to the cluster. K-means clustering solves:\n",
    "<br><br>\n",
    "$$\\arg\\min_{c} \\sum_{i=1}^k\\sum_{{x}\\in c_i} d({x},\\mu_i) = \\arg\\min_{c} \\sum_{i=1}^k\\sum_{{x}\\in c_i} \\left\\Vert {x}-\\mu_i \\right\\Vert_2^2$$\n",
    "<br><br>\n",
    "where ${c}_i$ is the set of points that belong to cluster i. The K-means clustering uses the square of the Euclidean distance $d({x},\\mu_i) = \\left\\Vert {x}-\\mu_i \\right\\Vert_2^2$. This problem is not trivial (in fact it is NP-hard), so the K-means algorithm only hopes to find the global minimum, possibly getting stuck in a different solution.\n",
    "\n",
    "<h2>K-means algorithm</h2>\n",
    "\n",
    "The Lloyd's algorithm, mostly known as k-means algorithm, is used to solve the k-means clustering problem and works as follows. First, decide the number of clusters k. Then:\n",
    "\n",
    "<table>\n",
    "<tbody><tr><td>1. Initialize the center of the clusters</td>\n",
    "<td>${\\mu}_i = $ some value $, i=1,...,k$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2. Attribute the closest cluster to each data point</td>\n",
    "<td>${c}_i = \\{j: d({x}_j, \\mu_i) \\le d({x}_j, \\mu_l),  l \\ne i, j=1,...,n\\}$ </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3. Set the position of each cluster to the mean of all data points belonging to that cluster</td>\n",
    "<td>$\\mu_i = \\frac{1}{|c_i|}\\sum_{j\\in c_i} {x}_j,\\forall i$</td>\n",
    "</tr>\n",
    "<tr><td>4. Repeat steps 2-3 until convergence</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr><td>Notation</td><td>${|c|} = $ number of elements in  ${c}$</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculating purity</h2>\n",
    "![purity illustration](http://www.candpgeneration.com/images/purity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`topUsers_Apr-Jul_2014_1000-words.txt`__  \n",
    "Notes about the data coding:  \n",
    "> This file consists of word frequency distributions for 1,000 twitter users. These Twitter users use language in very different ways,and were classified by hand according to the criteria:  \n",
    "\n",
    ">__0__: Human, _where only basic human-human communication is observed._  \n",
    "\n",
    ">__1__: Cyborg, _where language is primarily borrowed from other sources. (e.g., jobs listings, classifieds postings, advertisements, etc...)._  \n",
    "\n",
    "> __2__: Robot, _where language is formulaically derived from unrelated sources(e.g., weather/seismology, police/fire event logs, etc...)._  \n",
    "\n",
    ">__3__: Spammer, _where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc...)_\n",
    "\n",
    "Data format:\n",
    "> `USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...`  \n",
    "where   \n",
    "USERID = unique user identifier  \n",
    "CODE = 0/1/2/3 class code  \n",
    "TOTAL = sum of the word counts  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180025371,2,1724608,75552,827,57603,7128,4282,45674,66811,27632,0,8,23783,2,42853,0,62335,22349,21428,19801,4125,0,0,2,1585,21118,1,1,1,16079,19676,1587,0,19695,0,0,0,0,0,0,2,20216,60,4278,0,16,46,788,2,0,0,3,0,3,0,0,111122,0,12,0,0,0,2,739,0,176,0,0,0,38,626,0,0,0,6,1584,0,19672,510,0,0,0,12,0,1675,0,0,0,0,5,2,0,0,1,9,0,0,31,0,0,2,0,0,0,0,4,64,476,0,1,0,617,0,0,15672,70315,70317,0,2997,0,0,0,665,0,0,12,0,0,0,3135,1,2,39,0,0,0,0,23,0,1,0,179,667,0,0,32,0,0,224,5,0,0,66,0,3,450,96,0,0,0,0,8,15,15,0,115,0,0,19672,0,46,15,0,0,2,0,51,0,0,0,298,0,0,5,2,165,3,0,0,46497,0,19675,0,4,0,42036,0,0,40035,84,0,103,0,2,12,1924,7,0,0,0,0,3,0,42629,197,15490,0,0,45,0,0,0,0,0,0,301,0,0,0,0,134,3300,0,422,386,0,19826,2,0,0,46,9,354,175,71,165,20338,0,109,0,1,44376,0,1370,0,0,0,0,0,0,0,0,0,0,2,0,0,4462,0,0,5,0,202,436,408,0,61,0,0,39888,74,0,19672,0,0,0,0,0,19672,19672,2,2,349,0,13,0,30,0,0,8,0,40,0,23,0,12,337,0,12,19952,26,0,0,15489,0,0,0,0,39,0,0,26,0,0,19,144,161,0,0,0,5558,0,23,1561,52,0,0,0,9,0,0,35319,0,0,68,0,0,0,0,0,0,0,8,0,0,222,463,60,0,77,0,20219,0,1,4581,0,0,0,297,0,0,0,0,0,68,0,17942,0,38,226,0,0,0,0,9,19,0,0,0,0,0,217,12,261,0,25052,263,0,0,0,1,0,59,27,14,133,76,234,24966,0,0,1,2,11,44,3,0,0,43,0,3,3,2,0,0,0,2,0,0,7,47,0,0,0,0,2,0,0,1,0,0,0,0,175,0,0,5,1,0,0,0,7,0,0,104,53,0,0,16,13,26,0,11,0,6,77,0,17,0,360,0,0,1693,0,147,0,10,0,0,0,61,1,0,0,113,0,0,1,0,305,14,23,0,42,0,0,0,1,14,2391,6,0,21,0,0,911,3,0,0,0,0,0,0,0,0,39,0,25088,0,0,0,34,134,59,35,45,1584,40,2,8,0,249,0,72,0,0,0,0,0,2,0,0,0,0,0,25,0,13,0,0,4223,30,0,0,0,0,0,0,184,1,5558,0,0,24,0,61,7,38,0,309,0,25,0,0,0,0,0,0,0,0,0,3,258,0,0,0,0,0,710,0,62,0,21,0,0,0,0,820,0,0,19672,0,1,0,0,8,74,0,0,0,87,15390,12,20216,1,0,2,10810,11,0,0,47,0,0,797,95,19826,143,0,2,772,0,0,0,117,90,0,56,1,0,0,86,382,807,0,0,0,77,0,97,0,169,282,0,0,0,0,0,0,14,1,0,0,0,0,202,825,1,10,0,803,98,0,167,113,0,263,18506,0,18521,0,243,4,88,0,46,0,0,0,0,15,0,718,1,0,0,0,0,261,0,0,0,454,3028,0,0,0,48,0,0,0,48,0,112,0,0,140,0,1,0,0,0,0,0,2701,0,0,13,0,0,178,741,0,0,1,0,0,0,0,0,352,5,0,0,42,0,0,94,0,0,0,9,0,0,0,0,2,15489,0,11,225,0,5,0,811,0,5,0,43,0,16,277,26,0,11,0,0,0,0,0,0,0,0,0,0,0,0,15489,0,0,0,0,0,0,0,0,0,0,0,95,61,9,0,70,0,28,0,0,135,0,0,0,31,12,0,0,0,0,1,0,0,1,202,0,0,0,39,0,0,125,0,0,48,0,0,24,0,13,272,48,22,0,74,0,0,0,23,0,5,0,2,21,0,0,0,69,12512,0,0,38,0,0,16,0,0,0,0,0,7,2,1,0,17,1,0,0,0,0,18,14,0,0,0,90,0,0,0,128,45,0,1,0,0,0,2,8,0,30,11,15,28,0,0,0,0,0,0,0,0,0,113,0,0,0,50,0,0,0,5,0,118,0,6,85,56,15,12,0,0,0,176,10,57,12,289,0,27,0,0,0,0,0,23,89,0,221,0,16,0,0,0,0,0,14,33,126,11,32,1,137,13,0,0,0,0,0,0,0,0,0,1,0,98,0,0,0,45,0,215,0,0,0,0,71,76,0,0,0,15,108,0,0,176,0,0,0,0,121,0,0,0,0\n"
     ]
    }
   ],
   "source": [
    "# take a look at the first row\n",
    "!head -n 1 topUsers_Apr-Jul_2014_1000-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"1180025371,2,1724608,75552,827,57603,7128,4282,45674,66811,27632,0,8,23783,2,42853,0,62335,22349,21428,19801,4125,0,0,2,1585,21118,1,1,1,16079,19676,1587,0,19695,0,0,0,0,0,0,2,20216,60,4278,0,16,46,788,2,0,0,3,0,3,0,0,111122,0,12,0,0,0,2,739,0,176,0,0,0,38,626,0,0,0,6,1584,0,19672,510,0,0,0,12,0,1675,0,0,0,0,5,2,0,0,1,9,0,0,31,0,0,2,0,0,0,0,4,64,476,0,1,0,617,0,0,15672,70315,70317,0,2997,0,0,0,665,0,0,12,0,0,0,3135,1,2,39,0,0,0,0,23,0,1,0,179,667,0,0,32,0,0,224,5,0,0,66,0,3,450,96,0,0,0,0,8,15,15,0,115,0,0,19672,0,46,15,0,0,2,0,51,0,0,0,298,0,0,5,2,165,3,0,0,46497,0,19675,0,4,0,42036,0,0,40035,84,0,103,0,2,12,1924,7,0,0,0,0,3,0,42629,197,15490,0,0,45,0,0,0,0,0,0,301,0,0,0,0,134,3300,0,422,386,0,19826,2,0,0,46,9,354,175,71,165,20338,0,109,0,1,44376,0,1370,0,0,0,0,0,0,0,0,0,0,2,0,0,4462,0,0,5,0,202,436,408,0,61,0,0,39888,74,0,19672,0,0,0,0,0,19672,19672,2,2,349,0,13,0,30,0,0,8,0,40,0,23,0,12,337,0,12,19952,26,0,0,15489,0,0,0,0,39,0,0,26,0,0,19,144,161,0,0,0,5558,0,23,1561,52,0,0,0,9,0,0,35319,0,0,68,0,0,0,0,0,0,0,8,0,0,222,463,60,0,77,0,20219,0,1,4581,0,0,0,297,0,0,0,0,0,68,0,17942,0,38,226,0,0,0,0,9,19,0,0,0,0,0,217,12,261,0,25052,263,0,0,0,1,0,59,27,14,133,76,234,24966,0,0,1,2,11,44,3,0,0,43,0,3,3,2,0,0,0,2,0,0,7,47,0,0,0,0,2,0,0,1,0,0,0,0,175,0,0,5,1,0,0,0,7,0,0,104,53,0,0,16,13,26,0,11,0,6,77,0,17,0,360,0,0,1693,0,147,0,10,0,0,0,61,1,0,0,113,0,0,1,0,305,14,23,0,42,0,0,0,1,14,2391,6,0,21,0,0,911,3,0,0,0,0,0,0,0,0,39,0,25088,0,0,0,34,134,59,35,45,1584,40,2,8,0,249,0,72,0,0,0,0,0,2,0,0,0,0,0,25,0,13,0,0,4223,30,0,0,0,0,0,0,184,1,5558,0,0,24,0,61,7,38,0,309,0,25,0,0,0,0,0,0,0,0,0,3,258,0,0,0,0,0,710,0,62,0,21,0,0,0,0,820,0,0,19672,0,1,0,0,8,74,0,0,0,87,15390,12,20216,1,0,2,10810,11,0,0,47,0,0,797,95,19826,143,0,2,772,0,0,0,117,90,0,56,1,0,0,86,382,807,0,0,0,77,0,97,0,169,282,0,0,0,0,0,0,14,1,0,0,0,0,202,825,1,10,0,803,98,0,167,113,0,263,18506,0,18521,0,243,4,88,0,46,0,0,0,0,15,0,718,1,0,0,0,0,261,0,0,0,454,3028,0,0,0,48,0,0,0,48,0,112,0,0,140,0,1,0,0,0,0,0,2701,0,0,13,0,0,178,741,0,0,1,0,0,0,0,0,352,5,0,0,42,0,0,94,0,0,0,9,0,0,0,0,2,15489,0,11,225,0,5,0,811,0,5,0,43,0,16,277,26,0,11,0,0,0,0,0,0,0,0,0,0,0,0,15489,0,0,0,0,0,0,0,0,0,0,0,95,61,9,0,70,0,28,0,0,135,0,0,0,31,12,0,0,0,0,1,0,0,1,202,0,0,0,39,0,0,125,0,0,48,0,0,24,0,13,272,48,22,0,74,0,0,0,23,0,5,0,2,21,0,0,0,69,12512,0,0,38,0,0,16,0,0,0,0,0,7,2,1,0,17,1,0,0,0,0,18,14,0,0,0,90,0,0,0,128,45,0,1,0,0,0,2,8,0,30,11,15,28,0,0,0,0,0,0,0,0,0,113,0,0,0,50,0,0,0,5,0,118,0,6,85,56,15,12,0,0,0,176,10,57,12,289,0,27,0,0,0,0,0,23,89,0,221,0,16,0,0,0,0,0,14,33,126,11,32,1,137,13,0,0,0,0,0,0,0,0,0,1,0,98,0,0,0,45,0,215,0,0,0,0,71,76,0,0,0,15,108,0,0,176,0,0,0,0,121,0,0,0,0\"\n",
    "len(a.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_45.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_45.txt\n",
    "111,0,12,4,3,3,2\n",
    "222,1,15,10,2,2,1\n",
    "333,2,8,2,2,2,2\n",
    "444,3,10,10,0,0,0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`topUsers_Apr-Jul_2014_1000-words_summaries.txt`__\n",
    "This file contains 5 special word-frequency distributions.\n",
    "Notes about the format of this auxillary information:  \n",
    "> Row 1: Words  \n",
    "Row 2: 1000-user-wide aggregated distribution across all classes  \n",
    "Row 3-6 class-aggregated distributions for clases 0-3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ID\",\"CODE\",\"TOTAL_WORDS\",\"http\",\"I\",\"the\",\"to\",\"you\"\n",
      "ALL_CODES,NA,61819567,2488393,1989622,1329663,1259298,1181631\n",
      "CODE,0,35130977,449927,1668694,914155,957278,916553\n",
      "CODE,1,11423284,1239122,28497,117272,104367,10209\n",
      "CODE,2,9373246,613561,42672,191091,60120,31309\n",
      "CODE,3,5892060,185783,249759,107145,137533,223560\n"
     ]
    }
   ],
   "source": [
    "# take a look at the first few columns\n",
    "!cut -d ',' -f 1-8 topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 topUsers_Apr-Jul_2014_1000-words_summaries.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile Kmeans.py\n",
    "#!/usr/bin/env python\n",
    "#START STUDENT CODE45\n",
    "\n",
    "\n",
    "\n",
    "#END STUDENT CODE45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile kmeans_runner.py\n",
    "#!/usr/bin/env python\n",
    "#START STUDENT CODE45_RUNNER\n",
    "\n",
    "\n",
    "\n",
    "#END STUDENT CODE45_RUNNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.6  - (OPTIONAL) Scaleable K-MEANS++ \n",
    "\n",
    "Over half a century old and showing no signs of aging,\n",
    "k-means remains one of the most popular data processing\n",
    "algorithms. As is well-known, a proper initialization\n",
    "of k-means is crucial for obtaining a good final solution.\n",
    "The recently proposed k-means++ initialization algorithm\n",
    "achieves this, obtaining an initial set of centers that is provably\n",
    "close to the optimum solution. A major downside of the\n",
    "k-means++ is its inherent sequential nature, which limits its\n",
    "applicability to massive data: one must make k passes over\n",
    "the data to find a good initial set of centers. The paper listed below \n",
    "shows how to drastically reduce the number of passes needed\n",
    "to obtain, in parallel, a good initialization. This is unlike\n",
    "prevailing efforts on parallelizing k-means that have mostly\n",
    "focused on the post-initialization phases of k-means. The \n",
    "proposed initialization algorithm k-means||\n",
    "obtains a nearly optimal solution after a logarithmic number\n",
    "of passes; the paper also shows that in practice a constant\n",
    "number of passes suffices. Experimental evaluation on realworld\n",
    "large-scale data demonstrates that k-means|| outperforms\n",
    "k-means++ in both sequential and parallel settings.\n",
    "\n",
    "Read the following paper entitled \"Scaleable K-MEANS++\" located at:\n",
    "\n",
    "http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf \n",
    "\n",
    "In MrJob, implement K-MEANS|| and compare with a random initializtion when used in \n",
    "conjunction with the kmeans algorithm as an initialization step for the 2D  dataset \n",
    "generated using code in the following notebook:\n",
    "\n",
    "https://www.dropbox.com/s/lbzwmyv0d8rocfq/MrJobKmeans.ipynb?dl=0\n",
    "\n",
    "Plot the initialation centroids and the centroid trajectory as the K-MEANS|| algorithms iterates. \n",
    "Repeat this for a random initalization (i.e., pick a training vector at random for each inital centroid)\n",
    "of the kmeans algorithm. Comment on the trajectories of both algorithms.\n",
    "Report on the number passes over the training data, and time required to run both  clustering algorithms.\n",
    "Also report the rand index score for both algorithms and comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 (OPTIONAL) Apply K-MEANS||\n",
    "\n",
    "Apply your implementation of K-MEANS|| to the dataset  in HW 4.5 and compare to the a random initalization (i.e., pick a training vector at random for each inital centroid)of the kmeans algorithm.\n",
    "Report on the number passes over the training data, and time required to run all  clustering algorithms. \n",
    "Also report the rand index score for both algorithms and comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4.7 - (OPTIONAL) Canopy Clustering\n",
    "\n",
    "An alternative way to intialize the k-means algorithm is the  canopy clustering. The canopy clustering \n",
    "algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and \n",
    "Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the \n",
    "Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, \n",
    "where using another algorithm directly may be impractical due to the size of the data set.\n",
    "\n",
    "For more details on the Canopy Clustering algorithm see:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Canopy_clustering_algorithm\n",
    "\n",
    "Plot the initialation centroids and the centroid trajectory as the Canopy Clustering based K-MEANS algorithm iterates. \n",
    "Repeat this for a random initalization (i.e., pick a training vector at random for each inital centroid)\n",
    "of the kmeans algorithm. Comment on the trajectories of both algorithms.\n",
    "Report on the number passes over the training data, and time required to run both  clustering algorithms.\n",
    "Also report the rand index score for both algorithms and comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.1 (OPTIONAL) Apply Canopy Clustering based K-MEANS\n",
    "\n",
    "Apply your implementation Canopy Clustering based K-MEANS algorithm to the dataset  in HW 4.5 and compare to the a \n",
    "random initalization (i.e., pick a training vector at random for each inital centroid)of the kmeans algorithm.\n",
    "Report on the number passes over the training data, and time required to run both  clustering algorithms. \n",
    "Also report the rand index score for both algorithms and comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "380px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "410px",
    "left": "458.76361083984375px",
    "right": "20px",
    "top": "120px",
    "width": "397px"
   },
   "toc_section_display": "none",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
