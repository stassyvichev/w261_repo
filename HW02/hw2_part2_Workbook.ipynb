{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2 (part 2) - Naive Bayes in Hadoop MR\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2018`__\n",
    "\n",
    "Now that you're up to speed in writing Hadoop MR jobs we'll use this framework to implement your first parallelized machine learning algorithm: Naive Bayes. As you develop your implementation you'll test it on a small dataset that matches the 'Chinese Example' in the _Manning, Raghavan and Shutze_ reading for Week 2. For the main task in part 2 you'll be working with a small subset of the Enron Spam/Ham Corpus. By the end of this portion of the assignment you should be able to:\n",
    "* __... describe__ the Naive Bayes algorithm including both training and inference.\n",
    "* __... perform__ EDA on a corpus using Hadoop MR.\n",
    "* __... implement__ parallelized Naive Bayes.\n",
    "* __... explain__ how smoothing affects the bias and variance of  a Multinomial Naive Bayes model.\n",
    "\n",
    "As always, your work will be graded both on the correctness of your output and on the clarity and design of your code. __Please refer to the `README` for homework submission instructions.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup\n",
    "Before starting part 2, run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars (paths) - ADJUST AS NEEDED\n",
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.7.0.jar\"\n",
    "HDFS_DIR = \"/user/root/HW2\"\n",
    "HOME_DIR = \"/media/notebooks\"  # FILL IN HERE eg. /media/notebooks/w261-main/Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save path for use in Hadoop jobs (-cmdenv PATH={PATH})\n",
    "from os import environ\n",
    "PATH  = environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "ENRON = HOME_DIR + \"/HW02/data/enronemail_1h.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Data\n",
    "For the main task in this portion of the homework you will train a classifier to determine whether an email represents spam or not. You will train your Naive Bayes model on a 100 record subset of the Enron Spam/Ham corpus available in the HW02 data directory (__`HW02/data/enronemail_1h.txt`__).\n",
    "\n",
    "__Source:__   \n",
    "The original data included about 93,000 emails which were made public after the company's collapse. There have been a number raw and preprocessed versions of this corpus (including those available [here](http://www.aueb.gr/users/ion/data/enron-spam/index.html) and [here](http://www.aueb.gr/users/ion/publications.html)). The subset we will use is limited to emails from 6 Enron employees and a number of spam sources. It is part of [this data set](http://www.aueb.gr/users/ion/data/enron-spam/) which was created by researchers working on personlized Bayesian spam filters. Their original publication is [available here](http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf). __`IMPORTANT!`__ _For this homework please limit your analysis to the 100 email subset which we provide. No need to download or run your analysis on any of the original datasets, those links are merely provided as context._\n",
    "\n",
    "__Preprocessing:__  \n",
    "For their work, Metsis et al. (the authors) appeared to have pre-processed the data, not only collapsing all text to lower-case, but additionally separating \"words\" by spaces, where \"words\" unfortunately include punctuation. As a concrete example, the sentence:  \n",
    ">  `Hey Jon, I hope you don't get lost out there this weekend!`  \n",
    "\n",
    "... would have been reduced by Metsis et al. to the form:  \n",
    "> `hey jon , i hope you don ' t get lost out there this weekend !` \n",
    "\n",
    "... so we have reverted the data back toward its original state, removing spaces so that our sample sentence would now look like:\n",
    "> `hey jon, i hope you don't get lost out there this weekend!`  \n",
    "\n",
    "Thus we have at least preserved contractions and other higher-order lexical forms. However, one must be aware that this reversion is not complete, and that some object (specifically web sites) will be ill-formatted, and that all text is still lower-cased.\n",
    "\n",
    "\n",
    "__Format:__   \n",
    "All messages are collated to a tab-delimited format:  \n",
    "\n",
    ">    `ID \\t SPAM \\t SUBJECT \\t CONTENT \\n`  \n",
    "\n",
    "where:  \n",
    ">    `ID = string; unique message identifier`  \n",
    "    `SPAM = binary; with 1 indicating a spam message`  \n",
    "    `SUBJECT = string; title of the message`  \n",
    "    `CONTENT = string; content of the message`   \n",
    "    \n",
    "Note that either of `SUBJECT` or `CONTENT` may be \"NA\", and that all tab (\\t) and newline (\\n) characters have been removed from both of the `SUBJECT` and `CONTENT` columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0001.2000-01-17.beck\t0\t leadership development pilot\t\" sally:  what timing, ask and you shall receiv\n",
      "0001.2000-06-06.lokay\t0\t\" key dates and impact of upcoming sap implementation over the next few week\n",
      "0001.2001-02-07.kitchen\t0\t key hr issues going forward\t a) year end reviews-report needs generating \n"
     ]
    }
   ],
   "source": [
    "# take a look at the first 100 characters of the first 5 records (RUN THIS CELL AS IS)\n",
    "!head -n 5 {ENRON} | cut -c-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal {ENRON} {HDFS_DIR}/enron.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6:  Enron Ham/Spam EDA.\n",
    "Before building our classifier, lets get aquainted with our data. In particular, we're interested in which words occur more in spam emails than in real emails. In this question you'll implement two Hadoop MapReduce jobs to count and sort word occurrences by document class. You'll also learn about two new Hadoop streaming parameters that will allow you to control how the records output of your mappers are partitioned for reducing on separate nodes. \n",
    "\n",
    "__`IMPORTANT NOTE:`__ For this question and all subsequent items, you should include both the subject and the body of the email in your analysis (i.e. concatetate them to get the 'text' of the document).\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) code:__ Complete the missing components of the code in __`EnronEDA/mapper.py`__ and __`EnronEDA/reducer.py`__ to create a Hadoop MapReduce job that counts how many times each word in the corpus occurs in an email for each class. Pay close attention to the data format specified in the docstrings of these scripts _-- there are a number of ways to accomplish this task, we've chosen this format to help illustrate a technique in `part e`_. Run the provided unit tests to confirm that your code works as expected then run the provided Hadoop streaming command to apply your analysis to the Enron data.\n",
    "\n",
    "\n",
    "* __b) code + short response:__ How many times does the word \"__assistance__\" occur in each class? (`HINT:` Use a `grep` command to read from the results file you generated in '`a`' and then report the answer in the space provided.)\n",
    "\n",
    "\n",
    "* __c) short response:__ Would it have been possible to add some sorting parameters to the Hadoop streaming command that would cause our `part a` results to be sorted by count? Briefly explain why or why not.\n",
    "\n",
    "\n",
    "* __d) code + short response:__ Write a second Hadoop MapReduce job to sort the output of `part a` first by class and then by count. Run your job and save the results to a local file using the provide code. Then describe in words how you would go about printing the top 10 words in each class. (`HINT 1:` _remember that you can simply pass the `part a` output directory to the input field of this job; `HINT 2:` since this task is just reodering the records from `part a` we don't need to write a mapper or reducer, just use `/bin/cat` for both_)\n",
    "\n",
    "\n",
    "* __ e) code:__ A more efficient alternative to '`grep`-ing' for the top 10 words in each class would be to use the Hadoop framework to separate records from each class into its own partition so that we can just read the top lines in each. To do this, make sure to specify 2 reduce tasks and add the following parameters to your Hadoop streaming job from 'd':\n",
    ">__`-D mapreduce.partition.keypartitioner.options=\"-k2,2\"`__ : tells Hadoop to partition based on the second field (which indicates spam/ham in our data).  \n",
    ">__`-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner`__ : tells Hadoop that we want to partition on a field that is part of a composite key. (_This parameter should go at the end, not with the `-D` options at the top._)\n",
    "\n",
    "> __`NOTE:`__ these options should be used in conjunction with `-D stream.num.map.output.key.fields=#` which you will have already added as part of your sort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Student Answers:\n",
    "> __b)__ Assistance:\n",
    "assistance\t0\t2\t\n",
    "assistance\t1\t8\t\n",
    "\n",
    "> __c)__  No, the parameters which we can add to Hadoop streaming only allow us to control the order in which the mapper output is fed into the reducers. The output of the reducers themselves cannot be sorted explicitly. \n",
    "\n",
    "> __d)__ We can get the top 10 spam words just by doing head  -n 10 which would give us the top 10 lines. To get the top 10 ham words, we can use something like awk '\\$2 == 0' | head -n  10. The awk command should give us all lines where the second field is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - do your work in the provided scripts then RUN THIS CELL AS IS\n",
    "!chmod a+x EnronEDA/mapper.py\n",
    "!chmod a+x EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\t1\t1\n",
      "body\t1\t1\n",
      "title\t0\t1\n",
      "body\t0\t1\n"
     ]
    }
   ],
   "source": [
    "# part a - unit test EnronEDA/mapper.py (RUN THIS CELL AS IS)\n",
    "!echo -e \"d1\t1\ttitle\tbody\\nd2\t0\ttitle\tbody\" | EnronEDA/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\t0\t2\t\n",
      "one\t1\t1\t\n",
      "two\t0\t1\t\n",
      "two\t1\t0\t\n"
     ]
    }
   ],
   "source": [
    "# part a - unit test EnronEDA/reducer.py (RUN THIS CELL AS IS)\n",
    "!echo -e \"one\t1\t1\\none\t0\t1\\none\t0\t1\\ntwo\t0\t1\" | EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part a - clear output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob1066534205971389208.jar tmpDir=null\n",
      "18/01/20 20:04:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/20 20:04:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/20 20:04:29 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/01/20 20:04:29 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/20 20:04:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0021\n",
      "18/01/20 20:04:29 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0021\n",
      "18/01/20 20:04:29 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0021/\n",
      "18/01/20 20:04:29 INFO mapreduce.Job: Running job: job_1516450633555_0021\n",
      "18/01/20 20:04:34 INFO mapreduce.Job: Job job_1516450633555_0021 running in uber mode : false\n",
      "18/01/20 20:04:34 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/20 20:04:40 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/01/20 20:04:41 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/20 20:04:46 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "18/01/20 20:04:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/20 20:04:48 INFO mapreduce.Job: Job job_1516450633555_0021 completed successfully\n",
      "18/01/20 20:04:48 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=369010\n",
      "\t\tFILE: Number of bytes written=1206110\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217159\n",
      "\t\tHDFS: Number of bytes written=129303\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7593\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8523\n",
      "\t\tTotal time spent by all map tasks (ms)=7593\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8523\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7593\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8523\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7775232\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=8727552\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31490\n",
      "\t\tMap output bytes=306018\n",
      "\t\tMap output materialized bytes=369022\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=369022\n",
      "\t\tReduce input records=31490\n",
      "\t\tReduce output records=10130\n",
      "\t\tSpilled Records=62980\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=114\n",
      "\t\tCPU time spent (ms)=3880\n",
      "\t\tPhysical memory (bytes) snapshot=938434560\n",
      "\t\tVirtual memory (bytes) snapshot=5455409152\n",
      "\t\tTotal committed heap usage (bytes)=732954624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216945\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=129303\n",
      "18/01/20 20:04:48 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part a - Hadoop streaming job (RUN THIS CELL AS IS)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files EnronEDA/reducer.py,EnronEDA/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/enron.txt \\\n",
    "  -output {HDFS_DIR}/eda-output \\\n",
    "  -numReduceTasks 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - retrieve results from HDFS & copy them into a local file (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/eda-output/part-0000* > EnronEDA/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t0\t2\t\n",
      "assistance\t1\t8\t\n"
     ]
    }
   ],
   "source": [
    "# part b - write your grep command here\n",
    "!grep 'assistance' EnronEDA/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part d/e - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3656532941110478678.jar tmpDir=null\n",
      "18/01/20 20:16:52 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/20 20:16:52 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/20 20:16:52 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "18/01/20 20:16:52 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/20 20:16:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0025\n",
      "18/01/20 20:16:52 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0025\n",
      "18/01/20 20:16:53 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0025/\n",
      "18/01/20 20:16:53 INFO mapreduce.Job: Running job: job_1516450633555_0025\n",
      "18/01/20 20:16:59 INFO mapreduce.Job: Job job_1516450633555_0025 running in uber mode : false\n",
      "18/01/20 20:16:59 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/20 20:17:06 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/01/20 20:17:07 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/20 20:17:12 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "18/01/20 20:17:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/20 20:17:13 INFO mapreduce.Job: Job job_1516450633555_0025 completed successfully\n",
      "18/01/20 20:17:13 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=149575\n",
      "\t\tFILE: Number of bytes written=764988\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=129541\n",
      "\t\tHDFS: Number of bytes written=129303\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9455\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8567\n",
      "\t\tTotal time spent by all map tasks (ms)=9455\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8567\n",
      "\t\tTotal vcore-seconds taken by all map tasks=9455\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8567\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=9681920\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=8772608\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10130\n",
      "\t\tMap output records=10130\n",
      "\t\tMap output bytes=129303\n",
      "\t\tMap output materialized bytes=149587\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10130\n",
      "\t\tReduce shuffle bytes=149587\n",
      "\t\tReduce input records=10130\n",
      "\t\tReduce output records=10130\n",
      "\t\tSpilled Records=20260\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=110\n",
      "\t\tCPU time spent (ms)=3690\n",
      "\t\tPhysical memory (bytes) snapshot=943865856\n",
      "\t\tVirtual memory (bytes) snapshot=5477072896\n",
      "\t\tTotal committed heap usage (bytes)=665845760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=129303\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=129303\n",
      "18/01/20 20:17:13 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part d/e - write your Hadoop streaming job here\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k3,3nr\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k2,2\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/eda-output \\\n",
    "  -output {HDFS_DIR}/eda-sort-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - retrieve results from HDFS & copy them into a local file (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/eda-sort-output/part-0000* > EnronEDA/results-sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== part-00000=====\n",
      "\n",
      "the\t0\t549\t\n",
      "to\t0\t398\t\n",
      "ect\t0\t382\t\n",
      "and\t0\t278\t\n",
      "of\t0\t230\t\n",
      "hou\t0\t206\t\n",
      "a\t0\t196\t\n",
      "in\t0\t182\t\n",
      "for\t0\t170\t\n",
      "on\t0\t135\t\n",
      "cat: Unable to write to output stream.\n",
      "\n",
      "===== part-00001=====\n",
      "\n",
      "the\t1\t698\t\n",
      "to\t1\t566\t\n",
      "and\t1\t392\t\n",
      "your\t1\t357\t\n",
      "a\t1\t347\t\n",
      "you\t1\t345\t\n",
      "of\t1\t336\t\n",
      "in\t1\t236\t\n",
      "for\t1\t204\t\n",
      "com\t1\t153\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# part e - view the top 10 records from each partition (RUN THIS CELL AS IS) \n",
    "for idx in range(2):\n",
    "    print \"\\n===== part-0000%s=====\\n\" % (idx)\n",
    "    !hdfs dfs -cat {HDFS_DIR}/eda-sort-output/part-0000{idx} | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected output:__\n",
    "<table>\n",
    "<th>part-00000:</th>\n",
    "<th>part-00001:</th>\n",
    "<tr><td><pre>\n",
    "the\t0\t549\t\n",
    "to\t0\t398\t\n",
    "ect\t0\t382\t\n",
    "and\t0\t278\t\n",
    "of\t0\t230\t\n",
    "hou\t0\t206\t\n",
    "a\t0\t196\t\n",
    "in\t0\t182\t\n",
    "for\t0\t170\t\n",
    "on\t0\t135\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "the\t1\t698\t\n",
    "to\t1\t566\t\n",
    "and\t1\t392\t\n",
    "your\t1\t357\t\n",
    "a\t1\t347\t\n",
    "you\t1\t345\t\n",
    "of\t1\t336\t\n",
    "in\t1\t236\t\n",
    "for\t1\t204\t\n",
    "com\t1\t153\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Document Classification Task Overview.\n",
    "The week 2 assigned reading from Chapter 13 of _Introduction to Information Retrieval_ by Manning, Raghavan and Schutze provides a thorough introduction to the document classification task and the math behind Naive Bayes. In this question we'll use the example from Table 13.1 (reproduced below) to 'train' an unsmoothed Multinomial Naive Bayes model and classify a test document by hand.\n",
    "\n",
    "<table>\n",
    "<th>DocID</th>\n",
    "<th>Class</th>\n",
    "<th>Subject</th>\n",
    "<th>Body</th>\n",
    "<tr><td>Doc1</td><td>1</td><td></td><td>Chinese Beijing Chinese</td></tr>\n",
    "<tr><td>Doc2</td><td>1</td><td></td><td>Chinese Chinese Shanghai</td></tr>\n",
    "<tr><td>Doc3</td><td>1</td><td></td><td>Chinese Macao</td></tr>\n",
    "<tr><td>Doc4</td><td>0</td><td></td><td>Tokyo Japan Chinese</td></tr>\n",
    "</table>\n",
    "\n",
    "### Q7 Tasks:\n",
    "* __a) short response:__ Equation 13.3 in Manning, Raghavan and Shutze shows how a Multinomial Naive Bayes model classifies a document. It predicts the class, $c$, for which the estimated conditional probability of the class given the document's contents,  $\\hat{P}(c|d)$, is greatest. In this equation what two pieces of information are required to calculate  $\\hat{P}(c|d)$? Your answer should include both mathematical notatation and verbal explanation.\n",
    "\n",
    "\n",
    "* __b) short response:__ The Enron data includes two classes of documents: `spam` and `ham` (they're actually labeled `1` and `0`). In plain English, explain what  $\\hat{P}(c)$ and   $\\hat{P}(t_{k} | c)$ mean in the context of this data. How will we would estimate these values from a training corpus? How many passes over the data would we need to make to retrieve this information for all classes and all words?\n",
    "\n",
    "\n",
    "* __c) hand calculations:__ Above we've reproduced the document classification example from the textbook (we added an empty subject field to mimic the Enron data format). Remember that the classes in this \"Chinese Example\" are `1` (about China) and `0` (not about China). Calculate the class priors and the conditional probabilities for an __unsmoothed__ Multinomial Naive Bayes model trained on this data. Show the calculations that lead to your result using markdown and LaTeX in the space provided or by embedding an image of your hand written work. [`NOTE:` _Your results should NOT match those in the text -- they are training a model with +1 smoothing you are training a model without smoothing_]\n",
    "\n",
    "\n",
    "* __d) hand calculations:__ Use the model you trained to classify the following test document: `Chinese Chinese Chinese Tokyo Japan`. Show the calculations that lead to your result using markdown and LaTeX in the space provided or by embedding an image of your hand written work.\n",
    "\n",
    "\n",
    "* __e) short response:__ Compare the classification you get from this unsmoothed model in `d`/`e` to the results in the textbook's \"Example 1\" which reflects a model with Laplace plus 1 smoothing. How does smoothing affect our inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Student Answers:\n",
    "> __a)__ You need the prior probability of a document occurring with class c:\n",
    "\n",
    "> $P(c)$\n",
    "\n",
    "> You also need the product of the conditional probabilities of each term in the document occurring in a document of class c:\n",
    "\n",
    "> $\\prod_{1\\le k \\le n_d} P(t_k | c)$\n",
    "\n",
    "> __b)__ $\\hat{P}(c)$ is the probability that an email is classified as c (spam or ham) as estimated from the training set. This should be fairly easy to estimate with one pass through the data, counting the number of emails in each class. \n",
    "\n",
    "> $\\hat{P}(t_{k} | c)$ is the probability of a term occurring in a doccument of this class, again estimated from the training set. As we saw in the previous question, we can make one MapReduce run to get this information for every word in every class, followed by one final pass through the corpus (that can probably be linear with the right sorting) to calculate the actual probabilities. \n",
    "\n",
    "> __c)__ Show your calculations here using markdown & LaTeX or embed them below!\n",
    "\n",
    "> $P(c=1) = 3/4 = 0.75$\n",
    "\n",
    "> $P(c=0) = 1/4 = 0.25$\n",
    "\n",
    "> $P(Chinese | c=1) = 5/8$\n",
    "\n",
    "> $P(Chinese | c=0) = 1/3$\n",
    "\n",
    "> $P(Beijing | c=1) = 1/8$\n",
    "\n",
    "> $P(Beijing | c=0) = 0$\n",
    "\n",
    "> $P(Shanghai | c=1) = 1/8$\n",
    "\n",
    "> $P(Shanghai | c=0) = 0$\n",
    "\n",
    "> $P(Macao | c=1) = 1/8$\n",
    "\n",
    "> $P(Macao | c=0) = 0$\n",
    "\n",
    "> $P(Tokyo | c=1) = 0$\n",
    "\n",
    "> $P(Tokyo | c=0) = 1/3$\n",
    "\n",
    "> $P(Japan | c=1) = 0$\n",
    "\n",
    "> $P(Japan | c=0) = 1/3$\n",
    "\n",
    "> __d)__ Show your calculations here using markdown & LaTeX or embed them below!\n",
    "\n",
    "> $P(c=1 | Chinese Chinese Chinese Tokyo Japan ) = P(c=1)* P(Chinese|c=1)*P(Chinese|c=1)* P(Chinese|c=1)* P(Tokyo|c=1)* P(Japan|c=1)=(3/4)*(5/8)*(5/8)*(5/8)*0*0 = 0$\n",
    "\n",
    "> __e)__ Our calculations show that the document needs to be classified as non-Chinese because we have conditional probabilities for terms like Tokyo to be in documents classified as Chinese. Laplace smoothing allows us to take into account the fact that we probably haven't seen all possible samples, so it adds one to each conditional probability calculation in order to take this into account. With Laplace smoothing, the zero conditional probabilities from Tokyo and Japan become non-zero and allow us to do a more accurate calculation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d/e - if you didn't write out your calcuations above, embed a picture of them here:\n",
    "from IPython.display import Image\n",
    "Image(filename=\"path-to-hand-calulations-image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Naive Bayes Inference.\n",
    "In the next two questions you'll write code to parallelize the Naive Bayes calculations that you performed above. We'll do this in two phases: one MapReduce job to perform training and a second MapReduce to perform inference. While in practice we'd need to train a model before we can use it to classify documents, for learning purposes we're going to develop our code in the opposite order. By first focusing on the pieces of information/format we need to perform the classification (inference) task you should find it easier to develop a solid implementation for training phase when you get to question 9 below. In both of these questions we'll continue to use the Chinese example corpus from the textbook to help us test our MapReduce code as we develop it. Below we've reproduced the corpus, test set and model in text format that matches the Enron data.\n",
    "\n",
    "### Q8 Tasks:\n",
    "* __a) short response:__ run the provided cells to create the example files and load them in to HDFS. Then take a closer look at __`chinese/model.txt`__. This text file represents a Naive Bayes model trained (with Laplace +1 smoothing) on the example corpus. What are the 'keys' and 'values' in this file? Which record means something slightly different than the rest? The value field of each record includes two numbers which will be helpful for debugging but which we don't actually need to perform inference -- what are they? [`HINT`: _This file represents the model from Example 13.1 in the textbook, if you're having trouble getting oriented try comparing our file to the numbers in that example._]\n",
    "\n",
    "\n",
    "* __b) short response:__ When performing Naive Bayes in practice instead of multiplying the probabilities (as in equation 13.3) we add their logs (as in equation 13.4). Why do we choose to work with log probabilities? If we had an unsmoothed model, what potential error could arise from this transformation?\n",
    "\n",
    "\n",
    "* __c) short response:__ Documents 6 and 8 in the test set include a word that did not appear in the training corpus (and as a result does not appear in the model). What should we do at inference time when we need a class conditional probability for this word?\n",
    "\n",
    "\n",
    "* __d) short response:__ The goal of our MapReduce job is to stream over the test set and classify each document by peforming the calculation from equation 13.4. To do this we'll load the model file (which contains the probabilities for equation 13.4) into memory on the nodes where we do our mapping. This is called an in-memory join, we'll learn more about those in Week 5. For now, explain how this is a slight departure from one of the functional programming principles. From a scalability perspective when might this departure be justified? when would it be unwise?\n",
    "\n",
    "\n",
    "* __e) code:__ Complete the code in __`NaiveBayes/classify_mapper.py`__. Read the docstring carefully to understand how this script should work and the format it should return. Run the provided unit tests to confirm that your script works as expected then write a Hadoop streaming job to classify the Chinese example test set. [`HINT 1:` _you shouldn't need a reducer for this one._ `HINT 2:` _Don't forget to add the model file to the_ `-files` _parameter in your Hadoop streaming job so that it gets shipped to the mapper nodes where it will be accessed by your script._]\n",
    "\n",
    "\n",
    "* __f) short response:__ In our test example and in the Enron data set we have fairly short documents. Since these fit fine in memory on a mapper node we didn't need a reducer and could just do all of our calculations in the mapper. However with much longer documents (eg. books) we might want a higher level of parallelization -- for example we might want to process parts of a document on different nodes. In this hypothetical scenario how would our algorithm design change? What could the mappers still do? What key-value structure would they emit? What would the reducers have to do as a last step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 Student Answers:\n",
    "> __a)__ The key is the word in each line, the values are the fields after that. The first field shows how many times this word appears in c=0 classifications, the second field shows how many times this word appears in c=1 classifications, the third field shows the conditional probability of seeing this word given that c=0, and the final field shows the conditional probability of seeing this word given that c=1. We only use the final two fields for inference, so the first and second fields, showing the counts, can be used for debugging. The ClassPriors record is different from the rest because it signifies the class priors. \n",
    "\n",
    "> __b)__ When we work with logs, our long product of conditional probabilities turns into a summation. If it is a product, any single probability that is zero will make the whole result zero. Using logs and summation means that instead of zero, we get something very close to zero, but which still has a value. \n",
    "\n",
    "> __c)__ According to Figure 13.3 in the textbook, this word will be ignored and will not add anything when we don't have a conditional probability for it. \n",
    "\n",
    "> __d)__ This is a departure from functional programming in that our algorithm isn't stateless, since it depends on the results present in memory. This is useful in case where the information in memory is the same everywhere and is a constant. If the information is different across nodes or is being changed during the run, then processing run on one node with an input x might produce different outcomes if run on another node with the same input. \n",
    "\n",
    "> __e)__ Complete the coding portion of this question before answering 'f'.\n",
    "\n",
    "> __f)__ If we want to run multiple mappers on multiple nodes, the mappers can still sum up the log probabilities, but they will need to be reduced together with log probabilities from other parts of the document that ran on different mappers. They will emit the log probabilities with a document ID. The reducers would take this result for a document from several different mappers, sum it up and take the exponent, picking the right classification based on the final two probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these cells to create the example corpus and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing NaiveBayes/chineseTrain.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/chineseTrain.txt\n",
    "D1\t1\t\tChinese Beijing Chinese\n",
    "D2\t1\t\tChinese Chinese Shanghai\n",
    "D3\t1\t\tChinese Macao\n",
    "D4\t0\t\tTokyo Japan Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing NaiveBayes/chineseTest.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/chineseTest.txt\n",
    "D5\t1\t\tChinese Chinese Chinese Tokyo Japan\n",
    "D6\t1\t\tBeijing Shanghai Trade\n",
    "D7\t0\t\tJapan Macao Tokyo\n",
    "D8\t0\t\tTokyo Japan Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing NBmodel.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NBmodel.txt\n",
    "beijing\t0.0,1.0,0.111111111111,0.142857142857\n",
    "chinese\t1.0,5.0,0.222222222222,0.428571428571\n",
    "tokyo\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "shanghai\t0.0,1.0,0.111111111111,0.142857142857\n",
    "ClassPriors\t1.0,3.0,0.25,0.75\n",
    "japan\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "macao\t0.0,1.0,0.111111111111,0.142857142857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data files into HDFS\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseTrain.txt {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseTest.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your work for `part e` starts here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - do your work in NaiveBayes/classify_mapper.py first, then run this cell.\n",
    "!chmod a+x NaiveBayes/classify_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5\t1\t0.000135480702467\t0.000301213779972\t1\n",
      "d6\t1\t0.00308641975308\t0.0153061224489\t1\n",
      "d7\t0\t0.00137174211248\t0.000546647230321\t0\n",
      "d8\t0\t0.0123456790123\t0.00382653061225\t0\n"
     ]
    }
   ],
   "source": [
    "# part e - unit test NaiveBayes/classify_mapper.py (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/chinese-output\n"
     ]
    }
   ],
   "source": [
    "# part e - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob112883358079335547.jar tmpDir=null\n",
      "18/01/22 20:54:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/22 20:54:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/22 20:54:03 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/01/22 20:54:03 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/22 20:54:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0027\n",
      "18/01/22 20:54:03 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0027\n",
      "18/01/22 20:54:03 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0027/\n",
      "18/01/22 20:54:03 INFO mapreduce.Job: Running job: job_1516450633555_0027\n",
      "18/01/22 20:54:08 INFO mapreduce.Job: Job job_1516450633555_0027 running in uber mode : false\n",
      "18/01/22 20:54:08 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/22 20:54:15 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/01/22 20:54:16 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/22 20:54:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/22 20:54:20 INFO mapreduce.Job: Job job_1516450633555_0027 completed successfully\n",
      "18/01/22 20:54:20 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=179\n",
      "\t\tFILE: Number of bytes written=352125\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=403\n",
      "\t\tHDFS: Number of bytes written=165\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7320\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2929\n",
      "\t\tTotal time spent by all map tasks (ms)=7320\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2929\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7320\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2929\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7495680\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2999296\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=165\n",
      "\t\tMap output materialized bytes=185\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=185\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=104\n",
      "\t\tCPU time spent (ms)=1400\n",
      "\t\tPhysical memory (bytes) snapshot=759635968\n",
      "\t\tVirtual memory (bytes) snapshot=4098170880\n",
      "\t\tTotal committed heap usage (bytes)=619184128\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=165\n",
      "18/01/22 20:54:20 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-output\n"
     ]
    }
   ],
   "source": [
    "# part e - write your Hadooop streaming job here\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NBmodel.txt \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/chineseTest.txt \\\n",
    "  -output {HDFS_DIR}/chinese-output \\\n",
    "  -cmdenv PATH={PATH} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - retrieve test set results from HDFS\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-output/part-000* > NaiveBayes/chineseResults.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5\t1\t0.000135480702467\t0.000301213779972\t1\n",
      "d6\t1\t0.00308641975308\t0.0153061224489\t1\n",
      "d7\t0\t0.00137174211248\t0.000546647230321\t0\n",
      "d8\t0\t0.0123456790123\t0.00382653061225\t0\n"
     ]
    }
   ],
   "source": [
    "# part e - take a look\n",
    "!cat NaiveBayes/chineseResults.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th> Expected output for the test set:</th>\n",
    "<tr align=Left><td><pre>\n",
    "d5\t1\t0.00013548\t0.00030121\t1\n",
    "d6\t1\t0.00308641\t0.01530612\t1\n",
    "d7\t0\t0.00137174\t0.00054664\t0\n",
    "d8\t0\t0.01234567\t0.00382653\t0\n",
    "</pre></td><tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Naive Bayes Training.\n",
    "In Question 8 we used a model that we had trained by hand. Next we'll develop the code to do that same training in parallel, making it suitable for use with larger corpora (like the Enron emails). The end result of the MapReduce job you write in this question should be a model text file that looks just like the example (`NBmodel.txt`) that we created by hand above.\n",
    "\n",
    "To refresh your memory about the training process take a look at  `7a` and `7b` where you described the pieces of information you'll need to collect in order to encode a Multinomial Naive Bayes model. We now want to retrieve those pieces of information while streaming over a corpus. The bulk of the task will be very similar to the word counting excercises you've already done but you may want to consider a slightly different key-value record structure to efficiently tally counts for each class. \n",
    "\n",
    "The most challenging (interesting?) design question will be how to retrieve the totals (# of documents and # of words in documents for each class). Of course, counting these numbers is easy. The hard part is the timing: you'll need to make sure you have the counts totalled up _before_ you start estimating the class conditional probabilities for each word. It would be best (i.e. most scalable) if we could find a way to do this tallying without storing the whole vocabulary in memory... _can you see why this is gong to be a challenge?_ [`HINT:` _There is a problem with trying to tally in the mapper but also a problem with waiting to do it in the reducer. _]. __For part `b` of this question you will receive full credit for any design that results in the correct conditional probabilities _without hard coding the class totals_ (i.e. you _can_ store records in memory if you so choose).__ However, for fun, we challenge you to try and figure out the stateless solution. It involves a technique that you will learn formally next week but we think you may be able to figure it out for yourself. [`HINT:` _Think about Hadoop's default sorting behavior, what determines the order in which records arrive at the reducer?  If we compute partial totals in our mappers can you think of a way to make sure that those numbers arrive at our reducer before any of the other 'regular' key-value pairs?_`]\n",
    "\n",
    "__`IMPORTANT NOTE:`__ Please use a single reducer for all of the jobs in this question.\n",
    "\n",
    "### Q9 Tasks:\n",
    "* __a) make a plan:__  Fill in the docstrings for __`NaiveBayes/train_mapper.py`__ and __`NaiveBayes/train_reducer.py`__ to appropriately reflect the format that each script will input/output. [`HINT:` _the input files_ (`enronemail_1h.txt` & `chineseTrain.txt`) _have a prespecified format and your output file should match_ `NBmodel.txt` _so you really only have to decide on an internal format for Hadoop_].\n",
    "\n",
    "\n",
    "* __b) implement it:__ Complete the code in __`NaiveBayes/train_mapper.py`__ and __`NaiveBayes/train_reducer.py`__ so that together they train a Multinomial Naive Bayes model __with no smoothing__. Make sure your end result is formatted correctly (see note above). Test your scripts independently and together (using `chineseTrain.txt` or test input of your own devising). When you are satisfied with your Python code design and run a Hadoop streaming command to run your job in parallel on the __chineseTrain.txt__. Confirm that your trained model matches your hand calculations from Question 7.\n",
    "\n",
    "\n",
    "* __c) short response:__ We saw in Question 7 that adding Laplace +1 smoothing makes our classifications less sensitve to rare words. However implementing this technique requires access to one additional piece of information that we had not previously used in our Naive Bayes training. What is that extra piece of information? [`HINT:` see equation 13.7 in Manning, Raghavan and Schutze].\n",
    "\n",
    "\n",
    "* __d) short response:__ There are three approaches that we could take to handle the extra piece of information you identified in `c`: 1) we could hard code it into our reducer (_where would we get it in the first place?_). Or 2) we could compute it inside the reducer which would require storing some information in memory (_what information?_). Or 3) we could compute it in the reducer without storing any bulky information in memory but then we'd need some postprocessing or a second MapReduce job to complete the calculation (_why?_). Breifly explain what is non-ideal about each of these options. BONUS: which of these options is incompatible with using multiple reducers?\n",
    "\n",
    "\n",
    "* __e) code + short response:__ In future weeks we'll learn better ways to handle this kind of situation. For now, chose one of the 3 options above. State your choice & reasoning in the space below then use that strategy to complete the code in __`NaiveBayes/train_reducer_smooth.py`__. Test this alternate reducer then write and run a Hadoop streaming job to train an MNB model with smoothing on the Chinese example. Your results should match the model that we provided for you in Question 9 (and the calculations in the textbook example). [`HINT:` _don't start from scratch with this one -- you can just copy over your reducer code from part `b` and make the needed modifications_]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9 Student Answers:\n",
    "\n",
    "> __b)__ A note on this: my current solution is limited in that it only works for one reducer, and will need to be rewritten for multiple reducers. One possible solution involves having a second reducer after the first set of reducer(s), which will produce the final model. \n",
    "\n",
    "> __ c)__ This extra piece of information is the size of the whole vocabulary, all the unique words used in the training data. This will be 6 in this case. \n",
    "\n",
    "> __ d)__ 1) It may be difficult to have this information for a huge collection of documents and a significantly large corpus, since we might need to do an expensive pass of the complete data to calculate it. 2) we would need to store all the unique words we have seen so far while processing, as well as intermediate counts, but this has the drawback of taking up too much memory if we have a large corpus. This might also not be compatible with multiple reducers, since we depend on memory that will not be available across reducers.  3) The problem here is that we might need an extra step to count the total number of unique words at some point in the process. Even if this is after a reducer that has produced a single record per word, we still need to count the number of lines and put this result in memory for the final calculation. \n",
    "\n",
    "> __ e)__ I re-wrote my reducer so that it can now be run as multiple reducers for one hadoop job. The data it generates is explained in detail in its docstring. The produced file (intermediate) has V+2 lines, where V is the total number of unique words in all the documents. Counting the lines using wc -l and subtracting two gives us the vocabulary size needed for smoothing. The extra two lines contain information needed to calculate the prior probability and information on the number of words in each class. The final reducer, which has to be a single one, takes this information and does a single loop through the file to calculate conditional and prior probabilities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== MAPPER DOCSTRING ============\n",
      "Mapper reads in text documents and emits word counts by class.\n",
      "INPUT:\n",
      "    ID \\t SPAM \\t SUBJECT \\t CONTENT \\n\n",
      "OUTPUT:\n",
      "    word \\t class \\t count\n",
      "    \n",
      "=========== REDUCER DOCSTRING ============\n",
      "Reducer aggregates word counts by class and emits frequencies.\n",
      "\n",
      "INPUT:\n",
      "    word \\t class \\t count\n",
      "OUTPUT:\n",
      "    word \\t num_c0_class \\t num_c1_class \\t cond_prob_c0 \\t cond_prob_c1\n"
     ]
    }
   ],
   "source": [
    "# part a - do your work in train_mapper.py and train_reducer.py then RUN THIS CELL AS IS\n",
    "!chmod a+x NaiveBayes/train_mapper.py\n",
    "!chmod a+x NaiveBayes/train_reducer.py\n",
    "!echo \"=========== MAPPER DOCSTRING ============\"\n",
    "!head -n 8 NaiveBayes/train_mapper.py | tail -n 6\n",
    "!echo \"=========== REDUCER DOCSTRING ============\"\n",
    "!head -n 8 NaiveBayes/train_reducer.py | tail -n 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Q9 part c starts here`:__ MNB _without_ Smoothing (training on Chinese Example Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\t0\t1\n",
      "*\t1\t1\n",
      "*\t1\t1\n",
      "*\t1\t1\n",
      ".\t0\t1\n",
      ".\t0\t1\n",
      ".\t0\t1\n",
      ".\t1\t1\n",
      ".\t1\t1\n",
      ".\t1\t1\n",
      ".\t1\t1\n",
      ".\t1\t1\n",
      ".\t1\t1\n",
      ".\t1\t1\n",
      ".\t1\t1\n",
      "beijing\t1\t1\n",
      "chinese\t0\t1\n",
      "chinese\t1\t1\n",
      "chinese\t1\t1\n",
      "chinese\t1\t1\n",
      "chinese\t1\t1\n",
      "chinese\t1\t1\n",
      "japan\t0\t1\n",
      "macao\t1\t1\n",
      "shanghai\t1\t1\n",
      "tokyo\t0\t1\n"
     ]
    }
   ],
   "source": [
    "# part b - write a unit test for your mapper here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1,1 -k2,2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors\t1\t3\t0.25\t0.75\n",
      "beijing\t0\t0\t0.0\t0.0\n",
      "chinese\t1\t5\t0.333333333333\t0.625\n",
      "japan\t1\t0\t0.333333333333\t0.0\n",
      "macao\t0\t1\t0.0\t0.125\n",
      "shanghai\t0\t1\t0.0\t0.125\n",
      "tokyo\t1\t0\t0.333333333333\t0.0\n"
     ]
    }
   ],
   "source": [
    "# part b - write a unit test for your reducer here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1,1 -k2,2n | NaiveBayes/train_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors\t1\t3\t0.25\t0.75\n",
      "beijing\t0\t0\t0.0\t0.0\n",
      "chinese\t1\t5\t0.333333333333\t0.625\n",
      "japan\t1\t0\t0.333333333333\t0.0\n",
      "macao\t0\t1\t0.0\t0.125\n",
      "shanghai\t0\t1\t0.0\t0.125\n",
      "tokyo\t1\t0\t0.333333333333\t0.0\n"
     ]
    }
   ],
   "source": [
    "# part b - write a systems test for your mapper + reducer together here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1,1 -k2,2n | NaiveBayes/train_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/chinese-unsmooth-output\n"
     ]
    }
   ],
   "source": [
    "# part b - clear (and name) an output directory in HDFS for your unsmoothed chinese NB model\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-unsmooth-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6656376710911277347.jar tmpDir=null\n",
      "18/01/23 00:29:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/23 00:29:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/23 00:29:27 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/01/23 00:29:27 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/23 00:29:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0029\n",
      "18/01/23 00:29:28 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0029\n",
      "18/01/23 00:29:28 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0029/\n",
      "18/01/23 00:29:28 INFO mapreduce.Job: Running job: job_1516450633555_0029\n",
      "18/01/23 00:29:34 INFO mapreduce.Job: Job job_1516450633555_0029 running in uber mode : false\n",
      "18/01/23 00:29:34 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/23 00:29:39 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/01/23 00:29:40 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/23 00:29:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/23 00:29:45 INFO mapreduce.Job: Job job_1516450633555_0029 completed successfully\n",
      "18/01/23 00:29:45 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=435\n",
      "\t\tFILE: Number of bytes written=353711\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=387\n",
      "\t\tHDFS: Number of bytes written=182\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6852\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2634\n",
      "\t\tTotal time spent by all map tasks (ms)=6852\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2634\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6852\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2634\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7016448\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2697216\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=37\n",
      "\t\tMap output bytes=355\n",
      "\t\tMap output materialized bytes=441\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=17\n",
      "\t\tReduce shuffle bytes=441\n",
      "\t\tReduce input records=37\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=74\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=103\n",
      "\t\tCPU time spent (ms)=1480\n",
      "\t\tPhysical memory (bytes) snapshot=752758784\n",
      "\t\tVirtual memory (bytes) snapshot=4090966016\n",
      "\t\tTotal committed heap usage (bytes)=617611264\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=159\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=182\n",
      "18/01/23 00:29:45 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-unsmooth-output\n"
     ]
    }
   ],
   "source": [
    "# part b - write your hadoop streaming job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2n\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer.py \\\n",
    "  -input {HDFS_DIR}/chineseTrain.txt \\\n",
    "  -output {HDFS_DIR}/chinese-unsmooth-output \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - extract your results (i.e. model) to a local file\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-unsmooth-output/part-000* > NaiveBayes/chineseUnsmoothResults.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors\t1\t3\t0.25\t0.75\n",
      "beijing\t0\t1\t0.0\t0.125\n",
      "chinese\t1\t5\t0.333333333333\t0.625\n",
      "japan\t1\t0\t0.333333333333\t0.0\n",
      "macao\t0\t1\t0.0\t0.125\n",
      "shanghai\t0\t1\t0.0\t0.125\n",
      "tokyo\t1\t0\t0.333333333333\t0.0\n"
     ]
    }
   ],
   "source": [
    "# part b - print your model so that we can confirm that it matches expected results\n",
    "!cat NaiveBayes/chineseUnsmoothResults.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Q9 part e starts here`:__ MNB _with_ Smoothing (training on Chinese Example Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\t1\t3\n",
      ".\t3\t8\n",
      "beijing\t0\t1\n",
      "chinese\t1\t5\n",
      "japan\t1\t0\n",
      "macao\t0\t1\n",
      "shanghai\t0\t1\n",
      "tokyo\t1\t0\n"
     ]
    }
   ],
   "source": [
    "# part e - write a unit test for your NEW reducer here\n",
    "# This is an intermediate reducer, see docs for more details\n",
    "!chmod a+x NaiveBayes/train_reducer_smooth.py\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1,1 -k2,2n | NaiveBayes/train_reducer_smooth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the lines from this reducer, as we get the number of unique words \n",
    "# if we subtract two from the number. we save this in a txt file for use in next reducer\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1,1 -k2,2n | NaiveBayes/train_reducer_smooth.py | wc -l > NaiveBayes/p9WordCount.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors\t1\t3\t0.25\t0.75\n",
      "beijing\t0\t1\t0.111111111111\t0.142857142857\n",
      "chinese\t1\t5\t0.222222222222\t0.428571428571\n",
      "japan\t1\t0\t0.222222222222\t0.0714285714286\n",
      "macao\t0\t1\t0.111111111111\t0.142857142857\n",
      "shanghai\t0\t1\t0.111111111111\t0.142857142857\n",
      "tokyo\t1\t0\t0.222222222222\t0.0714285714286\n"
     ]
    }
   ],
   "source": [
    "# Passing all the data to the last reducer, which has to be single\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1,1 -k2,2n | NaiveBayes/train_reducer_smooth.py | sort -k1,1 -k2,2n | NaiveBayes/train_single_reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - clear (and name) an output directory in HDFS for your SMOOTHED chinese NB model\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-smooth-output-intermediate\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-smooth-output-final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3938595312589241639.jar tmpDir=null\n",
      "18/01/23 22:07:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/23 22:07:06 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/23 22:07:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/01/23 22:07:06 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/23 22:07:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0030\n",
      "18/01/23 22:07:06 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0030\n",
      "18/01/23 22:07:07 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0030/\n",
      "18/01/23 22:07:07 INFO mapreduce.Job: Running job: job_1516450633555_0030\n",
      "18/01/23 22:07:12 INFO mapreduce.Job: Job job_1516450633555_0030 running in uber mode : false\n",
      "18/01/23 22:07:12 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/23 22:07:18 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/01/23 22:07:19 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/23 22:07:23 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/23 22:07:23 INFO mapreduce.Job: Job job_1516450633555_0030 completed successfully\n",
      "18/01/23 22:07:23 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=275\n",
      "\t\tFILE: Number of bytes written=353529\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=387\n",
      "\t\tHDFS: Number of bytes written=79\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7257\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2752\n",
      "\t\tTotal time spent by all map tasks (ms)=7257\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2752\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7257\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2752\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7431168\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2818048\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=26\n",
      "\t\tMap output bytes=217\n",
      "\t\tMap output materialized bytes=281\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=11\n",
      "\t\tReduce shuffle bytes=281\n",
      "\t\tReduce input records=26\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=52\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=61\n",
      "\t\tCPU time spent (ms)=1420\n",
      "\t\tPhysical memory (bytes) snapshot=764129280\n",
      "\t\tVirtual memory (bytes) snapshot=4109004800\n",
      "\t\tTotal committed heap usage (bytes)=618659840\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=159\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=79\n",
      "18/01/23 22:07:23 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-smooth-output-intermediate\n"
     ]
    }
   ],
   "source": [
    "# part e - write your hadoop streaming job\n",
    "\n",
    "# we will use two hadoop streaming jobs with a count in the middle, as we want to apply a second reducer on the results of our first reducer\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2n\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer_smooth.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer_smooth.py \\\n",
    "  -input {HDFS_DIR}/chineseTrain.txt \\\n",
    "  -output {HDFS_DIR}/chinese-smooth-output-intermediate \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write and extract intermediate results\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-smooth-output-intermediate/part-000* > NaiveBayes/chineseSmoothResultsIntermediate.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\t1\t3\n",
      ".\t3\t8\n",
      "beijing\t0\t1\n",
      "chinese\t1\t5\n",
      "japan\t1\t0\n",
      "macao\t0\t1\n",
      "shanghai\t0\t1\n",
      "tokyo\t1\t0\n"
     ]
    }
   ],
   "source": [
    "# print intermediate results and write line count to file, to be used by last reducer\n",
    "!cat NaiveBayes/chineseSmoothResultsIntermediate.txt | wc -l > NaiveBayes/p9WordCount.txt\n",
    "!cat NaiveBayes/chineseSmoothResultsIntermediate.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4775445200809144717.jar tmpDir=null\n",
      "18/01/23 22:08:13 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/23 22:08:14 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/23 22:08:14 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/01/23 22:08:14 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/23 22:08:14 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0031\n",
      "18/01/23 22:08:15 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0031\n",
      "18/01/23 22:08:15 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0031/\n",
      "18/01/23 22:08:15 INFO mapreduce.Job: Running job: job_1516450633555_0031\n",
      "18/01/23 22:08:21 INFO mapreduce.Job: Job job_1516450633555_0031 running in uber mode : false\n",
      "18/01/23 22:08:21 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/23 22:08:26 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/01/23 22:08:27 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/23 22:08:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/23 22:08:31 INFO mapreduce.Job: Job job_1516450633555_0031 completed successfully\n",
      "18/01/23 22:08:31 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=101\n",
      "\t\tFILE: Number of bytes written=353196\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=405\n",
      "\t\tHDFS: Number of bytes written=275\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6947\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2650\n",
      "\t\tTotal time spent by all map tasks (ms)=6947\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2650\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6947\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2650\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7113728\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2713600\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=79\n",
      "\t\tMap output materialized bytes=107\n",
      "\t\tInput split bytes=286\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=107\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=77\n",
      "\t\tCPU time spent (ms)=1360\n",
      "\t\tPhysical memory (bytes) snapshot=751542272\n",
      "\t\tVirtual memory (bytes) snapshot=4098469888\n",
      "\t\tTotal committed heap usage (bytes)=549453824\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=119\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=275\n",
      "18/01/23 22:08:31 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-smooth-output-final\n"
     ]
    }
   ],
   "source": [
    "# write second hadoop job with no mapper and just the new reducer\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2n\" \\\n",
    "  -files NaiveBayes/p9WordCount.txt,NaiveBayes/train_single_reducer.py \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer train_single_reducer.py \\\n",
    "  -input {HDFS_DIR}/chinese-smooth-output-intermediate \\\n",
    "  -output {HDFS_DIR}/chinese-smooth-output-final \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - extract your results (i.e. model) to a local file\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-smooth-output-final/part-000* > NaiveBayes/chineseSmoothResultsFinal.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors\t1\t3\t0.25\t0.75\n",
      "beijing\t0\t1\t0.111111111111\t0.142857142857\n",
      "chinese\t1\t5\t0.222222222222\t0.428571428571\n",
      "japan\t1\t0\t0.222222222222\t0.0714285714286\n",
      "macao\t0\t1\t0.111111111111\t0.142857142857\n",
      "shanghai\t0\t1\t0.111111111111\t0.142857142857\n",
      "tokyo\t1\t0\t0.222222222222\t0.0714285714286\n"
     ]
    }
   ],
   "source": [
    "!cat NaiveBayes/chineseSmoothResultsFinal.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10: Enron Ham/Spam NB Classifier & Results.\n",
    "\n",
    "Fantastic work. We're finally ready to perform Spam Classification on the Enron Corpus. In this final homework question you'll run the analysis you've developed, report its performance, and draw some conclusions.\n",
    "\n",
    "### Q10 Tasks:\n",
    "* __a) train/test split:__ Run the provided code to split our Enron file into a training set and testing set then load them into HDFS. [`NOTE:` _If you hard coded the vocab size in question 9d make sure you re calculate the vocab size for just the training set!_]\n",
    "\n",
    "\n",
    "* __b) train 2 models:__ Write Hadoop Streaming jobs to train MNB Models on the training set with and without smoothing. Save your models to local files at __`NaiveBayes/Unsmoothed/NBmodel.txt`__ and __`NaiveBayes/Smoothed/NBmodel.txt`__. [`NOTE:` _This naming is important because we wrote our classification task so that it expects a file of that name... if this inelegance frustrates you there is an alternative that would involve a few adjustments to your code [read more about it here](http://www.tnoda.com/blog/2013-11-23). If you can let it slide for now, MRJob (the next framework we'll learn) will have an easier way to handle this._] Finally run the checks that we provide to confirm that your results are correct.\n",
    "\n",
    "\n",
    "* __c) code:__ Recall that we designed our classification job with just a mapper. An efficient way to report the performance of our models would be to simply add a reducer phase to this job and compute precision and recall right there. Complete the code in __`NaiveBayes/evaluation_reducer.py`__ and then write Hadoop jobs to evaluate your two models on the test set. Report their performance side by side. [`NOTE:` if you need a refresher on precision, recall and F1-score [Wikipedia](https://en.wikipedia.org/wiki/F1_score) is a good resource.]\n",
    "\n",
    "\n",
    "* __d) short response:__ Compare the performance of your two models. What do you notice about the unsmoothed model's predictions? Can you guess why this is happening? Which evaluation measure do you think is most relevant in our use case? [`NOTE:` _Feel free to answer using your common sense but if you want more information on evaluating the classification task checkout_ [this blogpost](https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/\n",
    ") or [this paper](http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf\n",
    ")]\n",
    "\n",
    "\n",
    "* __e) code + short response:__ Let's look at the top ten words with the highest conditional probability in `Spam` and in `Ham`. We'll do this by writing a Hadoop job that sorts the model file (`NaiveBayes/Smoothed/NBmodel.py`). Normally we'd have to run two jobs -- one that sorts on $P(word|ham)$ and another that sorts on $P(word|spam)$. However if we slighly modify the data format in the model file then we can get the top words in each class with just one job. We've written a mapper that will do just this for you. Read through __`NaiveBayes/model_sort_mapper.py`__ and then briefly explain how this mapper will allow us to partition and sort our model file. Write a Hadoop job that uses our mapper and `/bin/cat` for a reducer to partition and sort. Print out the top 10 words in each class (where 'top' == highest conditional probability).[`HINT:` _this should remind you a lot of what we did in Question 6._]\n",
    "\n",
    "\n",
    "* __f) short response:__ What do you notice about the 'top words' we printed in `e`? How would increasing the smoothing parameter 'k' affect the probabilities for the top words that you identified for 'e'. How would they affect the probabilities of words that occur much more in one class than another? In summary, how does the smoothing parameter 'k' affect the bias and the variance of our model. [`NOTE:` _you do not need to code anything for this task, but if you are struggling with it you could try changing 'k' and see what happens to the test set. We don't recommend doing this exploration with the Enron data because it will be harder to see the impact with such a big vocabulary_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10 Student Answers:\n",
    "> __d)__ \n",
    "\n",
    "> __e)__ Type your answer here!\n",
    "\n",
    "> __f)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test/Train split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - test/train split (RUN THIS CELL AS IS)\n",
    "!head -n 80 data/enronemail_1h.txt > data/enron_train.txt\n",
    "!tail -n 20 data/enronemail_1h.txt > data/enron_test.txt\n",
    "!hdfs dfs -copyFromLocal data/enron_train.txt {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal data/enron_test.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _without smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/enron-model\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3429723021223339073.jar tmpDir=null\n",
      "18/01/24 17:52:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/24 17:52:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/24 17:52:32 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/01/24 17:52:32 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/24 17:52:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0042\n",
      "18/01/24 17:52:32 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0042\n",
      "18/01/24 17:52:32 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0042/\n",
      "18/01/24 17:52:32 INFO mapreduce.Job: Running job: job_1516450633555_0042\n",
      "18/01/24 17:52:38 INFO mapreduce.Job: Job job_1516450633555_0042 running in uber mode : false\n",
      "18/01/24 17:52:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/24 17:52:44 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/01/24 17:52:45 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/24 17:52:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/24 17:52:51 INFO mapreduce.Job: Job job_1516450633555_0042 completed successfully\n",
      "18/01/24 17:52:51 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=494510\n",
      "\t\tFILE: Number of bytes written=1341822\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=167388\n",
      "\t\tHDFS: Number of bytes written=165309\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8456\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3105\n",
      "\t\tTotal time spent by all map tasks (ms)=8456\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3105\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8456\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3105\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8658944\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3179520\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=80\n",
      "\t\tMap output records=50214\n",
      "\t\tMap output bytes=394076\n",
      "\t\tMap output materialized bytes=494516\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5422\n",
      "\t\tReduce shuffle bytes=494516\n",
      "\t\tReduce input records=50214\n",
      "\t\tReduce output records=4556\n",
      "\t\tSpilled Records=100428\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=86\n",
      "\t\tCPU time spent (ms)=2920\n",
      "\t\tPhysical memory (bytes) snapshot=759230464\n",
      "\t\tVirtual memory (bytes) snapshot=4091826176\n",
      "\t\tTotal committed heap usage (bytes)=618659840\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167162\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=165309\n",
      "18/01/24 17:52:51 INFO streaming.StreamJob: Output directory: /user/root/HW2/enron-model\n",
      "mkdir: cannot create directory `NaiveBayes/Unsmoothed': File exists\n"
     ]
    }
   ],
   "source": [
    "# part b -  Unsmoothed model (FILL IN THE MISSING CODE BELOW)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/enron-model\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2n\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_train.txt \\\n",
    "  -output {HDFS_DIR}/enron-model \\\n",
    "# save the model locally\n",
    "!mkdir NaiveBayes/Unsmoothed\n",
    "!hdfs dfs -cat {HDFS_DIR}/enron-model/part-000* > NaiveBayes/Unsmoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2,4,0.000172547666293,0.000296823983378\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.000172547666293,0.000296823983378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1,22,8.62738331464e-05,0.00163253190858\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,8.62738331464e-05,0.00163253190858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _with Laplace +1 smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/smooth-model\n",
      "Deleted /user/root/HW2/smooth-model-output-intermediate\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7100281208411216763.jar tmpDir=null\n",
      "18/01/24 17:53:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/24 17:53:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/24 17:53:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/01/24 17:53:16 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/24 17:53:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0043\n",
      "18/01/24 17:53:16 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0043\n",
      "18/01/24 17:53:16 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0043/\n",
      "18/01/24 17:53:16 INFO mapreduce.Job: Running job: job_1516450633555_0043\n",
      "18/01/24 17:53:22 INFO mapreduce.Job: Job job_1516450633555_0043 running in uber mode : false\n",
      "18/01/24 17:53:22 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/24 17:53:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/24 17:53:34 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/24 17:53:34 INFO mapreduce.Job: Job job_1516450633555_0043 completed successfully\n",
      "18/01/24 17:53:34 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=494510\n",
      "\t\tFILE: Number of bytes written=1341990\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=167388\n",
      "\t\tHDFS: Number of bytes written=53414\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8414\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3056\n",
      "\t\tTotal time spent by all map tasks (ms)=8414\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3056\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8414\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3056\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8615936\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3129344\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=80\n",
      "\t\tMap output records=50214\n",
      "\t\tMap output bytes=394076\n",
      "\t\tMap output materialized bytes=494516\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5422\n",
      "\t\tReduce shuffle bytes=494516\n",
      "\t\tReduce input records=50214\n",
      "\t\tReduce output records=4557\n",
      "\t\tSpilled Records=100428\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=97\n",
      "\t\tCPU time spent (ms)=2970\n",
      "\t\tPhysical memory (bytes) snapshot=768204800\n",
      "\t\tVirtual memory (bytes) snapshot=4100083712\n",
      "\t\tTotal committed heap usage (bytes)=619184128\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167162\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=53414\n",
      "18/01/24 17:53:34 INFO streaming.StreamJob: Output directory: /user/root/HW2/smooth-model-output-intermediate\n",
      "mkdir: cannot create directory `NaiveBayes/Smoothed': File exists\n"
     ]
    }
   ],
   "source": [
    "# part b -  Smoothed model (FILL IN THE MISSING CODE BELOW)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smooth-model\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smooth-model-output-intermediate\n",
    "\n",
    "# hadoop command\n",
    "# we will use two hadoop streaming jobs with a count in the middle, as we want to apply a second reducer on the results of our first reducer\n",
    "\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2n\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer_smooth.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer_smooth.py \\\n",
    "  -input {HDFS_DIR}/enron_train.txt \\\n",
    "  -output {HDFS_DIR}/smooth-model-output-intermediate\n",
    "\n",
    "\n",
    "# write and extract intermediate results\n",
    "!mkdir NaiveBayes/Smoothed\n",
    "!hdfs dfs -cat {HDFS_DIR}/smooth-model-output-intermediate/part-000* > NaiveBayes/Smoothed/Intermediate.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3709233855020527961.jar tmpDir=null\n",
      "18/01/24 17:53:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/24 17:53:49 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/24 17:53:49 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/01/24 17:53:49 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/24 17:53:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0044\n",
      "18/01/24 17:53:50 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0044\n",
      "18/01/24 17:53:50 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0044/\n",
      "18/01/24 17:53:50 INFO mapreduce.Job: Running job: job_1516450633555_0044\n",
      "18/01/24 17:53:56 INFO mapreduce.Job: Job job_1516450633555_0044 running in uber mode : false\n",
      "18/01/24 17:53:56 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/24 17:54:01 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/01/24 17:54:02 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/24 17:54:08 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/24 17:54:08 INFO mapreduce.Job: Job job_1516450633555_0044 completed successfully\n",
      "18/01/24 17:54:08 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=62534\n",
      "\t\tFILE: Number of bytes written=478038\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=57792\n",
      "\t\tHDFS: Number of bytes written=217149\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7138\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3943\n",
      "\t\tTotal time spent by all map tasks (ms)=7138\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3943\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7138\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3943\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7309312\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4037632\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4557\n",
      "\t\tMap output records=4557\n",
      "\t\tMap output bytes=53414\n",
      "\t\tMap output materialized bytes=62540\n",
      "\t\tInput split bytes=282\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4557\n",
      "\t\tReduce shuffle bytes=62540\n",
      "\t\tReduce input records=4557\n",
      "\t\tReduce output records=4556\n",
      "\t\tSpilled Records=9114\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=89\n",
      "\t\tCPU time spent (ms)=2020\n",
      "\t\tPhysical memory (bytes) snapshot=745287680\n",
      "\t\tVirtual memory (bytes) snapshot=4089233408\n",
      "\t\tTotal committed heap usage (bytes)=548929536\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=57510\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=217149\n",
      "18/01/24 17:54:08 INFO streaming.StreamJob: Output directory: /user/root/HW2/smooth-model\n"
     ]
    }
   ],
   "source": [
    "# print intermediate results and write line count to file, to be used by last reducer\n",
    "!cat NaiveBayes/Smoothed/Intermediate.txt | wc -l > NaiveBayes/Smoothed/p9WordCount.txt\n",
    "\n",
    "# write second hadoop job with no mapper and just the new reducer\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2n\" \\\n",
    "  -files NaiveBayes/Smoothed/p9WordCount.txt,NaiveBayes/train_single_reducer.py \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer train_single_reducer.py \\\n",
    "  -input {HDFS_DIR}/smooth-model-output-intermediate \\\n",
    "  -output {HDFS_DIR}/smooth-model \n",
    "\n",
    "# save the model locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/smooth-model/part-000* > NaiveBayes/Smoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2,4,0.000185804533631,0.000277300205202\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.000185804533631,0.000277300205202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1,22,0.000123869689087,0.00127558094393\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,0.000123869689087,0.00127558094393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - write your code in NaiveBayes/evaluation_reducer.py then RUN THIS CELL\n",
    "!chmod a+x NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5\t1\t0.000135480702467\t0.000301213779972\t1\n",
      "d6\t1\t0.00308641975308\t0.0153061224489\t1\n",
      "d7\t0\t0.00137174211248\t0.000546647230321\t0\n",
      "d8\t0\t0.0123456790123\t0.00382653061225\t0\n",
      "d5\t1\t0.000135480702467\t0.000301213779972\tTrue\n",
      "d6\t1\t0.00308641975308\t0.0153061224489\tTrue\n",
      "d7\t0\t0.00137174211248\t0.000546647230321\tTrue\n",
      "d8\t0\t0.0123456790123\t0.00382653061225\tTrue\n",
      "Precision 1.0 \n",
      "Recall 1.0 \n",
      "Accuracy 1.0 \n",
      "F-score 1.0 \n"
     ]
    }
   ],
   "source": [
    "# part c - unit test your evaluation job on the chinese model (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py \n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py | NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/classify-unsmooth-output\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob1845833187154449192.jar tmpDir=null\n",
      "18/01/24 17:54:35 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/24 17:54:35 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/24 17:54:36 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/01/24 17:54:36 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/24 17:54:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0045\n",
      "18/01/24 17:54:36 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0045\n",
      "18/01/24 17:54:36 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0045/\n",
      "18/01/24 17:54:36 INFO mapreduce.Job: Running job: job_1516450633555_0045\n",
      "18/01/24 17:54:42 INFO mapreduce.Job: Job job_1516450633555_0045 running in uber mode : false\n",
      "18/01/24 17:54:42 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/24 17:54:47 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/01/24 17:54:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/24 17:54:52 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/24 17:54:52 INFO mapreduce.Job: Job job_1516450633555_0045 completed successfully\n",
      "18/01/24 17:54:52 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=750\n",
      "\t\tFILE: Number of bytes written=354473\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49908\n",
      "\t\tHDFS: Number of bytes written=828\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7307\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2692\n",
      "\t\tTotal time spent by all map tasks (ms)=7307\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2692\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7307\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2692\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7482368\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2756608\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=704\n",
      "\t\tMap output materialized bytes=756\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=756\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=24\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=63\n",
      "\t\tCPU time spent (ms)=1290\n",
      "\t\tPhysical memory (bytes) snapshot=750379008\n",
      "\t\tVirtual memory (bytes) snapshot=4097843200\n",
      "\t\tTotal committed heap usage (bytes)=548929536\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49684\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=828\n",
      "18/01/24 17:54:52 INFO streaming.StreamJob: Output directory: /user/root/HW2/classify-unsmooth-output\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the UNSMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/classify-unsmooth-output\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NaiveBayes/evaluation_reducer.py,NaiveBayes/Unsmoothed/NBmodel.txt \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/classify-unsmooth-output \\\n",
    "  -cmdenv PATH={PATH} \n",
    "\n",
    "\n",
    "# retrieve results locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/classify-unsmooth-output/part-000* > NaiveBayes/Unsmoothed/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/classify-smooth-output': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob1748274860188178654.jar tmpDir=null\n",
      "18/01/24 17:55:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/24 17:55:05 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/24 17:55:05 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/01/24 17:55:05 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/24 17:55:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0046\n",
      "18/01/24 17:55:06 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0046\n",
      "18/01/24 17:55:06 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0046/\n",
      "18/01/24 17:55:06 INFO mapreduce.Job: Running job: job_1516450633555_0046\n",
      "18/01/24 17:55:11 INFO mapreduce.Job: Job job_1516450633555_0046 running in uber mode : false\n",
      "18/01/24 17:55:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/24 17:55:17 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/01/24 17:55:18 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/24 17:55:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/24 17:55:22 INFO mapreduce.Job: Job job_1516450633555_0046 completed successfully\n",
      "18/01/24 17:55:22 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1014\n",
      "\t\tFILE: Number of bytes written=354989\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49908\n",
      "\t\tHDFS: Number of bytes written=1117\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6975\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2846\n",
      "\t\tTotal time spent by all map tasks (ms)=6975\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2846\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6975\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2846\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7142400\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2914304\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=968\n",
      "\t\tMap output materialized bytes=1020\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=1020\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=24\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=79\n",
      "\t\tCPU time spent (ms)=1250\n",
      "\t\tPhysical memory (bytes) snapshot=742395904\n",
      "\t\tVirtual memory (bytes) snapshot=4089163776\n",
      "\t\tTotal committed heap usage (bytes)=552599552\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49684\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1117\n",
      "18/01/24 17:55:22 INFO streaming.StreamJob: Output directory: /user/root/HW2/classify-smooth-output\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the SMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/classify-smooth-output\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NaiveBayes/evaluation_reducer.py,NaiveBayes/Smoothed/NBmodel.txt \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/classify-smooth-output \\\n",
    "  -cmdenv PATH={PATH} \n",
    "\n",
    "\n",
    "# retrieve results locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/classify-smooth-output/part-000* > NaiveBayes/Smoothed/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== UNSMOOTHED MODEL ============\n",
      "0018.1999-12-14.kaminski\t0\t0.0\t0.0\tTrue\n",
      "0018.2001-07-13.sa_and_hp\t1\t0.0\t0.0\tFalse\n",
      "0018.2003-12-18.gp\t1\t0.0\t0.0\tFalse\n",
      "no precision\t\n",
      "no recall\t\n",
      "Accuracy 0.45 \t\n",
      "no f-score\t\n",
      "=========== SMOOTHED MODEL ============\n",
      "0017.2004-08-01.bg\t1\t2.73878566681e-62\t1.95150799641e-59\tTrue\n",
      "0017.2004-08-02.bg\t1\t0.0\t0.0\tFalse\n",
      "0018.1999-12-14.kaminski\t0\t0.0\t0.0\tTrue\n",
      "0018.2001-07-13.sa_and_hp\t1\t0.0\t0.0\tFalse\n",
      "0018.2003-12-18.gp\t1\t0.0\t0.0\tFalse\n",
      "Precision 0.75 \t\n",
      "Recall 0.545454545455 \t\n",
      "Accuracy 0.65 \t\n",
      "F-score 0.631578947368 \t\n"
     ]
    }
   ],
   "source": [
    "# part c - display results (RUN THIS CELL AS IS*)\n",
    "# NOTE: *feel free to modify the tail commands to match the format of your results file\n",
    "print '=========== UNSMOOTHED MODEL ============'\n",
    "!tail -n 7 NaiveBayes/Unsmoothed/results.txt\n",
    "print '=========== SMOOTHED MODEL ============'\n",
    "!tail -n 9 NaiveBayes/Smoothed/results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`EXPECTED RESULTS:`__ \n",
    "<table>\n",
    "<th>Unsmoothed Model</th>\n",
    "<th>Smoothed Model</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "# Documents:\t20\n",
    "True Positives:\t0\n",
    "True Negatives:\t9\n",
    "False Positives:\t0\n",
    "False Negatives:\t11\n",
    "Accuracy\t0.45\n",
    "Recall\t0.0\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "# Documents:\t20\n",
    "True Positives:\t6\n",
    "True Negatives:\t7\n",
    "False Positives:\t2\n",
    "False Negatives:\t5\n",
    "Accuracy\t0.65\n",
    "Precision\t0.75\n",
    "Recall\t0.5454\n",
    "F-Score\t0.6315\n",
    "</pre></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "__`NOTE:`__ _Don't be too disappointed if these seem low to you. We've trained and tested on a very very small corpus... bigger datasets coming soon!_\n",
    "\n",
    "__`Q10 part d starts here:`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - write your Hadoop job here (sort smoothed model on P(word|class))\n",
    "\n",
    "# clear output directory\n",
    "\n",
    "# hadoop job\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - print top words in each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Question: Sorting with multiple Reducers (Optional)\n",
    "\n",
    "Congratulations you've completed all the required questions! If you have energy for more, let's take a moment to preview one of the important design issues that we'll tackle next week: total order sort.\n",
    "\n",
    "In the jobs in this assignment we've mostly used a single reducer and mostly used the default Hadoop sorting (alphabetical by key). However when we took the time to specify an alternate key to sort on and tried using multiple reducers (as we did in Questions 5d, 6e and 10d)... we saw that sorting and partitioning the data on different fields can cause our final output to no longer be sorted from top to bottom. Instead it will usually be sorted just within each partition. This is called a partial sort and in the examples we saw that was mostly what we wanted.\n",
    "\n",
    "So far, we've also always partitioned on a categorical field. But what if we wanted to sort on values (eg. counts in a word count file) but have too much data to fit on a single reducer and there isn't a logical categorical variable to use as a partition key? How could we ensure the the top-n words end up in `part-00000`, the next n end up in `part-00001`, etc? The solution is similar in idea to what we did in Question 10b (where we created extra fields to partition on)... but a bit more complicated because the way that Hadoop orders its partitions and writes them to file depends on an internal hash function (eg. we might expect the partition with key `ham` to end up in `part-00000` and `spam` to end up in `part-00001` but Hadoop won't necessarily order the words the way a human reader would).\n",
    "\n",
    "__Challenge Exercise:__ Write a Hadoop job that will use 5 reducers and will sort your `NBmodel.txt` file (you can use the smoothed or unsmoothed one) in descending order according to the words' conditional probability in `spam`. A few tips:\n",
    "* start by reading the [Total Sort Notebook](https://github.com/UCB-w261/main/blob/master/Resources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb) that is provided in the course repo under the HelpfulResources folder.\n",
    "* you will need to write a mapper for this job, you can do this easily on the fly using the jupyter magic command `%%writefile`.\n",
    "* when you run your job, you should see that the records in `part-00000` have higher conditional probabilities in Spam, than the records in `part-00001` which in turn are higher than in `part-00002` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demo/bonus_mapper.py\n",
    "\"\"\"\n",
    "Write your mapper here!\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS - write your hadoop job here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS - show us it worked here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 ends here, please refer to the `README.md` for submission instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
