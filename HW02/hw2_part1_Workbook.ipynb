{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2 (part 1) - Intro to Hadoop Streaming\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2018`__\n",
    "\n",
    "Last week you implemented your first MapReduce Algorithm using a bash script framework. We saw that adding a sorting component to our framework allowed us to write a more efficient reducer script and perform word counting in parallel. In the first part of this week's assignment we'll introduce a new framework: Hadoop Streaming. Like before, you'll write mapper and reducer scripts in python then pass them to the framework which will stream over your input files, split them into chunks and sort to your specification. Although Hadoop Streaming is not used in production anymore it is the precursor to systems like Spark and as such is a useful way to illustrate key concepts in parallel computation. By the end of part 1 of this homework you should be able to:\n",
    "* __... describe__ the main components and default behavior of the Hadoop Streaming framework.\n",
    "* __... write__ a Hadoop MapReduce job from scratch.\n",
    "* __... access__ the Hadoop Streaming UI and use it in debugging your jobs.\n",
    "* __... design__ Hadoop MapReduce implementations for simple tasks like counting and ordering.\n",
    "* __... explain__ why sorting with multiple reducers requires some extra work (as opposed to sorting with a single reducer).\n",
    "\n",
    "A few things to keep in mind during this portion of the assigment: Hadoop Streaming can be a bit of a steep learning curve if you've never used it before because the syntax is very particular. We encourage you to rigorously test your python scripts before passing them to the Hadoop job and pay careful attention to the order in which you specify the Hadoop job parameters. __Please refer to the `README` for homework submission instructions and additional resources.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, lets set a few global variables for paths you'll use frequently. __`NOTE:`__ _you may need to modify the jar file and HDFS (or local home) directory paths to match your environment. The paths below should work on the course Docker image. Refer to_ [this debugging FAQ](.../HelpfulResources/debugging.md) _if you are unsure of the correct paths or encounter errors._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to java archive files for the hadoop streaming application on your machine\n",
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.7.0.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hdfs directory where  we'll store files for this assignment\n",
    "HDFS_DIR = \"/user/root/HW2\"\n",
    "!hdfs dfs -mkdir {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# local directory where you've cloned the course assignments repo \n",
    "# eg. /media/notebooks/w261-main/Assignments\n",
    "HOME_DIR = \"/media/notebooks\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store notebook environment path\n",
    "from os import environ\n",
    "PATH  = environ['PATH']\n",
    "# NOTE: we can pass this variable to our Hadoop Jobs using -cmdenv PATH={PATH}\n",
    "# This will ensure that, among other things, Hadoop uses the right python version.\n",
    "# You should not *need* this until the very last question in part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part of HW2 we'll continue working with the  _Alice in Wonderland_ text file from HW 1 and the test file we created for debugging. Run the following cell to confirm that you have access to these files and save their location. __`NOTE:`__ _if the files are missing for some reason refer to HW1 to reload them and/or adjust the paths below to match their location on your machine._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice.txt\t  alice_counts_A-Z.txt\t   alice_test.txt\n",
      "alice.txt.output  alice_counts_sorted.txt  alice_test.txt.output\n",
      "alice_counts.txt  alice_pCounts.txt\t   tmp\n"
     ]
    }
   ],
   "source": [
    "# check the HW01 data directory for the 'alice.txt' and 'alice_test.txt' files \n",
    "!ls {HOME_DIR}/HW01/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the paths -- ADJUST THIS CELL AS NEEDED\n",
    "ALICE_TXT = HOME_DIR + \"/HW01/data/alice.txt\"\n",
    "TEST_TXT = HOME_DIR + \"/HW01/data/alice_test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Programming Paradigm\n",
    "The week 2 reading from _Data Intensive Text Processing With Map Reduce_ by Lin and Dyer gave a high level overview of the key issues faced by parallel computation frameworks. It also introduced the Hadoop MapReduce framework. The questions below are designed to make sure you captured the key points from this reading. Feel free to answer each one very briefly.\n",
    "\n",
    "### Q1 Tasks:\n",
    "\n",
    "* __a) short response:__ What is MapReduce? How does it differ from Hadoop?\n",
    "\n",
    "* __b) short response:__ What \"programming paradigm\" is Hadoop MapReduce based on? What are the main ideas of this programming paradigm and how does MapReduce exemplify these ideas?\n",
    "\n",
    "* __c) short response:__ What is the basic data structure used in Hadoop MapReduce?\n",
    "\n",
    "* __d) short response:__ What does 'data/code co-location' mean? How does this principle contribute to the efficiency of a distributed computation?\n",
    "\n",
    "* __e) short response:__ What is a race condition in the context of parallel computation? Give an example.\n",
    "\n",
    "* __f) short response:__ What kind of _synchronization_ does Hadoop MapReduce perform by default? As a result of this synchronization, what is  Hadoop MapReduce's default sorting behavior? What aspect of the synchronization process is computationally costly?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Student Answers:\n",
    "> __a)__ MapReduce is a programming model that makes use of a user-defined mapper and reducer to process large quantites of data in parallel. Hadoop is one of the leading frameworks that can be set up on a cluster of computers, allowing for the execution of MapReduce algorithms\n",
    "\n",
    "> __b)__ Hadoop MapReduce is based on the divide and conquer paradigm, which starts by dividing the data and performing parallel operations to parts of it at the same time. In MapReduce, Hadoop splits the data and passes it to mappers, which can map each datum to a specific value. Then data from the mappers is sorted and passed to specific reducers by key, which then reduce the data to get the final information. \n",
    " \n",
    "> __c)__ Key-value pairs and arrays are extensively used in MR jobs.   \n",
    "\n",
    "> __d)__ This means that the code is put where the data is, so that data doesn't need to be moved around so much, as this is a computationally expensive operation.  \n",
    "\n",
    "> __e)__ When two operations (such as squaring) run in parallel and have to square the same variable ( with value 2). If the operations are performed sequentially, you would get 16. If instead they both read the value and square it separately, you would get 4. The problem with race conditions is that you don't know which of the two cases above will happen without synchronization of operations. \n",
    "\n",
    "> __f)__ Haddop MR synchronizes the processing by employing a shuffle between the map and reduce phase. All the parallel operations during the map and during the reduce phase are embarassingly parallel and don't share any resources, so there is no need for synchronization there. The shuffle phase sorts the output of all the mappers by keys and sends the data to the reducers. Sending the data around is computationally costly as it involves network transfer and disk writes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: WordCount in Hadoop MapReduce\n",
    "A basic Hadoop MapReduce job consists of three components: a mapper script, a reducer script and a  line of code in which you pass these scripts to the Hadoop streaming application/framework. The mapper and reducer can be any executable that will read from `stdin` and write to `stdout` (including `/bin/cat` which simply passes on the lines of your file unchanged). For the Hadoop jobs you write in this assignment you will use python scripts similar to those you wrote in HW1. In this question we've provided an example of a Hadoop MapReduce job that performs word counting on an input file of your choice. The mapper and reducer are provided at __`WordCount/mapper.py`__ and __`WordCount/reducer.py`__. The Hadoop streaming command is in a cell below. As you read through the example we provide and go on to write your own Hadoop MapReduce jobs you may want to refer to Michael Noll's [blogpost on writing an MapReduce job](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/) and/or the [Hadoop Streaming documention](https://hadoop.apache.org/docs/r1.2.1/streaming.html).\n",
    "\n",
    "### Q2 Tasks:\n",
    "* __a) write docstrings:__ Read through the WordCount mapper and reducer script and fill in the docstrings for each file so that it (briefly) explains what the script does and the expected input/output record formats. [`HINT`: _docstrings are a way to record information to help your reader (future-self/collaborator/grader) quickly orient to a piece of code. They should describe_ __what__ (_not_ how) _is being done. For more information refer to the [PEP 8 Style Guide for Python](https://www.python.org/dev/peps/pep-0008/)_]\n",
    "\n",
    "\n",
    "* __b) short response:__ What are the 'keys' and what are the 'values' in this MapReduce job? What delimiter separates them when we write to standard output? How will you expect Hadoop to sort the records emitted by the mapper script? Why is this order important given how the reducer script is written?\n",
    "\n",
    "\n",
    "* __c) run provided code:__ Run the cells provided to make sure that your mapper and reducer scripts are executable, load the input files into HDFS and clear the HDFS directory where the job will write its output. You will need to do these preparation steps for all future Hadoop MapReduce jobs.\n",
    "\n",
    "\n",
    "* __d) unit test:__ A good habit when writing Hadoop streaming jobs is to test your mappers and reducers locally before passing them to a Hadoop streaming command. An easy way to do this is to pipe in a small line of text. We've provided the unix code to do so and added a unix sort to mimic Hadoop's default sorting. Run these cells to confirm that our mapper and reducer work properly.\n",
    "\n",
    "\n",
    "* __e) code:__ We've provided the code to run your Hadoop streaming command on the test file. Read through this command and be sure you understand each paramater that we're passing in, then run it and confirm that the output performs word counting correctly. Finally, modify the code provided to run the Hadoop MapReduce job on the _Alice and Wonderland_ text instead of the test file. Remember that the input path you pass to the Hadoop streaming command should be a location in HDFS not a local path. Take a look at the ouptput and confirm you get the same count for 'alice' as in HW1. Does the sorting match what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q2 Student Answers:\n",
    "> __b)__ The keys are individual words, and the values are their counts. They are separated by a tab (\\t). Hadoop should order the mapper results by key so that they can be fed to the reducer in the right order. The reducer depends on this order, since it doesn't use a hash map to record intermediate results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Mapper to be called by Hadoop Streaming for performing word count\n",
      "INPUT:\n",
      "    Lines of text passed to stdin, where individual words are specified by spaces. \n",
      "    Example:\n",
      "    the quick brown fox jumped over the lazy dog\n",
      "OUTPUT:\n",
      "    Each word encountered in the text, followed by a tab and int(1)\n",
      "    Example:\n",
      "    the     1\n",
      "    quick   1\n"
     ]
    }
   ],
   "source": [
    "# part a - display docstring you wrote in WordCount/mapper.py (RUN THIS CELL AS IS)\n",
    "!head -n 12 ./WordCount/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "\"\"\"    \n",
      "Reducer used for counting words.\n",
      "INPUT:\n",
      "    Key-value pairs read from stdin in the form word\\tcount \n",
      "    for a set of word and all their counts, sorted by key. \n",
      "OUTPUT:\n",
      "    key-value pairs to stdout with each word and its number of counts. \n"
     ]
    }
   ],
   "source": [
    "# part a - display the docstring you wrote in WordCount/reducer.py (RUN THIS CELL AS IS)\n",
    "!head -n 8 ./WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep for Hadoop Streaming Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part c - make sure the mapper and reducer are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x WordCount/mapper.py\n",
    "!chmod a+x WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part c - load the input files into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal {TEST_TXT} {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal {ALICE_TXT} {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/wordcount-output\n"
     ]
    }
   ],
   "source": [
    "# part c - clear the output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/wordcount-output\n",
    "# NOTE: this directly won't exist unless you are re-running a job, that's fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit test your scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\t1\n",
      "foo\t1\n",
      "quux\t1\n",
      "labs\t1\n",
      "foo\t1\n",
      "bar\t1\n",
      "quux\t1\n"
     ]
    }
   ],
   "source": [
    "# part d - unit test mapper script (RUN THIS CELL AS IS)\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\t2\n",
      "quux\t1\n",
      "labs\t1\n",
      "foo\t1\n",
      "bar\t1\n",
      "quux\t1\n"
     ]
    }
   ],
   "source": [
    "# part d - unit test reducer script (RUN THIS CELL AS IS)\n",
    "!echo -e \"foo\t1\\nfoo\t1\\nquux\t1\\nlabs\t1\\nfoo\t1\\nbar\t1\\nquux\t1\" | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\t2\n",
      "quux\t1\n",
      "labs\t1\n",
      "foo\t1\n",
      "bar\t1\n",
      "quux\t1\n"
     ]
    }
   ],
   "source": [
    "# part d - systems text mapper and reducer together (RUN THIS CELL AS IS)\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\t1\n",
      "foo\t3\n",
      "labs\t1\n",
      "quux\t2\n"
     ]
    }
   ],
   "source": [
    "# part d - systems text mapper and reducer together with sort (RUN THIS CELL AS IS)\n",
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py | sort -k1,1 | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.7.0.jar\n",
      "/user/root/HW2\n"
     ]
    }
   ],
   "source": [
    "!echo {JAR_FILE}\n",
    "!echo {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop streaming command. __`NOTE:`__ _don't forget to clear the output directory before re-running this cell (see part c above)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7887591689264254747.jar tmpDir=null\n",
      "18/01/20 14:52:40 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/20 14:52:41 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/01/20 14:52:41 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "18/01/20 14:52:41 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/01/20 14:52:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1516450633555_0002\n",
      "18/01/20 14:52:42 INFO impl.YarnClientImpl: Submitted application application_1516450633555_0002\n",
      "18/01/20 14:52:42 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1516450633555_0002/\n",
      "18/01/20 14:52:42 INFO mapreduce.Job: Running job: job_1516450633555_0002\n",
      "18/01/20 14:52:48 INFO mapreduce.Job: Job job_1516450633555_0002 running in uber mode : false\n",
      "18/01/20 14:52:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/01/20 14:52:55 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "18/01/20 14:52:56 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/01/20 14:53:01 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/01/20 14:53:01 INFO mapreduce.Job: Job job_1516450633555_0002 completed successfully\n",
      "18/01/20 14:53:01 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=275132\n",
      "\t\tFILE: Number of bytes written=901350\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=177905\n",
      "\t\tHDFS: Number of bytes written=28506\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10379\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3488\n",
      "\t\tTotal time spent by all map tasks (ms)=10379\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3488\n",
      "\t\tTotal vcore-seconds taken by all map tasks=10379\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3488\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10628096\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3571712\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3736\n",
      "\t\tMap output records=30423\n",
      "\t\tMap output bytes=214280\n",
      "\t\tMap output materialized bytes=275138\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3009\n",
      "\t\tReduce shuffle bytes=275138\n",
      "\t\tReduce input records=30423\n",
      "\t\tReduce output records=3009\n",
      "\t\tSpilled Records=60846\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=82\n",
      "\t\tCPU time spent (ms)=3010\n",
      "\t\tPhysical memory (bytes) snapshot=725569536\n",
      "\t\tVirtual memory (bytes) snapshot=4089303040\n",
      "\t\tTotal committed heap usage (bytes)=612892672\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177691\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=28506\n",
      "18/01/20 14:53:01 INFO streaming.StreamJob: Output directory: /user/root/HW2/wordcount-output\n"
     ]
    }
   ],
   "source": [
    "# part e - Hadoop streaming job (RUN THIS CELL AS IS FIRST, then make your modification)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files WordCount/reducer.py,WordCount/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/alice.txt \\\n",
    "  -output {HDFS_DIR}/wordcount-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part e - retrieve results from HDFS & copy them into a local file (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/wordcount-output/part-0000* > WordCount/results.txt\n",
    "# NOTE: we would never do this for a really large output file! \n",
    "# (but it's convenient for illustration in this assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t690\n",
      "abide\t2\n",
      "able\t1\n",
      "about\t102\n",
      "above\t3\n",
      "absence\t1\n",
      "absurd\t2\n",
      "accept\t1\n",
      "acceptance\t1\n",
      "accepted\t2\n"
     ]
    }
   ],
   "source": [
    "# part e - view results (RUN THIS CELL AS IS)\n",
    "!head WordCount/results.txt\n",
    "# NOTE: these words and counts should match your results in HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\n"
     ]
    }
   ],
   "source": [
    "# part e - check 'alice' count (RUN THIS CELL AS IS after running the job on the full file)\n",
    "!grep 'alice' WordCount/results.txt\n",
    "# EXPECTED OUTPUT: 403"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Uppercase and Lowercase Counts.\n",
    "\n",
    "What if we didn't care about individual word counts but rather wanted to know how many of the words in the _Alice_ file are upper and lower case? We could retrieve this information easily using a Hadoop streaming job with the same reducer script as in Question 2 (`WordCount/reducer.py`) but a slightly different mapper. In this question you'll design and write your own Hadoop streaming job to do just this.\n",
    "\n",
    "### Q3 Tasks:\n",
    "* __a) short response__ What should the keys be for this task? [`HINT:` _we'll need a key for each thing we want to count._]\n",
    "\n",
    "\n",
    "* __b) code:__ Complete the docstring and code in the __`UpperLower/mapper.py`__ to create a mapper that reads each input line, splits it into words and emits an appropriate key-value pair for each one. [`HINT:` _we're going to use this mapper in conjunction with the reducer from Question 2 so your key-value format should look very similar to the one in question 2's mapper._]\n",
    "\n",
    "\n",
    "* __c) Unit test:__ Run the provided cells to make your new mapper executable and test that it works as you expect.\n",
    "\n",
    "\n",
    "* __d) code:__ We've provided the start of a Hadoop streaming command. Fill in the missing parameters following the example provided in Question 2. Run your Hadoop job on the test file to confirm that it works. When you are happy with the results replace the test filepath with the real _Alice in Wonderland_ filepath and rerun the job so that we can grade your final counts.\n",
    "\n",
    "\n",
    "* __e) short response:__ Like our bash script from homework 1, Hadoop automatically splits up your records to be processed in parallel on separate mapper and reducer \"nodes\" (called \"tasks\" by Hadoop). Judging from the jobs you've run so far, what are the default number of 'map tasks' and 'reduce tasks' that Hadoop uses? Does this framework allow us to directly control the number of mappers and reducers? [`HINTS:` _to answer the first part of this question, look at the \"Job Counters\" section in the logging from your Hadoop job; for the second part of this question refer back to_ Lin & Dyer p24 _at the very bottom_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Student Answers:\n",
    "> __a)__ Type your answer here! \n",
    "\n",
    "> __b-d)__ _complete the coding portions of this question before answering 'e'_\n",
    "\n",
    "> __e)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part b - write your code in UpperLower/mapper.py (then RUN THIS CELL AS IS)\n",
    "!chmod a+x UpperLower/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part c - unit test your new mapper (RUN THIS CELL AS IS)\n",
    "!echo \"Foo foo Quux Labs foo bar quux\" | UpperLower/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part c - systems test your new mapper with the reducer from question 2 (RUN THIS CELL AS IS)\n",
    "!echo \"Foo foo Quux Labs foo bar quux\" | UpperLower/mapper.py | sort -k1,1 | WordCount/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part d - clear output directory before (re)running your Hadoop Job (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/upperlower-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part d - Hadoop streaming command (FILL IN MISSING ARGUMENTS)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files UpperLower/mapper.py,WordCount/reducer.py \\\n",
    "\n",
    "    \n",
    "    \n",
    "  -output {HDFS_DIR}/upperlower-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part d - results (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/upperlower-output/part-000* > UpperLower/results.txt\n",
    "!cat UpperLower/results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Number of Unique Words\n",
    "\n",
    "Another variation on the simple word counting job would be to count the number of unique words in the book (instead of counting the unique occurrences of each word). Of course in reality the easiest way to get this information would be to count the number of lines in our word count output file... but since our goal here is to practice designing and writing Hadoop jobs, let's pretend for a moment that we don't have access to that file and instead think about how we'd do this from scratch. In this question we'll also introduce an important flag you can add to your Hadoop streaming jobs to control the degree of parallelization. \n",
    "\n",
    "### Q4 Tasks:\n",
    "* __a) short response:__ What should our keys be for this task? Does it make most sense to check for 'uniqueness' inside the mapper or inside the reducer? why? [`HINT:` _for the second part of this question, think about our discussion of memory constraints in HW1 and about the synchronization that Hadoop performs for us automatically between the map and reduce phases._]\n",
    "\n",
    "\n",
    "* __b) code + unit test:__ Since the mapper we wrote for `WordCount` already emits the right keys, let's simply reuse that mapper. Fill in the docstring and code in __`VocabSize/reducer.py`__ so that this new reducer processes the records emitted by `WordCount/mapper.py` and outputs the count of the number of unique words that appear in the input file. Run the provided unit test to confirm that your reducer works as you want it to.\n",
    "\n",
    "\n",
    "* __c) code + short reponse:__ Write and run a Hadoop streaming job to calculate the number of unique words in _Alice and Wonderland_ and report it in the space provided (`NOTE:` _for 'd' you'll modify this job and overwrite the original result which is why we'll ask you to record it in markdown._)\n",
    "\n",
    "\n",
    "* __d) code + short response:__ Add the flag `-numReduceTasks 3` to the very end of the Hadoop streaming command you wrote for `part d`. This flag tells Hadoop to use 3 separate reduce tasks, in other words, we'll make 3 'partitions' from the records emitted by your map phase and perform the reducing on each part. Rerun the job with this added flag and observe the result. What do you notice about the contents of the HDFS output directory and the final output itself? How would we have to post process our results to get the answer we're looking for?\n",
    "\n",
    "\n",
    "* __ e) Hadoop UI screenshot:__ In addition to the logging that Hadoop prints to your notebook you can also access a UI with more detailed information about your hadoop streaming jobs. The link to this UI can be found near the top of the logging from your job. Look for a line that reads something like:\n",
    ">`The url to track the job: http://quickstart.cloudera:8088/proxy/application_########_#####/`\n",
    "\n",
    "* For `part e` Navigate to this UI and confirm that your job used 2 map tasks and 3 reduce tasks. Take a screenshot of the UI for this job and embed it in this notebook using the Jupyter image code provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Student Answers:\n",
    "> __a)__ Type your answer here!\n",
    "\n",
    "> __c)__ Type your answer here!\n",
    "\n",
    "> __d)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part b - do your work in VocabSize/reducer.py first, then RUN THIS CELL AS IS\n",
    "!chmod a+x VocabSize/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part b - unit test your new reducer (RUN THIS CELL AS IS)\n",
    "!echo -e \"foo\t1\\nfoo\t1\\nquux\t1\\nlabs\t1\\nfoo\t1\\nbar\t1\\nquux\t1\" | sort -k1,1 | VocabSize/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part c - clear output directory before (re)running your Hadoop Job (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/vocabsize-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __`TIPS:`__ _When writing your job below make sure that you have the correct paths to your input file, output directory and mapper/reducer script. Don't forget the `-files` option, and_ DO NOT _put spaces between the paths that you pass to this option_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parts c/d - write/modify your Hadoop streaming command here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parts c/d - take a look at the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -ls {HDFS_DIR}/vocabsize-output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parts c/d - view results (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/vocabsize-output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - replace the image path in the code below to embed your UI screenshot\n",
    "from IPython.display import Image\n",
    "Image(filename=\"/media/notebooks/example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Secondary Sort\n",
    "In question 2 we talked a little bit about the default sorting that Hadoop observes. However often we'll want to sort not just by the key but also by value. For example, we might want to sort the words by their count to find the most frequent words but then break ties by the word in alphabetical order. This is called a 'secondary sort'. In this question we'll learn about specifying parameters for sorting in Hadoop jobs. In particular you'll add three new parameters to your Hadoop Streaming command:\n",
    "> __`-D stream.num.map.output.key.fields=2`__ : tells Hadoop to treat both the first and the second (tab separated) fields as a composite key.  \n",
    "\n",
    "> __`-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator`__ : tells Hadoop that we want to make comparisons (for sorting) based on the fields in this composite key\n",
    "\n",
    "> __`-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\"`__: Tells Hadoop to perform a reverse numerical sort on the second field in the composite key and then break ties by sorting (alphabetically) on the first field in the composite key.\n",
    "\n",
    "To find the top words in the _Alice in Wonderland_ text we'll use the output of our word counting job as the input for this new sorting task. Recall that this output is a file in alphabetical order whose lines are of the format `word \\t count`. Also recall that this file is already available in HDFS at the path `{HDFS_DIR}/wordcount-output`. You can simply pass this directory path in to the Hadoop streaming input parameter and it will understand that it should read in the directory's contents. __`IMPORTANT`:__ _please use a single reduce task for parts a-c._\n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) short response:__ Before we get to the full secondary sort it's worth noting that there is a really easy way to get Hadoop to sort our file by count: we could just switch the order of the count and the word when we print to standard output in our mapper. Briefly explain why this would work.\n",
    "\n",
    "\n",
    "* __b) code + short response:__ Complete the code in __`TopWords/mapper.py`__ so that it performs the switch described in 'a'. For debugging purposes we'll first run this job using a test file of word counts instead of the full _Alice_ file. Run the provided Hadoop streaming command to confirm that our sneaky solution works. Do you see any problems with the result?\n",
    "\n",
    "\n",
    "* __c) code:__ Ok, for obvious reasons our 'sneaky' solution didn't quite give us the output we wanted. So let's do a secondary sort properly this time. Then add the three new Hadoop options described in the intro to this question. Run your job with these new specifications on the dummy count file. When you are satisfied that your job works, change the input path to specify the alice count output that is already in HDFS. Your list of top words should match the result you got in HW1. __`Two important warnings:`__  1) Parameters starting with the `-D` flag must come immediately after the line where you specify the jar file and before the parameters `-files`, `-mapper`, etc; 2) The options we provided you above specify a reverse numeric sort on the second field and tie breaking using the first field... but the mapper you wrote in part a switched the order of the words and the counts. You will need to make a small adjustment to the options we provided so that it instead reverse numerically sorts by the first field and breaks ties on the second. \n",
    "\n",
    "\n",
    "* __d) code + short response:__ Run your Hadoop job one more time but this time add the parameter to specify that the job should use 2 reduce tasks instead of 1. For convenience of illustration, you should do this using the sample counts file instead of the full _Alice_ text. What is wrong with the results? Use the provided code to look at the output of each partition independently. Can you explain why our results are off? Imagine we had a really large file and performed a sort using 100 partitions, what post processing would we have to do to get a fully ordered list (Total Order Sort)? Compare the computational cost of this postprocessing to the postprocessing we had to do in the VocabSize job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Student Answers:\n",
    "> __a)__ Type your answer here!\n",
    "\n",
    "> __b)__ Type your answer here!\n",
    " \n",
    "> __d)__ Type your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ The cell below will create a short file of word counts that we can load into HDFS and use to test our Hadoop MapReduce job. Take a moment to read this sample file and figure out what a reverse numerical sort (with alphabetical tie breaking) should yield. Then go on to complete your tasks as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile TopWords/sample.txt\n",
    "foo\t5\n",
    "quux\t9\n",
    "labs\t100\n",
    "bar\t5\n",
    "qi\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load sample file into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal TopWords/sample.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your work for b-d starts here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part b - complete your work in TopWords/mapper.py then RUN THIS CELL AS IS\n",
    "!chmod a+x TopWords/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parts b/c/d - clear output directory (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/topwords-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b/c/d - Hadoop streaming command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files TopWords/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/sample.txt \\\n",
    "  -output {HDFS_DIR}/topwords-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part b/c/d - Save results locally (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/topwords-output/part-0000* > TopWords/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part b/c/d - view results (RUN THIS CELL AS IS)\n",
    "!head TopWords/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part d - look at first partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/topwords-output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# part d - look at second partition (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/topwords-output/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One last part 1 task...\n",
    "You've now seen most of the basic functionality of writing and running Hadoop streaming jobs. In Part 2 and over the course of the next few weeks we'll explore additional options and tricks to add to our jobs. As we do this you will want to be able to quickly distinguish between errors that occur due to a mistake in your algorithm design or Hadoop streaming command and errors that are rooted in your Python code. Unfortunaly the error logs printed to console do not always make this distinction obvious. Luckily, the Hadoop UI logs do make it very easy to identify Python coding errors. Before you move on to part 2 we'd like to make sure you know where to find these logs and how to fix two common mistakes. __Below, we provided code that contains two common errors for you to debug. Your job is to:__\n",
    "1. __Run the provided code as is, it will throw an error.__\n",
    "\n",
    "2. __Navigate to the Hadoop UI and find the relevant logs explaining your error.__ \n",
    " * Under `Task Type`, click `Map` > `task_XXXXXX` >`logs`\n",
    "\n",
    "3. __Take a screenshot of the error description and embed it below.__\n",
    "\n",
    "4. __Fix the error(s) and re-run the job__. \n",
    "\n",
    "__`NOTE:`__ There are two differet kinds of errors in the mapper code. See the inline comments for specific fixes: one involves adding a parameter to your Hadoop job the other two you must fix in the mapper code (re-run that cell to overwrite the old mapper). I'd recommend fixing them one at a time so that you can see how the logs and error messages change depending on the type of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the following cells to create the demo mapper\n",
    "!mkdir demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demo/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "This is a silly mapper to demonstrate some errors.\n",
    "\"\"\"\n",
    "import sys\n",
    "import numpy as np  # To use numpy add -cmdenv PATH={PATH} to your Hadoop Job\n",
    "\n",
    "for line in sys.stdin:\n",
    "    msg = (\"a message\"   # missing a parenthesis here\n",
    "    print 1/0            # dividing by zero is a no-go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear HDFS output directory when you re-run the job\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/demo-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hadoop streaming command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files demo/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/sample.txt \\\n",
    "  -output {HDFS_DIR}/demo-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed your Hadoop UI error logs screenshot here\n",
    "from IPython.display import Image\n",
    "Image(filename=\"/media/notebooks/screenshot1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed your Hadoop UI error logs screenshot here\n",
    "from IPython.display import Image\n",
    "Image(filename=\"/media/notebooks/screenshot2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 ends here, continue on to part 2 in the notebook provided."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
