{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MIDS---w261-Machine-Learning-At-Scale\" data-toc-modified-id=\"MIDS---w261-Machine-Learning-At-Scale-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MIDS - w261 Machine Learning At Scale</a></span><ul class=\"toc-item\"><li><span><a href=\"#Assignment---HW5\" data-toc-modified-id=\"Assignment---HW5-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Assignment - HW5</a></span><ul class=\"toc-item\"><li><span><a href=\"#INSTRUCTIONS-for-SUBMISSIONS\" data-toc-modified-id=\"INSTRUCTIONS-for-SUBMISSIONS-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>INSTRUCTIONS for SUBMISSIONS</a></span></li></ul></li></ul></li><li><span><a href=\"#Optional-reading\" data-toc-modified-id=\"Optional-reading-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Optional reading</a></span></li><li><span><a href=\"#HW-Problems\" data-toc-modified-id=\"HW-Problems-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>HW Problems</a></span></li><li><span><a href=\"#HW5.0--data-warehouse;-star-schema\" data-toc-modified-id=\"HW5.0--data-warehouse;-star-schema-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>HW5.0  data warehouse; star schema</a></span></li><li><span><a href=\"#HW5.1-Databases:-3NF;-denormalized\" data-toc-modified-id=\"HW5.1-Databases:-3NF;-denormalized-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>HW5.1 Databases: 3NF; denormalized</a></span></li><li><span><a href=\"#HW5.2--Memory-backed-map-side\" data-toc-modified-id=\"HW5.2--Memory-backed-map-side-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>HW5.2  Memory-backed map-side</a></span></li><li><span><a href=\"#HW5.2.1-(OPTIONAL)-Almost-stateless-reducer-side-join\" data-toc-modified-id=\"HW5.2.1-(OPTIONAL)-Almost-stateless-reducer-side-join-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>HW5.2.1 (OPTIONAL) Almost stateless reducer-side join</a></span></li><li><span><a href=\"#Pairwise-similarity----PHASE-1\" data-toc-modified-id=\"Pairwise-similarity----PHASE-1-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Pairwise similarity  - PHASE 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Run-Systems-tests-locally-on-small-datasets-(PHASE1)\" data-toc-modified-id=\"Run-Systems-tests-locally-on-small-datasets-(PHASE1)-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Run Systems tests locally on small datasets (PHASE1)</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#1:-unit/systems-first-10-lines\" data-toc-modified-id=\"1:-unit/systems-first-10-lines-8.1.0.1\"><span class=\"toc-item-num\">8.1.0.1&nbsp;&nbsp;</span>1: unit/systems first-10-lines</a></span></li><li><span><a href=\"#2:-unit/systems-atlas-boon\" data-toc-modified-id=\"2:-unit/systems-atlas-boon-8.1.0.2\"><span class=\"toc-item-num\">8.1.0.2&nbsp;&nbsp;</span>2: unit/systems atlas-boon</a></span></li><li><span><a href=\"#3:-unit/systems-stripe-docs-test\" data-toc-modified-id=\"3:-unit/systems-stripe-docs-test-8.1.0.3\"><span class=\"toc-item-num\">8.1.0.3&nbsp;&nbsp;</span>3: unit/systems stripe-docs-test</a></span></li></ul></li></ul></li><li><span><a href=\"#5.3-build-stripes-for-all-the-test-data-sets---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"5.3-build-stripes-for-all-the-test-data-sets---run-the-commands-and-insure-that-your-output-matches-the-output-below-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>5.3 build stripes for all the test data sets - run the commands and insure that your output matches the output below</a></span></li><li><span><a href=\"#5.4-Build-Inverted-Index---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"5.4-Build-Inverted-Index---run-the-commands-and-insure-that-your-output-matches-the-output-below-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>5.4 Build Inverted Index - run the commands and insure that your output matches the output below</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inverted-Index\" data-toc-modified-id=\"Inverted-Index-8.3.1\"><span class=\"toc-item-num\">8.3.1&nbsp;&nbsp;</span>Inverted Index</a></span></li></ul></li><li><span><a href=\"#5.5-Calculate-similarities---run-the-commands-and-insure-that-your-output-matches-the-output-below\" data-toc-modified-id=\"5.5-Calculate-similarities---run-the-commands-and-insure-that-your-output-matches-the-output-below-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>5.5 Calculate similarities - run the commands and insure that your output matches the output below</a></span><ul class=\"toc-item\"><li><span><a href=\"#NOTE:-I-will-be-calculating-cosine-similarity,-which-means-I-need-to-re-write-my-inverted-index-MRJob\" data-toc-modified-id=\"NOTE:-I-will-be-calculating-cosine-similarity,-which-means-I-need-to-re-write-my-inverted-index-MRJob-8.4.1\"><span class=\"toc-item-num\">8.4.1&nbsp;&nbsp;</span>NOTE: I will be calculating cosine similarity, which means I need to re-write my inverted index MRJob</a></span><ul class=\"toc-item\"><li><span><a href=\"#NOTE:-you-must-run-in-hadoop-mode-to-generate-sorted-similarities\" data-toc-modified-id=\"NOTE:-you-must-run-in-hadoop-mode-to-generate-sorted-similarities-8.4.1.1\"><span class=\"toc-item-num\">8.4.1.1&nbsp;&nbsp;</span>NOTE: you must run in hadoop mode to generate sorted similarities</a></span></li></ul></li><li><span><a href=\"#Pairwise-Similairity\" data-toc-modified-id=\"Pairwise-Similairity-8.4.2\"><span class=\"toc-item-num\">8.4.2&nbsp;&nbsp;</span>Pairwise Similairity</a></span></li></ul></li></ul></li><li><span><a href=\"#===-END-OF-PHASE-1-===\" data-toc-modified-id=\"===-END-OF-PHASE-1-===-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>=== END OF PHASE 1 ===</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5\n",
    "\n",
    "\n",
    "---\n",
    "__Name:__  Stanimir Vichev   \n",
    "__Class:__ MIDS w261 (Section 1, e.g., Group 1)     \n",
    "__Email:__  stassyvichev@iSchool.Berkeley.edu \n",
    "__Week:__   5\n",
    "    \n",
    "__Due Time:__ HW is due the Thursday of the following week by 8AM (West coast time). I.e., Thursday, Feb 14, 2017 in the case of this homework. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as Altiscale's PaaS or on AWS) and is due Thursday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the Altiscale cluster and will be due Thursday, Feb 22 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. \n",
    "\n",
    "\n",
    "\n",
    "### INSTRUCTIONS for SUBMISSIONS \n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "Each student has a `HW-<user>` repository for all assignments.   \n",
    "\n",
    "Push the following to your HW github repo into the master branch:\n",
    "* Your local HW5 directory. Your repo file structure should look like this:\n",
    "\n",
    "```\n",
    "HW-<user>\n",
    "    --HW3\n",
    "       |__MIDS-W261-HW-03-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-03-<Student_id>.pdf\n",
    "       |__some other hw3 file\n",
    "    --HW4\n",
    "       |__MIDS-W261-HW-04-<Student_id>.ipynb\n",
    "       |__MIDS-W261-HW-04-<Student_id>.pdf\n",
    "       |__some other hw4 file\n",
    "    etc..\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional reading\n",
    "http://stanford.edu/~rezab/papers/disco.pdf   \n",
    "https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.0  data warehouse; star schema\n",
    "\n",
    "- What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A data warehouse is a set of tools and processes responsible for collecting, managing and serving the data that is needed for analysis by a business. It is usually a department within the company responsible for this, with a focus on data reliability, dependability and security, as well as analysis. \n",
    "\n",
    "> A star schema is a framework for a database architecture where different tables link to each other via primary and foreign keys, allowing information for a record to be stored separately in different tables in a normalized way. They are useful for records with a lot of attributes that can logically be split into attribute-specific tables for ease of storing, processing and logical analysis. Such tables heavily utilize joins to produce the complete record from the attributes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.1 Databases: 3NF; denormalized \n",
    "\n",
    "- In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "- In what form does ML consume data?\n",
    "- Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A 3NF table in a database has several characteristics:\n",
    "\n",
    "> 1. It has no cells with multiple values; Each row has a primary key (from 1NF)\n",
    "\n",
    "> 2. The primary key contains multiple columns and non-key columns depend on the whole key ( from 2NF)\n",
    "\n",
    "> 3. Each non-key attribute in a row does not depend on the entry in another non-key column.\n",
    "\n",
    "> ML might have to work with 3NF data in situations where it uses information from a relational database, and RDBs are used extensively in business. As such, ML might have to pre-process the data in order to get all data for a record in one place. This is needed because ML might need all parameters associated with a record in order to do training on inference, since with ML we almost always want more data if we can have it rather than less. Log files are useful because they have all the data associated with a record in one row, which is quite convenient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "PATH  = environ['PATH']\n",
    "# tell matplotlib not to open a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 2.7.14 \n",
      "HDFS filesystem running at: \n",
      "\t hdfs://quickstart.cloudera:8020\n"
     ]
    }
   ],
   "source": [
    "# print some configuration details for future replicability.\n",
    "print 'Python Version: %s' % (sys.version.split('|')[0])\n",
    "hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER\n",
    "#hdfs_conf = !hdfs getconf -confKey fs.default.name ### UNCOMMENT ON ALTISCALE\n",
    "print 'HDFS filesystem running at: \\n\\t %s' % (hdfs_conf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an HDFS directory for this assignment\n",
    "!hdfs dfs -mkdir hw51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_DIR = \"/user/root/hw51\" # eg. /user/root/hw3 \n",
    "HOME_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.2  Memory-backed map-side\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Use the following tables for this HW and join based on the country code (third column of the transactions table and the second column of the Countries table:\n",
    "\n",
    "<pre>\n",
    "transactions.dat\n",
    "Alice Bob|$10|US\n",
    "Sam Sneed|$1|CA\n",
    "Jon Sneed|$20|CA\n",
    "Arnold Wesise|$400|UK\n",
    "Henry Bob|$2|US\n",
    "Yo Yo Ma|$2|CA\n",
    "Jon York|$44|CA\n",
    "Alex Ball|$5|UK\n",
    "Jim Davis|$66|JA\n",
    "\n",
    "Countries.dat\n",
    "United States|US\n",
    "Canada|CA\n",
    "United Kingdom|UK\n",
    "Italy|IT\n",
    "\n",
    "</pre>\n",
    "\n",
    "**student task**\n",
    "\n",
    "\n",
    "Please report the number of rows resulting from: (left = transactions, right = countries)\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n",
    "\n",
    "__NOTE:__ For the Right and Inner joins, no reducer should be used. For the left outer join you can use a reducer.\n",
    "\n",
    "__NOTE:__ If you need a refresher for MRJobs, please take a look at HW4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> My prediction before I do this, just from looking at file:\n",
    "\n",
    "> Left outer join: 9\n",
    "\n",
    "> Right outer join: 9\n",
    "\n",
    "> Inner join: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Countries.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile Countries.dat\n",
    "United States|US\n",
    "Canada|CA\n",
    "United Kingdom|UK\n",
    "Italy|IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transactions.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile transactions.dat\n",
    "Alice Bob|$10|US\n",
    "Sam Sneed|$1|CA\n",
    "Jon Sneed|$20|CA\n",
    "Arnold Wesise|$400|UK\n",
    "Henry Bob|$2|US\n",
    "Yo Yo Ma|$2|CA\n",
    "Jon York|$44|CA\n",
    "Alex Ball|$5|UK\n",
    "Jim Davis|$66|JA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting leftOuterJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile leftOuterJoin.py\n",
    "#!/usr/bin/env python\n",
    "#START STUDENT CODE44\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import logging\n",
    "class MRLeftOuterJoin(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    JOBCONF = {\"mapreduce.job.reduces\": \"1\"}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRLeftOuterJoin, self).__init__(*args,**kwargs)\n",
    "        self.countries = {}\n",
    "        with open(\"Countries.dat\") as file:\n",
    "            for line in file.readlines():\n",
    "                fields = line.split(\"|\")\n",
    "                self.countries[fields[1].replace(\"\\n\",\"\")]=fields[0]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.split(\"|\")\n",
    "        yield fields[0], (fields[1], self.countries.get(fields[2]))\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper = self.mapper)\n",
    "        ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRLeftOuterJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Alice Bob', [u'$10', u'United States'])\n",
      "(u'Sam Sneed', [u'$1', u'Canada'])\n",
      "(u'Jon Sneed', [u'$20', u'Canada'])\n",
      "(u'Arnold Wesise', [u'$400', u'United Kingdom'])\n",
      "(u'Henry Bob', [u'$2', u'United States'])\n",
      "(u'Yo Yo Ma', [u'$2', u'Canada'])\n",
      "(u'Jon York', [u'$44', u'Canada'])\n",
      "(u'Alex Ball', [u'$5', u'United Kingdom'])\n",
      "(u'Jim Davis', [u'$66', None])\n",
      "Left outer line count: 9\n"
     ]
    }
   ],
   "source": [
    "from leftOuterJoin import MRLeftOuterJoin\n",
    "# ,\"-r\", \"hadoop\"\n",
    "mr_job = MRLeftOuterJoin(args=[\"transactions.dat\",\"--file\", \"Countries.dat\"])\n",
    "count = 0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        count +=1\n",
    "        print mr_job.parse_output_line(line)\n",
    "print \"Left outer line count: \"+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rightOuterJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rightOuterJoin.py\n",
    "#!/usr/bin/env python\n",
    "#START STUDENT CODE44\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import logging\n",
    "class MRRightOuterJoin(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    JOBCONF = {\"mapreduce.job.reduces\": \"1\"}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRRightOuterJoin, self).__init__(*args,**kwargs)\n",
    "        self.countries = {}\n",
    "        with open(\"Countries.dat\") as file:\n",
    "            for line in file.readlines():\n",
    "                fields = line.split(\"|\")\n",
    "                self.countries[fields[1].replace(\"\\n\",\"\")]=fields[0]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.split(\"|\")\n",
    "        if self.countries.get(fields[2]):\n",
    "            yield self.countries.get(fields[2]), (fields[1], fields[0])\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        self.countries_done = []   \n",
    "    \n",
    "    def reducer_final(self):\n",
    "        for key, value in self.countries.items():\n",
    "            if value not in self.countries_done:\n",
    "                yield value, None\n",
    "    \n",
    "    def reducer(self, key, value):\n",
    "        if key in self.countries.values():\n",
    "            self.countries_done.append(key)\n",
    "        for v in value:\n",
    "            yield key, v\n",
    "            \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper = self.mapper,\n",
    "                   reducer = self.reducer,\n",
    "                   reducer_init = self.reducer_init,\n",
    "                   reducer_final = self.reducer_final)\n",
    "        ]\n",
    "if __name__ == \"__main__\":\n",
    "    MRRightOuterJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Canada', [u'$1', u'Sam Sneed'])\n",
      "(u'Canada', [u'$2', u'Yo Yo Ma'])\n",
      "(u'Canada', [u'$20', u'Jon Sneed'])\n",
      "(u'Canada', [u'$44', u'Jon York'])\n",
      "(u'United Kingdom', [u'$400', u'Arnold Wesise'])\n",
      "(u'United Kingdom', [u'$5', u'Alex Ball'])\n",
      "(u'United States', [u'$10', u'Alice Bob'])\n",
      "(u'United States', [u'$2', u'Henry Bob'])\n",
      "(u'Italy', None)\n",
      "Right outer line count: 9\n"
     ]
    }
   ],
   "source": [
    "from rightOuterJoin import MRRightOuterJoin\n",
    "# ,\"-r\", \"hadoop\"\n",
    "mr_job = MRRightOuterJoin(args=[\"transactions.dat\",\"--file\", \"Countries.dat\"])\n",
    "count = 0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        count +=1\n",
    "        print mr_job.parse_output_line(line)\n",
    "print \"Right outer line count: \"+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing innerJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile innerJoin.py\n",
    "#!/usr/bin/env python\n",
    "#START STUDENT CODE44\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import logging\n",
    "class MRInnerJoin(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "    JOBCONF = {\"mapreduce.job.reduces\": \"1\"}\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MRInnerJoin, self).__init__(*args,**kwargs)\n",
    "        self.countries = {}\n",
    "        with open(\"Countries.dat\") as file:\n",
    "            for line in file.readlines():\n",
    "                fields = line.split(\"|\")\n",
    "                self.countries[fields[1].replace(\"\\n\",\"\")]=fields[0]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.split(\"|\")\n",
    "        if self.countries.get(fields[2]):\n",
    "            yield fields[0], (fields[1], self.countries.get(fields[2]))\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper = self.mapper)\n",
    "        ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRInnerJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Alice Bob', [u'$10', u'United States'])\n",
      "(u'Sam Sneed', [u'$1', u'Canada'])\n",
      "(u'Jon Sneed', [u'$20', u'Canada'])\n",
      "(u'Arnold Wesise', [u'$400', u'United Kingdom'])\n",
      "(u'Henry Bob', [u'$2', u'United States'])\n",
      "(u'Yo Yo Ma', [u'$2', u'Canada'])\n",
      "(u'Jon York', [u'$44', u'Canada'])\n",
      "(u'Alex Ball', [u'$5', u'United Kingdom'])\n",
      "Inner line count: 8\n"
     ]
    }
   ],
   "source": [
    "from innerJoin import MRInnerJoin\n",
    "# ,\"-r\", \"hadoop\"\n",
    "mr_job = MRInnerJoin(args=[\"transactions.dat\",\"--file\", \"Countries.dat\"])\n",
    "count = 0\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        count +=1\n",
    "        print mr_job.parse_output_line(line)\n",
    "print \"Inner line count: \"+str(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.2.1 (OPTIONAL) Almost stateless reducer-side join\n",
    "\n",
    "The following MRJob code, implements a reduce-side join for an inner join. The reducer is almost stateless, i.e., uses as little memory as possible. Use the tables from HW5.2 for this HW and join based on the country code (third column of the transactions table and the second column of the Countries table perform. Perform  an left, right, inner joins using the code provided below and report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n",
    "\n",
    "Again make smart decisions about which table should be the left table (i.e., crosscheck the code). \n",
    "\n",
    "__Some notes on the code__ \n",
    "Here, the mapper receives its set of input splits either from the transaction table or from the countries table and makes the appropriate transformations: splitting the line into fields, and emitting a key/value. The key is the join key - in this case, the country code field of both sets of records. The mapper knows which file and type of record it is receiving based on the length of the fields. The records it emits contain the join field as the key, which acts as the partitioning key; We use the SORT_VALUES option, which ensures the values are sorted as well. Then, we employ a trick to ensure that for each join key, country records are seen always before transaction records. We achieve this by adding an arbitrary key to the front of the value: 'A' for countries, 'B' for customers. This makes countries sort before customers for each and every join/partition key. After that trick, the join is simply a matter of storing countries ('A' records) and crossing this array with each customer record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, re\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Performs secondary sort\n",
    "  SORT_VALUES = True\n",
    "\n",
    "  def mapper(self, _, line):\n",
    "    splits = line.rstrip(\"\\n\").split(\"|\")\n",
    "\n",
    "    if len(splits) == 2: # country data\n",
    "      symbol = 'A' # make country sort before transaction data\n",
    "      country2digit = splits[1]\n",
    "      yield country2digit, [symbol, splits]\n",
    "    else: # person data\n",
    "      symbol = 'B'\n",
    "      country2digit = splits[2]\n",
    "      yield country2digit, [symbol, splits]\n",
    "\n",
    "  def reducer(self, key, values):\n",
    "    countries = [] # should come first, as they are sorted on artificia key 'A'\n",
    "    for value in values:\n",
    "      if value[0] == 'A':\n",
    "        countries.append(value)\n",
    "      if value[0] == 'B':\n",
    "        for country in countries:\n",
    "          yield key, country[1:] + value[1:]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  MRJoin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise similarity  - PHASE 1\n",
    "\n",
    "In this part of the assignment we will focus on developing methods for detecting synonyms, using the Google 5-grams dataset. To accomplish this you must script three main tasks using MRJob:\n",
    "\n",
    "\n",
    "- (1) Using the systems tests data sets, write mrjob code to build the stripes\n",
    "- (2) Write mrjob code to build an inverted index from the stripes\n",
    "- (3) Using two (symmetric) comparison methods of your choice (e.g., correlations, distances, similarities), pairwise compare all stripes (vectors), and output to a file.   \n",
    "\n",
    "\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    " - Jaccard\n",
    " - Cosine similarity\n",
    " - Spearman correlation\n",
    " - Euclidean distance\n",
    " - Taxicab (Manhattan) distance\n",
    " - Shortest path graph distance (a graph, because our data is symmetric!)\n",
    " - Pearson correlation\n",
    " - Kendall correlation\n",
    " \n",
    " ...\n",
    " \n",
    " (for more details: http://stanford.edu/~rezab/papers/disco.pdf pg5, table 1)\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to parallelize than others, and do not perform more associations than is necessary, since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Systems tests locally on small datasets (PHASE1)\n",
    "\n",
    "Complete 5.3-5.5 and systems test using the below test datasets. Phase 2 will focus on the entire Ngram dataset.\n",
    "\n",
    "To help you through these tasks please verify that your code gives the results below (for stripes, inverted index, and pairwise similarities).\n",
    "\n",
    "Test datasets:\n",
    "\n",
    "* googlebooks-eng-all-5gram-20090715-0-filtered.txt [see below]\n",
    "* atlas-boon-test [see below]\n",
    "* stripe-docs-test [see below]\n",
    "\n",
    "\n",
    "A large subset of the Google n-grams dataset   \n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox:   \n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "You can download from Dropbox using wget:   \n",
    "`wget -O googlebooks-eng-all-5gram-20090715-0-filtered.txt https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACr50woxiBWoaiiLmnwduX8a/googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0`\n",
    "\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "The next cell shows the first 10 lines of the googlebooks-eng-all-5gram-20090715-0-filtered.txt file.\n",
    "\n",
    "\n",
    "__DISCLAIMER__: Each record is already a 5-gram. In real life, we would calculate the stripes cooccurrence data from the raw text by windowing over the raw text and not from the 5-gram preprocessed data (as we are doing here).  Calculatating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some simialr terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: unit/systems first-10-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: unit/systems atlas-boon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing atlas-boon-systems-test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas-boon-systems-test.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: unit/systems stripe-docs-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three terms, A,B,C and their corresponding stripe-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 build stripes for all the test data sets - run the commands and insure that your output matches the output below\n",
    "\n",
    "* __Note__: Your results may not match 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing buildStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile buildStripes.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import re\n",
    "import mrjob\n",
    "import json\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRbuildStripes(MRJob):\n",
    "    SORT_VALUES = True\n",
    "    JOBCONF = {\"mapreduce.job.reduces\": \"1\"}\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        fields = line.lower().strip(\"\\n\").split(\"\\t\")\n",
    "        words = fields[0].split(\" \")\n",
    "        occurrence_count = int(fields[1])\n",
    "        count = 1\n",
    "        for word in words:\n",
    "            for i in xrange(count, len(words)):\n",
    "                yield word, (words[i], occurrence_count)\n",
    "                yield words[i], (word, occurrence_count)\n",
    "            count += 1\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        print\n",
    "        \n",
    "    def reducer(self, word, occurrence_counts):\n",
    "        stripe = {}\n",
    "        for other_word, occurrence_count in occurrence_counts:\n",
    "            stripe[other_word] = stripe.get(other_word,0)+occurrence_count\n",
    "        yield word, stripe\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRbuildStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rm: Illegal option --recursive\n",
      "Usage: hadoop fs [generic options] -rm [-f] [-r|-R] [-skipTrash] <src> ...\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.root.20180211.101746.383545\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/buildStripes.root.20180211.101746.383545/output...\n",
      "Removing temp directory /tmp/buildStripes.root.20180211.101746.383545...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 1\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm --recursive systems_test_stripes_1\n",
    "!python buildStripes.py -r local googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt > systems_test_stripes_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"a\"\t{\"limited\":55,\"sea\":62,\"general\":92,\"female\":447,\"in\":1201,\"religious\":59,\"george\":92,\"biography\":92,\"city\":62,\"for\":59,\"tales\":123,\"government\":102,\"the\":124,\"forms\":116,\"wales\":1099,\"christmas\":1099,\"child's\":1099,\"collection\":239,\"by\":62,\"case\":604,\"circumstantial\":62,\"of\":1011,\"study\":604,\"bill\":59,\"establishing\":59,\"narrative\":62,\"fairy\":123}\n",
      "\"bill\"\t{\"a\":59,\"religious\":59,\"for\":59,\"establishing\":59}\n",
      "\"biography\"\t{\"a\":92,\"of\":92,\"george\":92,\"general\":92}\n",
      "\"by\"\t{\"a\":62,\"city\":62,\"the\":62,\"sea\":62}\n",
      "\"case\"\t{\"a\":604,\"limited\":55,\"government\":102,\"of\":502,\"study\":604,\"female\":447,\"in\":102}\n",
      "\"child's\"\t{\"a\":1099,\"wales\":1099,\"christmas\":1099,\"in\":1099}\n",
      "\"christmas\"\t{\"a\":1099,\"wales\":1099,\"in\":1099,\"child's\":1099}\n",
      "\"circumstantial\"\t{\"a\":62,\"of\":62,\"the\":62,\"narrative\":62}\n",
      "\"city\"\t{\"a\":62,\"the\":62,\"by\":62,\"sea\":62}\n",
      "\"collection\"\t{\"a\":239,\"forms\":116,\"fairy\":123,\"tales\":123,\"of\":355}\n",
      "\"establishing\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"for\":59}\n",
      "\"fairy\"\t{\"a\":123,\"of\":123,\"tales\":123,\"collection\":123}\n",
      "\"female\"\t{\"a\":447,\"case\":447,\"study\":447,\"of\":447}\n",
      "\"for\"\t{\"a\":59,\"bill\":59,\"religious\":59,\"establishing\":59}\n",
      "\"forms\"\t{\"a\":116,\"of\":232,\"collection\":116}\n",
      "\"general\"\t{\"a\":92,\"of\":92,\"george\":92,\"biography\":92}\n",
      "\"george\"\t{\"a\":92,\"of\":92,\"biography\":92,\"general\":92}\n",
      "\"government\"\t{\"a\":102,\"case\":102,\"study\":102,\"in\":102}\n",
      "\"in\"\t{\"a\":1201,\"case\":102,\"government\":102,\"study\":102,\"child's\":1099,\"wales\":1099,\"christmas\":1099}\n",
      "\"limited\"\t{\"a\":55,\"case\":55,\"study\":55,\"of\":55}\n",
      "\"narrative\"\t{\"a\":62,\"of\":62,\"the\":62,\"circumstantial\":62}\n",
      "\"of\"\t{\"a\":1011,\"case\":502,\"circumstantial\":62,\"limited\":55,\"of\":232,\"study\":502,\"collection\":355,\"general\":92,\"forms\":232,\"tales\":123,\"female\":447,\"narrative\":62,\"fairy\":123,\"the\":62,\"george\":92,\"biography\":92}\n",
      "\"religious\"\t{\"a\":59,\"bill\":59,\"for\":59,\"establishing\":59}\n",
      "\"sea\"\t{\"a\":62,\"city\":62,\"the\":62,\"by\":62}\n",
      "\"study\"\t{\"a\":604,\"case\":604,\"limited\":55,\"of\":502,\"government\":102,\"female\":447,\"in\":102}\n",
      "\"tales\"\t{\"a\":123,\"of\":123,\"fairy\":123,\"collection\":123}\n",
      "\"the\"\t{\"a\":124,\"city\":62,\"circumstantial\":62,\"of\":62,\"sea\":62,\"narrative\":62,\"by\":62}\n",
      "\"wales\"\t{\"a\":1099,\"in\":1099,\"christmas\":1099,\"child's\":1099}\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\"a\"\t{\"limited\": 55, \"female\": 447, \"general\": 92, \"sea\": 62, \"in\": 1201, \"religious\": 59, \"george\": 92, \"biography\": 92, \"city\": 62, \"for\": 59, \"tales\": 123, \"child's\": 1099, \"by\": 62, \"forms\": 116, \"wales\": 1099, \"christmas\": 1099, \"government\": 102, \"collection\": 239, \"fairy\": 123, \"case\": 604, \"circumstantial\": 62, \"of\": 1011, \"study\": 604, \"bill\": 59, \"establishing\": 59, \"narrative\": 62, \"the\": 124}\n",
    "\"bill\"\t{\"a\": 59, \"religious\": 59, \"for\": 59, \"establishing\": 59}\n",
    "\"biography\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"general\": 92}\n",
    "\"by\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"sea\": 62}\n",
    "\"case\"\t{\"a\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"study\": 604, \"female\": 447, \"in\": 102}\n",
    "\"child's\"\t{\"a\": 1099, \"wales\": 1099, \"christmas\": 1099, \"in\": 1099}\n",
    "\"christmas\"\t{\"a\": 1099, \"wales\": 1099, \"in\": 1099, \"child's\": 1099}\n",
    "\"circumstantial\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"narrative\": 62}\n",
    "\"city\"\t{\"a\": 62, \"the\": 62, \"by\": 62, \"sea\": 62}\n",
    "\"collection\"\t{\"a\": 239, \"forms\": 116, \"fairy\": 123, \"tales\": 123, \"of\": 355}\n",
    "\"establishing\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"for\": 59}\n",
    "\"fairy\"\t{\"a\": 123, \"of\": 123, \"tales\": 123, \"collection\": 123}\n",
    "\"female\"\t{\"a\": 447, \"case\": 447, \"study\": 447, \"of\": 447}\n",
    "\"for\"\t{\"a\": 59, \"bill\": 59, \"religious\": 59, \"establishing\": 59}\n",
    "\"forms\"\t{\"a\": 116, \"of\": 232, \"collection\": 116}\n",
    "\"general\"\t{\"a\": 92, \"of\": 92, \"george\": 92, \"biography\": 92}\n",
    "\"george\"\t{\"a\": 92, \"of\": 92, \"biography\": 92, \"general\": 92}\n",
    "\"government\"\t{\"a\": 102, \"case\": 102, \"study\": 102, \"in\": 102}\n",
    "\"in\"\t{\"a\": 1201, \"case\": 102, \"government\": 102, \"study\": 102, \"child's\": 1099, \"wales\": 1099, \"christmas\": 1099}\n",
    "\"limited\"\t{\"a\": 55, \"case\": 55, \"study\": 55, \"of\": 55}\n",
    "\"narrative\"\t{\"a\": 62, \"of\": 62, \"the\": 62, \"circumstantial\": 62}\n",
    "\"of\"\t{\"a\": 1011, \"case\": 502, \"circumstantial\": 62, \"george\": 92, \"limited\": 55, \"tales\": 123, \"collection\": 355, \"general\": 92, \"forms\": 232, \"female\": 447, \"narrative\": 62, \"study\": 502, \"fairy\": 123, \"the\": 62, \"biography\": 92}\n",
    "\"religious\"\t{\"a\": 59, \"bill\": 59, \"for\": 59, \"establishing\": 59}\n",
    "\"sea\"\t{\"a\": 62, \"city\": 62, \"the\": 62, \"by\": 62}\n",
    "\"study\"\t{\"a\": 604, \"case\": 604, \"limited\": 55, \"government\": 102, \"of\": 502, \"female\": 447, \"in\": 102}\n",
    "\"tales\"\t{\"a\": 123, \"of\": 123, \"fairy\": 123, \"collection\": 123}\n",
    "\"the\"\t{\"a\": 124, \"city\": 62, \"circumstantial\": 62, \"of\": 62, \"sea\": 62, \"narrative\": 62, \"by\": 62}\n",
    "\"wales\"\t{\"a\": 1099, \"in\": 1099, \"christmas\": 1099, \"child's\": 1099}\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rm: Illegal option --recursive\n",
      "Usage: hadoop fs [generic options] -rm [-f] [-r|-R] [-skipTrash] <src> ...\n",
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/buildStripes.root.20180211.101756.853750\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/buildStripes.root.20180211.101756.853750/output...\n",
      "Removing temp directory /tmp/buildStripes.root.20180211.101756.853750...\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# Make Stripes from ngrams for systems test 2\n",
    "###########################################################################\n",
    "\n",
    "!hdfs dfs -rm --recursive systems_test_stripes_2\n",
    "!python buildStripes.py -r local atlas-boon-systems-test.txt > systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"atlas\"\t{\"dipped\":15,\"boon\":50}\n",
      "\"boon\"\t{\"atlas\":50,\"dipped\":10,\"cava\":10}\n",
      "\"cava\"\t{\"dipped\":10,\"boon\":10}\n",
      "\"dipped\"\t{\"atlas\":15,\"boon\":10,\"cava\":10}\n"
     ]
    }
   ],
   "source": [
    "!cat systems_test_stripes_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\"atlas\"   {\"dipped\": 15, \"boon\": 50}   \n",
    "\"boon\"    {\"atlas\": 50, \"dipped\": 10, \"cava\": 10}   \n",
    "\"cava\"    {\"dipped\": 10, \"boon\": 10} \n",
    "\"dipped\"  {\"atlas\": 15, \"boon\": 10, \"cava\": 10}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20,\"Y\":30,\"Z\":5}\n",
      "\"DocB\"\t{\"X\":100,\"Y\":20}\n",
      "\"DocC\"\t{\"M\":5,\"N\":20,\"Z\":5,\"Y\":1}\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Stripes for systems test 3 (given, no need to build stripes)\n",
    "########################################################################\n",
    "\n",
    "with open(\"systems_test_stripes_3\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20,\"Y\":30,\"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100,\"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5,\"N\":20,\"Z\":5,\"Y\":1}\\n'\n",
    "    ])\n",
    "!cat systems_test_stripes_3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Build Inverted Index - run the commands and insure that your output matches the output below\n",
    "\n",
    "* You might find this [paper](https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndex.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRinvertedIndex(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        if line:\n",
    "            word,stripe = line.split()\n",
    "            stripe=json.loads(stripe)\n",
    "            len_dict = len(stripe)\n",
    "            for key, _ in stripe.items():\n",
    "                yield key, (word.strip(\"\\\"\"), len_dict)\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        yield key, values\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndex.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.root.20180211.102158.103698\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.root.20180211.102158.103698/output...\n",
      "Removing temp directory /tmp/invertedIndex.root.20180211.102158.103698...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r local systems_test_stripes_1 > systems_test_index_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.root.20180211.102202.953187\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.root.20180211.102202.953187/output...\n",
      "Removing temp directory /tmp/invertedIndex.root.20180211.102202.953187...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r local systems_test_stripes_2 > systems_test_index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndex.root.20180211.102207.028985\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndex.root.20180211.102207.028985/output...\n",
      "Removing temp directory /tmp/invertedIndex.root.20180211.102207.028985...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndex.py -r local systems_test_stripes_3 > systems_test_index_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  1  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"a\" |          bill 4 |     biography 4 |            by 4\n",
      "          \"bill\" |            a 27 |  establishing 4 |           for 4\n",
      "     \"biography\" |            a 27 |       general 4 |        george 4\n",
      "            \"by\" |            a 27 |          city 4 |           sea 4\n",
      "          \"case\" |            a 27 |        female 4 |    government 4\n",
      "       \"child's\" |            a 27 |     christmas 4 |            in 7\n",
      "     \"christmas\" |            a 27 |       child's 4 |            in 7\n",
      "\"circumstantial\" |            a 27 |     narrative 4 |           of 16\n",
      "          \"city\" |            a 27 |            by 4 |           sea 4\n",
      "    \"collection\" |            a 27 |         fairy 4 |         forms 3\n",
      "  \"establishing\" |            a 27 |          bill 4 |           for 4\n",
      "         \"fairy\" |            a 27 |    collection 5 |           of 16\n",
      "        \"female\" |            a 27 |          case 7 |           of 16\n",
      "           \"for\" |            a 27 |          bill 4 |  establishing 4\n",
      "         \"forms\" |            a 27 |    collection 5 |           of 16\n",
      "       \"general\" |            a 27 |     biography 4 |        george 4\n",
      "        \"george\" |            a 27 |     biography 4 |       general 4\n",
      "    \"government\" |            a 27 |          case 7 |            in 7\n",
      "            \"in\" |            a 27 |          case 7 |       child's 4\n",
      "       \"limited\" |            a 27 |          case 7 |           of 16\n",
      "     \"narrative\" |            a 27 |circumstantial 4 |           of 16\n",
      "            \"of\" |            a 27 |     biography 4 |          case 7\n",
      "     \"religious\" |            a 27 |          bill 4 |  establishing 4\n",
      "           \"sea\" |            a 27 |            by 4 |          city 4\n",
      "         \"study\" |            a 27 |          case 7 |        female 4\n",
      "         \"tales\" |            a 27 |    collection 5 |         fairy 4\n",
      "           \"the\" |            a 27 |            by 4 |circumstantial 4\n",
      "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  2  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "         \"atlas\" |          boon 3 |        dipped 3 |                \n",
      "          \"boon\" |         atlas 2 |          cava 2 |        dipped 3\n",
      "          \"cava\" |          boon 3 |        dipped 3 |                \n",
      "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"M\" |          DocC 4 |                 |                \n",
      "             \"N\" |          DocC 4 |                 |                \n",
      "             \"X\" |          DocA 3 |          DocB 2 |                \n",
      "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
      "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "\n",
    "import json\n",
    "\n",
    "for i in range(1,4):\n",
    "    print \"—\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"—\"*100  \n",
    "    with open(\"systems_test_index_\"+str(i),\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "            print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "                (word), stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Systems test  1  - Inverted Index\n",
    "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "             \"a\" |          bill 4 |     biography 4 |            by 4\n",
    "          \"bill\" |            a 27 |  establishing 4 |           for 4\n",
    "     \"biography\" |            a 27 |       general 4 |        george 4\n",
    "            \"by\" |            a 27 |          city 4 |           sea 4\n",
    "          \"case\" |            a 27 |        female 4 |    government 4\n",
    "       \"child's\" |            a 27 |     christmas 4 |            in 7\n",
    "     \"christmas\" |            a 27 |       child's 4 |            in 7\n",
    "\"circumstantial\" |            a 27 |     narrative 4 |           of 15\n",
    "          \"city\" |            a 27 |            by 4 |           sea 4\n",
    "    \"collection\" |            a 27 |         fairy 4 |         forms 3\n",
    "  \"establishing\" |            a 27 |          bill 4 |           for 4\n",
    "         \"fairy\" |            a 27 |    collection 5 |           of 15\n",
    "        \"female\" |            a 27 |          case 7 |           of 15\n",
    "           \"for\" |            a 27 |          bill 4 |  establishing 4\n",
    "         \"forms\" |            a 27 |    collection 5 |           of 15\n",
    "       \"general\" |            a 27 |     biography 4 |        george 4\n",
    "        \"george\" |            a 27 |     biography 4 |       general 4\n",
    "    \"government\" |            a 27 |          case 7 |            in 7\n",
    "            \"in\" |            a 27 |          case 7 |       child's 4\n",
    "       \"limited\" |            a 27 |          case 7 |           of 15\n",
    "     \"narrative\" |            a 27 |circumstantial 4 |           of 15\n",
    "            \"of\" |            a 27 |     biography 4 |          case 7\n",
    "     \"religious\" |            a 27 |          bill 4 |  establishing 4\n",
    "           \"sea\" |            a 27 |            by 4 |          city 4\n",
    "         \"study\" |            a 27 |          case 7 |        female 4\n",
    "         \"tales\" |            a 27 |    collection 5 |         fairy 4\n",
    "           \"the\" |            a 27 |            by 4 |circumstantial 4\n",
    "         \"wales\" |            a 27 |       child's 4 |     christmas 4\n",
    "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Systems test  2  - Inverted Index\n",
    "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "         \"atlas\" |          boon 3 |        dipped 3 |                \n",
    "          \"boon\" |         atlas 2 |          cava 2 |        dipped 3\n",
    "          \"cava\" |          boon 3 |        dipped 3 |                \n",
    "        \"dipped\" |         atlas 2 |          boon 3 |          cava 2\n",
    "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Systems test  3  - Inverted Index\n",
    "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "             \"M\" |          DocC 4 |                 |                \n",
    "             \"N\" |          DocC 4 |                 |                \n",
    "             \"X\" |          DocA 3 |          DocB 2 |                \n",
    "             \"Y\" |          DocA 3 |          DocB 2 |          DocC 4\n",
    "             \"Z\" |          DocA 3 |          DocC 4 |                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Calculate similarities - run the commands and insure that your output matches the output below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: I will be calculating cosine similarity, which means I need to re-write my inverted index MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndexCosine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndexCosine.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRinvertedIndexCosine(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        if line:\n",
    "            word,stripe = line.split()\n",
    "            stripe=json.loads(stripe)\n",
    "            stripe_length = 1/math.sqrt(len(stripe))\n",
    "            for key, _ in stripe.items():\n",
    "                yield key, (word.strip(\"\\\"\"), stripe_length)\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        yield key, values\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndexCosine.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndexCosine.root.20180211.115513.567171\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndexCosine.root.20180211.115513.567171/output...\n",
      "Removing temp directory /tmp/invertedIndexCosine.root.20180211.115513.567171...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndexCosine.py -r local systems_test_stripes_1 > systems_test_index_cosine_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndexCosine.root.20180211.115516.616693\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndexCosine.root.20180211.115516.616693/output...\n",
      "Removing temp directory /tmp/invertedIndexCosine.root.20180211.115516.616693...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndexCosine.py -r local systems_test_stripes_2 > systems_test_index_cosine_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/invertedIndexCosine.root.20180211.115519.710130\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/invertedIndexCosine.root.20180211.115519.710130/output...\n",
      "Removing temp directory /tmp/invertedIndexCosine.root.20180211.115519.710130...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndexCosine.py -r local systems_test_stripes_3 > systems_test_index_cosine_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  1  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"a\" |        bill 0.5 |   biography 0.5 |          by 0.5\n",
      "          \"bill\" |  a 0.1924500897 |establishing 0.5 |         for 0.5\n",
      "     \"biography\" |  a 0.1924500897 |     general 0.5 |      george 0.5\n",
      "            \"by\" |  a 0.1924500897 |        city 0.5 |         sea 0.5\n",
      "          \"case\" |  a 0.1924500897 |      female 0.5 |  government 0.5\n",
      "       \"child's\" |  a 0.1924500897 |   christmas 0.5 |  in 0.377964473\n",
      "     \"christmas\" |  a 0.1924500897 |     child's 0.5 |  in 0.377964473\n",
      "\"circumstantial\" |  a 0.1924500897 |   narrative 0.5 |         of 0.25\n",
      "          \"city\" |  a 0.1924500897 |          by 0.5 |         sea 0.5\n",
      "    \"collection\" |  a 0.1924500897 |       fairy 0.5 |forms 0.5773502692\n",
      "  \"establishing\" |  a 0.1924500897 |        bill 0.5 |         for 0.5\n",
      "         \"fairy\" |  a 0.1924500897 |collection 0.4472135955 |         of 0.25\n",
      "        \"female\" |  a 0.1924500897 |case 0.377964473 |         of 0.25\n",
      "           \"for\" |  a 0.1924500897 |        bill 0.5 |establishing 0.5\n",
      "         \"forms\" |  a 0.1924500897 |collection 0.4472135955 |         of 0.25\n",
      "       \"general\" |  a 0.1924500897 |   biography 0.5 |      george 0.5\n",
      "        \"george\" |  a 0.1924500897 |   biography 0.5 |     general 0.5\n",
      "    \"government\" |  a 0.1924500897 |case 0.377964473 |  in 0.377964473\n",
      "            \"in\" |  a 0.1924500897 |case 0.377964473 |     child's 0.5\n",
      "       \"limited\" |  a 0.1924500897 |case 0.377964473 |         of 0.25\n",
      "     \"narrative\" |  a 0.1924500897 |circumstantial 0.5 |         of 0.25\n",
      "            \"of\" |  a 0.1924500897 |   biography 0.5 |case 0.377964473\n",
      "     \"religious\" |  a 0.1924500897 |        bill 0.5 |establishing 0.5\n",
      "           \"sea\" |  a 0.1924500897 |          by 0.5 |        city 0.5\n",
      "         \"study\" |  a 0.1924500897 |case 0.377964473 |      female 0.5\n",
      "         \"tales\" |  a 0.1924500897 |collection 0.4472135955 |       fairy 0.5\n",
      "           \"the\" |  a 0.1924500897 |          by 0.5 |circumstantial 0.5\n",
      "         \"wales\" |  a 0.1924500897 |     child's 0.5 |   christmas 0.5\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  2  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "         \"atlas\" |boon 0.5773502692 |dipped 0.5773502692 |                \n",
      "          \"boon\" |atlas 0.7071067812 |cava 0.7071067812 |dipped 0.5773502692\n",
      "          \"cava\" |boon 0.5773502692 |dipped 0.5773502692 |                \n",
      "        \"dipped\" |atlas 0.7071067812 |boon 0.5773502692 |cava 0.7071067812\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "             \"M\" |        DocC 0.5 |                 |                \n",
      "             \"N\" |        DocC 0.5 |                 |                \n",
      "             \"X\" |DocA 0.5773502692 |DocB 0.7071067812 |                \n",
      "             \"Y\" |DocA 0.5773502692 |DocB 0.7071067812 |        DocC 0.5\n",
      "             \"Z\" |DocA 0.5773502692 |        DocC 0.5 |                \n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index Cosine\n",
    "##########################################################\n",
    "\n",
    "import json\n",
    "\n",
    "for i in range(1,4):\n",
    "    print \"—\"*100\n",
    "    print \"Systems test \",i,\" - Inverted Index\"\n",
    "    print \"—\"*100  \n",
    "    with open(\"systems_test_index_cosine_\"+str(i),\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            word,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "            stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "            print \"{0:>16} |{1:>16} |{2:>16} |{3:>16}\".format(\n",
    "                (word), stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarity.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import math\n",
    "import mrjob\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRSimilarity(MRJob):\n",
    "    SORT_VALUES = True\n",
    "    JOBCONF = {\"mapreduce.job.reduces\": \"1\"}\n",
    "    \n",
    "    def mapper_first(self, _, line):\n",
    "        word, stripe = line.split(\"\\t\")\n",
    "        stripe = json.loads(stripe)\n",
    "        if len(stripe)>1:\n",
    "            count = 1\n",
    "            for s in stripe:\n",
    "                for i in xrange(count, len(stripe)):\n",
    "                    yield (s[0], stripe[i][0]), s[1]*stripe[i][1]*1.0\n",
    "                count += 1\n",
    "    \n",
    "    def reducer_first(self, pair, partSims):\n",
    "        sum_sims = sum(partSims)\n",
    "        yield sum_sims, (pair, sum_sims)\n",
    "        \n",
    "    def reducer_second(self, key, value):\n",
    "        yield float(key), list(value)[0]\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper = self.mapper_first,\n",
    "                reducer = self.reducer_first\n",
    "            ),\n",
    "            MRStep(\n",
    "                reducer = self.reducer_second,\n",
    "                jobconf={\n",
    "                        \"mapreduce.job.reduces\": \"1\",\n",
    "                        \"stream.num.map.output.key.fields\": 1,\n",
    "                        \"mapreduce.job.output.key.comparator.class\" : \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                        \"mapreduce.partition.keycomparator.options\":\"-k1,1nr\",\n",
    "                        \"mapred.num.key.comparator.options\":\"-k1,1nr\",\n",
    "                        \"mapred.text.key.comparator.options\": \"-k1,1nr\",\n",
    "                        \"SORT_VALUES\":True\n",
    "                   }\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRSimilarity.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: you must run in hadoop mode to generate sorted similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20180211.125320.735958\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125320.735958/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6593545834351242203.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518344134162_0015\n",
      "  Submitted application application_1518344134162_0015\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1518344134162_0015/\n",
      "  Running job: job_1518344134162_0015\n",
      "  Job job_1518344134162_0015 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518344134162_0015 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125320.735958/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4590\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=17678\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=24875\n",
      "\t\tFILE: Number of bytes written=404895\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4942\n",
      "\t\tHDFS: Number of bytes written=17678\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8894464\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3093504\n",
      "\t\tTotal time spent by all map tasks (ms)=8686\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8686\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3021\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3021\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8686\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=3021\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1560\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=90\n",
      "\t\tInput split bytes=352\n",
      "\t\tMap input records=28\n",
      "\t\tMap output bytes=23493\n",
      "\t\tMap output materialized bytes=24881\n",
      "\t\tMap output records=688\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=747913216\n",
      "\t\tReduce input groups=378\n",
      "\t\tReduce input records=688\n",
      "\t\tReduce output records=378\n",
      "\t\tReduce shuffle bytes=24881\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1376\n",
      "\t\tTotal committed heap usage (bytes)=552075264\n",
      "\t\tVirtual memory (bytes) snapshot=4074962944\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7507140040764545101.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518344134162_0016\n",
      "  Submitted application application_1518344134162_0016\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1518344134162_0016/\n",
      "  Running job: job_1518344134162_0016\n",
      "  Job job_1518344134162_0016 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518344134162_0016 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125320.735958/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=21774\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1861\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=18440\n",
      "\t\tFILE: Number of bytes written=393645\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=22114\n",
      "\t\tHDFS: Number of bytes written=1861\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8001536\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3066880\n",
      "\t\tTotal time spent by all map tasks (ms)=7814\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7814\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2995\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2995\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7814\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2995\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1530\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=71\n",
      "\t\tInput split bytes=340\n",
      "\t\tMap input records=378\n",
      "\t\tMap output bytes=17678\n",
      "\t\tMap output materialized bytes=18446\n",
      "\t\tMap output records=378\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=751677440\n",
      "\t\tReduce input groups=33\n",
      "\t\tReduce input records=378\n",
      "\t\tReduce output records=33\n",
      "\t\tReduce shuffle bytes=18446\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=756\n",
      "\t\tTotal committed heap usage (bytes)=619184128\n",
      "\t\tVirtual memory (bytes) snapshot=4071874560\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125320.735958/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125320.735958...\n",
      "Removing temp directory /tmp/similarity.root.20180211.125320.735958...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_cosine_1 > systems_test_similarities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20180211.125444.655159\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125444.655159/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3818410674907357171.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518344134162_0017\n",
      "  Submitted application application_1518344134162_0017\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1518344134162_0017/\n",
      "  Running job: job_1518344134162_0017\n",
      "  Job job_1518344134162_0017 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518344134162_0017 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125444.655159/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=401\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=367\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=338\n",
      "\t\tFILE: Number of bytes written=355821\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=753\n",
      "\t\tHDFS: Number of bytes written=367\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8229888\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3018752\n",
      "\t\tTotal time spent by all map tasks (ms)=8037\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8037\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2948\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2948\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8037\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2948\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1380\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=88\n",
      "\t\tInput split bytes=352\n",
      "\t\tMap input records=4\n",
      "\t\tMap output bytes=316\n",
      "\t\tMap output materialized bytes=344\n",
      "\t\tMap output records=8\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=765730816\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=6\n",
      "\t\tReduce shuffle bytes=344\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=16\n",
      "\t\tTotal committed heap usage (bytes)=619708416\n",
      "\t\tVirtual memory (bytes) snapshot=4089995264\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5984229074200017124.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518344134162_0018\n",
      "  Submitted application application_1518344134162_0018\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1518344134162_0018/\n",
      "  Running job: job_1518344134162_0018\n",
      "  Job job_1518344134162_0018 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518344134162_0018 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125444.655159/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=551\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=183\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=385\n",
      "\t\tFILE: Number of bytes written=357535\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=891\n",
      "\t\tHDFS: Number of bytes written=183\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=7444480\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2965504\n",
      "\t\tTotal time spent by all map tasks (ms)=7270\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7270\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2896\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2896\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7270\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2896\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1490\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=99\n",
      "\t\tInput split bytes=340\n",
      "\t\tMap input records=6\n",
      "\t\tMap output bytes=367\n",
      "\t\tMap output materialized bytes=391\n",
      "\t\tMap output records=6\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=754647040\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=391\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=12\n",
      "\t\tTotal committed heap usage (bytes)=552075264\n",
      "\t\tVirtual memory (bytes) snapshot=4100210688\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125444.655159/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125444.655159...\n",
      "Removing temp directory /tmp/similarity.root.20180211.125444.655159...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_cosine_2 > systems_test_similarities_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/similarity.root.20180211.125556.281154\n",
      "Copying local files to hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125556.281154/files/...\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob1091797265403280663.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518344134162_0019\n",
      "  Submitted application application_1518344134162_0019\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1518344134162_0019/\n",
      "  Running job: job_1518344134162_0019\n",
      "  Job job_1518344134162_0019 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518344134162_0019 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125556.281154/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=288\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=180\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=206\n",
      "\t\tFILE: Number of bytes written=355557\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=640\n",
      "\t\tHDFS: Number of bytes written=180\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8537088\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2987008\n",
      "\t\tTotal time spent by all map tasks (ms)=8337\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8337\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2917\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2917\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8337\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=2917\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1530\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=79\n",
      "\t\tInput split bytes=352\n",
      "\t\tMap input records=5\n",
      "\t\tMap output bytes=190\n",
      "\t\tMap output materialized bytes=212\n",
      "\t\tMap output records=5\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=755716096\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=212\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=10\n",
      "\t\tTotal committed heap usage (bytes)=617086976\n",
      "\t\tVirtual memory (bytes) snapshot=4084625408\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8202694359918721312.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1518344134162_0020\n",
      "  Submitted application application_1518344134162_0020\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1518344134162_0020/\n",
      "  Running job: job_1518344134162_0020\n",
      "  Job job_1518344134162_0020 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1518344134162_0020 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125556.281154/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=180\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=192\n",
      "\t\tFILE: Number of bytes written=357149\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=610\n",
      "\t\tHDFS: Number of bytes written=180\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8197120\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4179968\n",
      "\t\tTotal time spent by all map tasks (ms)=8005\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8005\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4082\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4082\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8005\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4082\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1710\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=102\n",
      "\t\tInput split bytes=340\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=180\n",
      "\t\tMap output materialized bytes=198\n",
      "\t\tMap output records=3\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=765038592\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=198\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=6\n",
      "\t\tTotal committed heap usage (bytes)=552599552\n",
      "\t\tVirtual memory (bytes) snapshot=4107370496\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125556.281154/output...\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/similarity.root.20180211.125556.281154...\n",
      "Removing temp directory /tmp/similarity.root.20180211.125556.281154...\n"
     ]
    }
   ],
   "source": [
    "!python similarity.py -r hadoop systems_test_index_cosine_3 > systems_test_similarities_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  1  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       1.000000 |[u'female', u'limited'] |       1.000000 \n",
      "       0.866025 |[u'forms', u'tales'] |       0.866025 \n",
      "       0.857143 |[u'case', u'study'] |       0.857143 \n",
      "       0.750000 |[u'bill', u'establishing'] |       0.750000 \n",
      "       0.721688 |  [u'a', u'of'] |       0.721688 \n",
      "       0.670820 |[u'collection', u'tales'] |       0.670820 \n",
      "       0.577350 |[u'forms', u'narrative'] |       0.577350 \n",
      "       0.566947 |[u'case', u'limited'] |       0.566947 \n",
      "       0.559017 |[u'collection', u'of'] |       0.559017 \n",
      "       0.516398 |[u'collection', u'forms'] |       0.516398 \n",
      "       0.500000 |[u'general', u'tales'] |       0.500000 \n",
      "       0.472456 |[u'case', u'of'] |       0.472456 \n",
      "       0.447214 |[u'collection', u'limited'] |       0.447214 \n",
      "       0.436436 |[u'case', u'forms'] |       0.436436 \n",
      "       0.436436 |[u'a', u'case'] |       0.436436 \n",
      "       0.433013 |[u'forms', u'of'] |       0.433013 \n",
      "       0.428571 |[u'in', u'study'] |       0.428571 \n",
      "       0.377964 |[u'biography', u'study'] |       0.377964 \n",
      "       0.375000 |[u'government', u'of'] |       0.375000 \n",
      "       0.344265 |[u'a', u'collection'] |       0.344265 \n",
      "       0.338062 |[u'case', u'collection'] |       0.338062 \n",
      "       0.288675 |[u'forms', u'wales'] |       0.288675 \n",
      "       0.288675 |[u'a', u'bill'] |       0.288675 \n",
      "       0.285714 |[u'case', u'the'] |       0.285714 \n",
      "       0.283473 | [u'in', u'of'] |       0.283473 \n",
      "       0.250000 |[u'city', u'george'] |       0.250000 \n",
      "       0.223607 |[u\"child's\", u'collection'] |       0.223607 \n",
      "       0.222222 |[u'a', u'forms'] |       0.222222 \n",
      "       0.218218 |[u'forms', u'in'] |       0.218218 \n",
      "       0.188982 |[u'the', u'wales'] |       0.188982 \n",
      "       0.169031 |[u'collection', u'in'] |       0.169031 \n",
      "       0.142857 |[u'in', u'the'] |       0.142857 \n",
      "       0.125000 |[u'establishing', u'of'] |       0.125000 \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  2  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       1.000000 |[u'atlas', u'cava'] |       1.000000 \n",
      "       0.666667 |[u'boon', u'dipped'] |       0.666667 \n",
      "       0.408248 |[u'cava', u'dipped'] |       0.408248 \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Systems test  3  - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "        average |           pair |         cosine\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "       0.816497 |[u'DocA', u'DocB'] |       0.816497 \n",
      "       0.577350 |[u'DocA', u'DocC'] |       0.577350 \n",
      "       0.353553 |[u'DocB', u'DocC'] |       0.353553 \n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "# Note: adjust print formatting if you need to\n",
    "############################################\n",
    "\n",
    "import json\n",
    "for i in range(1,4):\n",
    "    print '—'*110\n",
    "    print \"Systems test \",i,\" - Similarity measures\"\n",
    "    print '—'*110\n",
    "    print \"{0:>15} |{1:>15} |{2:>15}\".format(\n",
    "          \"average\", \"pair\", \"cosine\")\n",
    "    print '-'*110\n",
    "\n",
    "    with open(\"systems_test_similarities_\"+str(i),\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            avg,stripe = line.split(\"\\t\")\n",
    "            stripe = json.loads(stripe)\n",
    "\n",
    "            print \"{0:>15f} |{1:>15} |{2:>15f} \".format(\n",
    "              float(avg), stripe[0], float(stripe[1]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Similairity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Systems test  1  - Similarity measures\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  1.000000 |    female - limited |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
    "  0.868292 |       fairy - forms |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
    "  0.868292 |       forms - tales |       0.866025 |       0.750000 |       1.000000 |       0.857143\n",
    "  0.830357 |        case - study |       0.857143 |       0.750000 |       0.857143 |       0.857143\n",
    "  0.712500 | bill - establishing |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |   christmas - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |circumstantial - narrative |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |            by - sea |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |           by - city |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |     child's - wales |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 |  biography - george |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  0.712500 | child's - christmas |       0.750000 |       0.600000 |       0.750000 |       0.750000\n",
    "  ...\n",
    "  \n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Systems test  2  - Similarity measures\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  1.000000 |        atlas - cava |       1.000000 |       1.000000 |       1.000000 |       1.000000\n",
    "  0.625000 |       boon - dipped |       0.666667 |       0.500000 |       0.666667 |       0.666667\n",
    "  0.389562 |       cava - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |         boon - cava |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |      atlas - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "  0.389562 |        atlas - boon |       0.408248 |       0.250000 |       0.500000 |       0.400000\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "Systems test  3  - Similarity measures\n",
    "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
    "   average |                pair |         cosine |        jaccard |        overlap |           dice\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "  0.820791 |         DocA - DocB |       0.816497 |       0.666667 |       1.000000 |       0.800000\n",
    "  0.553861 |         DocA - DocC |       0.577350 |       0.400000 |       0.666667 |       0.571429\n",
    "  0.346722 |         DocB - DocC |       0.353553 |       0.200000 |       0.500000 |       0.333333\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === END OF PHASE 1 ==="
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1080px",
    "left": "0px",
    "right": "1331px",
    "top": "107px",
    "width": "287px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
